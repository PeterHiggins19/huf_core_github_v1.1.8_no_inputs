{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Higgins Unity Framework (HUF) \u2014 Documentation","text":"<p>This site is meant to help you run HUF first, then learn the rest in small steps.</p>"},{"location":"#start-here","title":"Start here","text":"<ul> <li>\u2705 Beginner (no GitHub knowledge required): Get Started (Zero GitHub)</li> <li>\u25b6\ufe0f Copy/paste demo runner: Quick Run</li> <li>Recommended reading order: Learning Path</li> <li>Download data (Markham + Toronto): Data Sources &amp; Fetching</li> <li>\u25b6\ufe0f Run examples (\u201ccases\u201d): Cases</li> <li>Fix common problems: Troubleshooting</li> </ul>"},{"location":"#long-tail-accounting-lens","title":"Long tail (accounting lens)","text":"<p>Not ML class imbalance: here \u201clong tail\u201d means mass distribution + exception reweighting (baseline vs filtered view). Start here: Long tail (accounting lens)</p>"},{"location":"#what-is-huf","title":"What is HUF?","text":"<ul> <li>What is the Higgins Unity Framework?</li> </ul>"},{"location":"#for-developers","title":"For developers","text":"<ul> <li>Start Here (Developer)</li> <li>Reference Manual</li> <li>Theory Notes (optional)</li> <li>GitHub for Beginners</li> </ul>"},{"location":"The_Higgins_Unity_Framework/","title":"The Higgins Unity Framework (HUF)","text":"<p>This project is the HUF Core implementation: a practical toolkit you can run to turn messy real\u2011world datasets into consistent, comparable outputs.</p> <p>If you\u2019re not technical, don\u2019t worry \u2014 you can still use HUF by following the step\u2011by\u2011step guides.</p>"},{"location":"The_Higgins_Unity_Framework/#plainenglish-idea","title":"Plain\u2011English idea","text":"<p>Real systems produce lots of different signals:</p> <ul> <li>budgets, traffic timing, anomalies, logs, counts, categories\u2026</li> <li>all with different units, scales, and missing values</li> </ul> <p>HUF\u2019s core trick is to convert those signals into a normalized representation so that:</p> <ul> <li>different sources can be compared fairly</li> <li>changes over time are easier to detect</li> <li>\u201cwhat matters most\u201d can be ranked without hand\u2011tuning every dataset</li> </ul> <p>In this repo, that \u201cnormalization\u201d mostly shows up as:</p> <ul> <li>cleaning inputs</li> <li>mapping columns into a consistent schema</li> <li>producing standardized outputs you can analyze or visualize</li> </ul>"},{"location":"The_Higgins_Unity_Framework/#what-you-get-from-this-repository","title":"What you get from this repository","text":"<ul> <li>Repeatable demos (\u201ccases\u201d) with real civic datasets (Markham + Toronto)</li> <li>A command line tool (<code>huf ...</code>) to run those cases</li> <li>A structure you can copy to add your own data adapters and cases</li> </ul>"},{"location":"The_Higgins_Unity_Framework/#key-concepts-no-math","title":"Key concepts (no math)","text":""},{"location":"The_Higgins_Unity_Framework/#1-inputs-adapters-outputs","title":"1) Inputs \u2192 adapters \u2192 outputs","text":"<p>A \u201ccase\u201d takes some input file(s), runs a transformation, and writes results to an output folder.</p> <ul> <li>Input examples: <code>.xlsx</code>, <code>.csv</code>, large datasets (Planck is guided/manual)</li> <li>Output examples: cleaned tables, normalized metrics, reports</li> </ul>"},{"location":"The_Higgins_Unity_Framework/#2-normalization-the-unity-idea","title":"2) Normalization (the \u201cunity\u201d idea)","text":"<p>Normalization means turning different kinds of numbers into a consistent scale so they can be compared.</p> <p>Example: - Dataset A ranges from 0\u201310 - Dataset B ranges from 0\u201310,000</p> <p>After normalization, both can live on the same scale, so ranking and anomaly detection are meaningful.</p>"},{"location":"The_Higgins_Unity_Framework/#3-cases-are-learning-modules","title":"3) \u201cCases\u201d are learning modules","text":"<p>Each case is both: - a working example you can run today - a template you can copy when adding your own workflow</p>"},{"location":"The_Higgins_Unity_Framework/#where-to-begin","title":"Where to begin","text":"<ol> <li>Follow the beginner path: Learning Path </li> <li>Run a demo: Cases </li> <li>If you hit errors: Troubleshooting</li> </ol>"},{"location":"The_Higgins_Unity_Framework/#advanced-theory-optional","title":"Advanced / theory (optional)","text":"<p>Some HUF writings discuss deeper mathematics (categories, morphisms, topology, etc.). Those are not required to run the tools in this repo.</p> <p>If you want the deeper background, start with: - Theory Notes - Handbook</p>"},{"location":"The_Higgins_Unity_Framework/#glossary","title":"Glossary","text":"<ul> <li>Case: a runnable example workflow (input \u2192 process \u2192 output).</li> <li>Adapter: code that reads a particular dataset shape and maps it into HUF\u2019s expected schema.</li> <li>Normalization: converting values into a consistent scale to compare across sources.</li> <li>Schema: the column names / fields that HUF expects for a given workflow.</li> </ul> <p>Note: The original author notes and drafts existed as <code>.Markdown</code>. This repo now keeps documentation in Markdown (<code>.md</code>) so it renders well on GitHub and GitHub Pages.</p>"},{"location":"cli_huf_reference/","title":"CLI: HUF command lists, labels, terminology","text":"<p>This page is a \u201csingle place\u201d to answer: - what commands exist, - what files they expect, - what artifacts they emit, - and what words mean (regime, \u03c4, active set\u2026).</p>"},{"location":"cli_huf_reference/#run-commands-in-the-shell-powershell-not-inside-python","title":"Run commands in the shell (PowerShell), not inside Python","text":"<p>If your prompt looks like this:</p> <ul> <li><code>&gt;&gt;&gt;</code></li> </ul> <p>\u2026you are inside the Python REPL. Shell commands like <code>huf ...</code> will fail with <code>SyntaxError</code>.</p> <p>Exit back to PowerShell:</p> <ul> <li>type <code>exit()</code> or</li> <li>press Ctrl+Z then Enter</li> </ul> <p>Then run commands like:</p> <pre><code>.\\.venv\\Scripts\\huf --help\n</code></pre>"},{"location":"cli_huf_reference/#windowsconda-rule-copypaste-reliability","title":"Windows/Conda rule (copy/paste reliability)","text":"<p>After the repo venv exists, always run tools via the repo executables:</p> <pre><code>.\\.venv\\Scripts\\python -V\n.\\.venv\\Scripts\\huf --help\n.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre> <p>For file paths inside commands and docs, prefer forward slashes: - \u2705 <code>scripts/fetch_data.py</code> - \u2705 <code>cases/traffic_phase/inputs/...</code> - Only use backslashes for the venv executables: <code>.\\.venv\\Scripts\\python</code></p>"},{"location":"cli_huf_reference/#discover-commands-dont-guess","title":"Discover commands (don\u2019t guess)","text":"<p>Canonical command lists come from <code>--help</code>:</p> <pre><code>.\\.venv\\Scripts\\huf --help\n.\\.venv\\Scripts\\huf traffic --help\n.\\.venv\\Scripts\\huf traffic-anomaly --help\n</code></pre> <p>If a flag name differs between versions, trust <code>--help</code> over any doc page.</p>"},{"location":"cli_huf_reference/#planck-guide-windows","title":"Planck guide (Windows)","text":"<p>There is no <code>make</code> on Windows. Print the Planck download guide like this:</p> <pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --planck-guide\n</code></pre> <p>Then run Planck after placing the FITS and installing <code>astropy</code> in the same venv:</p> <pre><code>.\\.venv\\Scripts\\python -m pip install astropy\n.\\.venv\\Scripts\\huf planck --fits \"cases/planck70/inputs/LFI_SkyMap_070_1024_R3.00_full.fits\" --out out/planck70 --retained-target 0.97 --nside-out 64\n</code></pre>"},{"location":"cli_huf_reference/#output-artifacts-the-contract","title":"Output artifacts (the contract)","text":"<p>Every valid HUF run emits the \u201ccontract artifacts\u201d in the run output folder:</p> <ul> <li><code>artifact_1_coherence_map.csv</code> \u2014 \u201cWhere the budget went\u201d by regime (ranked)</li> <li><code>artifact_2_active_set.csv</code> \u2014 retained items with global + local shares</li> <li><code>artifact_3_trace_report.jsonl</code> \u2014 line-by-line trace records (provenance)</li> <li><code>artifact_4_error_budget.json</code> \u2014 explicit discarded budget + diagnostics</li> </ul> <p>If any of these are missing, treat the run as non-auditable.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome \u2014 even small things like typo fixes or \u201cthis step confused me\u201d issues help a lot.</p>"},{"location":"contributing/#the-easiest-ways-to-help","title":"The easiest ways to help","text":"<ol> <li>Open an Issue (best for feedback / questions)</li> <li>Describe what you tried, what happened, and what you expected.</li> <li> <p>If it\u2019s about a command, paste the exact command + the terminal output.</p> </li> <li> <p>Send a Pull Request</p> </li> <li>Fork the repo \u2192 create a branch \u2192 make changes \u2192 open a PR.</li> <li>Docs-only PRs are totally fine (they\u2019re often the most valuable).</li> </ol>"},{"location":"contributing/#suggested-contribution-ideas","title":"Suggested contribution ideas","text":"<ul> <li>Add a small new \u201cworked example\u201d page for a dataset you care about</li> <li>Improve Windows copy/paste reliability</li> <li>Add tiny helper scripts (inspect artifacts, sanity checks, etc.)</li> </ul>"},{"location":"contributing/#repo-settings-that-help-contributors","title":"Repo settings that help contributors","text":"<ul> <li>Issues: Enabled</li> <li>Pull requests from forks: Enabled</li> <li>Branch protection (optional): require CI checks on <code>main</code></li> </ul> <p>The canonical contributor guidance lives in the root CONTRIBUTING.md in the repo.</p>"},{"location":"data_sources/","title":"Data Sources &amp; Fetching","text":"<p>This repo ships small inputs for Markham and Toronto via <code>scripts/fetch_data.py</code>. Planck is guide-only because the file is large.</p>"},{"location":"data_sources/#fetch-markham-toronto-inputs","title":"Fetch Markham + Toronto inputs","text":"<pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --markham --toronto --yes\n</code></pre> <p>If you haven't created the venv yet:</p> <pre><code>python scripts/bootstrap.py\n</code></pre>"},{"location":"data_sources/#docs-preview","title":"Docs preview","text":"<pre><code>.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre>"},{"location":"docs_inventory/","title":"Docs inventory","text":"<p>This page is generated from the doc catalog:</p> <ul> <li><code>notes/doc_catalog/docs_current.json</code></li> <li><code>notes/doc_catalog/docs_removed.json</code></li> </ul> <p>Do not edit this page by hand. Run:</p> <pre><code>.\\.venv\\Scripts\\python scripts/docs_hygiene.py\n</code></pre>"},{"location":"docs_inventory/#summary","title":"Summary","text":"<ul> <li>Current docs: 0</li> <li>Removed docs tracked: 0</li> </ul>"},{"location":"docs_inventory/#current-docs","title":"Current docs","text":"Group Page Path"},{"location":"docs_inventory/#removed-docs-tracked","title":"Removed docs (tracked)","text":"<p>None currently tracked.</p>"},{"location":"docs_inventory/#what-changed","title":"What changed?","text":"<p>If you want a visible changelog for stakeholders, keep <code>docs_removed.json</code> populated with entries like:</p> <pre><code>[{\"path\":\"old_page.md\",\"removed_at\":\"2026-02-20\",\"reason\":\"superseded\",\"replaced_by\":\"new_page.md\"}]\n</code></pre>"},{"location":"ethics/","title":"Ethics &amp; disclosure (AI-assisted drafting)","text":""},{"location":"ethics/#what-this-page-is","title":"What this page is","text":"<p>A transparency statement about how these documents were produced and what the reader should and should not infer about the authority of the text.</p>"},{"location":"ethics/#ai-assistance-disclosure","title":"AI assistance disclosure","text":"<p>Portions of this documentation were drafted and edited using an AI-assisted drafting tool. The tool was used for: - reformatting and restructuring text - improving clarity and readability - generating alternative phrasings and organizing outlines</p>"},{"location":"ethics/#author-responsibility","title":"Author responsibility","text":"<p>The author reviewed and curated the final wording and is responsible for: - the intended meaning of formal definitions and claims - the scope boundaries between formal core and interpretive discussion - the correctness of any results presented as established</p>"},{"location":"ethics/#reproducibility-commitment","title":"Reproducibility commitment","text":"<p>Where claims depend on computation, the project aims to make results reproducible through: - code and scripts in the repository - deterministic runs where possible - documented inputs/outputs and audit checks</p>"},{"location":"ethics/#conservative-stance-on-claims","title":"Conservative stance on claims","text":"<p>This documentation avoids presenting speculative equivalences as established results. In particular: - analogies to other mathematical areas are labeled as interpretive unless formally proven - domain applications are presented as use cases, not as derivations of domain laws</p>"},{"location":"ethics/#how-to-cite-use-responsibly","title":"How to cite / use responsibly","text":"<p>If you use HUF in academic or engineering work: - cite the repository and the specific version of the docs - report assumptions and failure modes - prefer minimal, testable claims over broad unification language</p>"},{"location":"get_started_readme/","title":"HUF Get Started Package","text":"<p>Open Start_Here.md or Start_Here.md.</p> <ul> <li>If you already have the HUF GitHub package, run the start scripts in the repo root:</li> <li>Windows: <code>START_HERE_WINDOWS.bat</code></li> <li>macOS: <code>START_HERE_MAC.command</code></li> <li>Linux: <code>start_here_linux.sh</code></li> </ul> <p>This package does not include large input datasets. Data sources and instructions are in <code>docs/data_sources.md</code>.</p>"},{"location":"get_started_zero_github/","title":"Start Here (Zero GitHub Knowledge)","text":"<p>You can run HUF without learning command-line git.</p> <p>The goal is:</p> <ul> <li>you can run <code>.\\.venv\\Scripts\\huf --help</code></li> <li>you can produce <code>out/.../run_stamp.json</code></li> </ul>"},{"location":"get_started_zero_github/#option-1-easiest-recommended-the-one-click-starter","title":"Option 1 \u2014 easiest (recommended): the one-click starter","text":"<p>From the repo folder:</p> <ul> <li>Windows: double-click <code>START_HERE_WINDOWS.bat</code></li> <li>macOS: right-click <code>START_HERE_MAC.command</code> \u2192 Open</li> <li>Linux: <code>./start_here_linux.sh</code></li> </ul> <p>This creates a local <code>.venv</code> and installs what you need.</p>"},{"location":"get_started_zero_github/#option-2-manual-windows-powershell","title":"Option 2 \u2014 manual (Windows PowerShell)","text":"<p>From the repo root:</p>"},{"location":"get_started_zero_github/#1-create-a-repo-virtual-environment","title":"1) Create a repo virtual environment","text":"<p>If you plan to run the docs locally, install <code>.[dev,docs]</code> (MkDocs is pinned in the <code>docs</code> extra).</p> <pre><code>python -m venv .venv\n.\\.venv\\Scripts\\python -m pip install --upgrade pip setuptools wheel\n.\\.venv\\Scripts\\python -m pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"get_started_zero_github/#2-fetch-the-civic-inputs-markham-toronto","title":"2) Fetch the civic inputs (Markham + Toronto)","text":"<pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --markham --toronto --yes\n</code></pre>"},{"location":"get_started_zero_github/#3-run-the-demo-cases","title":"3) Run the demo cases","text":"<pre><code>.\\.venv\\Scripts\\huf markham --xlsx cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx --out out/markham2018\n.\\.venv\\Scripts\\huf traffic --csv cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_phase\n.\\.venv\\Scripts\\huf traffic-anomaly --csv cases/traffic_anomaly/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_anomaly --status \"Green Termination\"\n</code></pre>"},{"location":"get_started_zero_github/#4-preview-the-docs-locally-optional","title":"4) Preview the docs locally (optional)","text":"<p>Always run MkDocs through the repo venv:</p> <pre><code>.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre>"},{"location":"get_started_zero_github/#important-windows-note-slashes","title":"Important Windows note: slashes","text":"<ul> <li>Use forward slashes for file paths in docs and commands: <code>scripts/fetch_data.py</code>, <code>cases/...</code>, <code>out/...</code></li> <li>Use backslashes only for the venv executables: <code>.\\.venv\\Scripts\\python</code>, <code>.\\.venv\\Scripts\\huf</code></li> </ul>"},{"location":"github_for_beginners/","title":"GitHub for HUF (Beginner, GUI-first)","text":"<p>This is a plain-language guide to using HUF on GitHub with minimal jargon.</p>"},{"location":"github_for_beginners/#the-easiest-way-github-desktop-point-and-click","title":"The easiest way: GitHub Desktop (point-and-click)","text":"<ol> <li>Install GitHub Desktop</li> <li>Clone the repository</li> <li>Run the one-click starter scripts</li> </ol> <p>Then fetch data:</p> <pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --markham --toronto --yes\n</code></pre> <p>Run a demo:</p> <pre><code>.\\.venv\\Scripts\\huf --help\n</code></pre>"},{"location":"gui_quickstart/","title":"GUI Quickstart (non\u2011GitHub\u2011native users)","text":"<p>This page is for people who prefer GUI workflows (e.g., GitHub Desktop, file explorers, Word/Excel) but still want to run HUF and keep record copies.</p>"},{"location":"gui_quickstart/#note","title":"Note","text":"<p>This docs-only package does not include the code/CLI. To run HUF, download the GitHub package release as well.</p>"},{"location":"gui_quickstart/#what-you-can-do-without-git","title":"What you can do without Git","text":"<p>If you don\u2019t want Git at all:</p> <ol> <li>Download the latest release ZIP from GitHub (look for Releases on the right side of the repo page).</li> <li>Unzip it to a folder like <code>Documents/HUF/</code>.</li> <li>Open the Markdown record copies in <code>docs/</code>:</li> <li><code>docs/handbook.md</code></li> <li><code>docs/reference_manual.md</code></li> <li><code>docs/data_sources.md</code></li> </ol> <p>You can still run the CLI from this unzipped folder (see below).</p>"},{"location":"gui_quickstart/#using-github-desktop-recommended-for-updates","title":"Using GitHub Desktop (recommended for updates)","text":"<p>If you want one-click updates:</p> <ol> <li>Install GitHub Desktop.</li> <li>In GitHub Desktop: File \u2192 Clone repository\u2026</li> <li>Pick a local folder (e.g., <code>Documents/GitHub/huf-core</code>).</li> <li>To update later: press Fetch origin then Pull origin.</li> </ol>"},{"location":"gui_quickstart/#one-time-setup-to-run-huf","title":"One-time setup to run HUF","text":"<p>You need Python 3.10+ installed.</p>"},{"location":"gui_quickstart/#step-1-open-a-terminal-in-the-repo-folder","title":"Step 1 \u2014 Open a terminal in the repo folder","text":"<ul> <li>Windows: open File Explorer \u2192 go to the repo folder \u2192 right\u2011click \u2192 Open in Terminal (or PowerShell).</li> <li>macOS: Finder \u2192 repo folder \u2192 right\u2011click \u2192 New Terminal at Folder (or open Terminal and <code>cd</code>).</li> <li>Linux: open Terminal and <code>cd</code> into the folder.</li> </ul>"},{"location":"gui_quickstart/#step-2-run-the-bootstrap-crossplatform","title":"Step 2 \u2014 Run the bootstrap (cross\u2011platform)","text":"<p>From the repo root:</p> <pre><code>python scripts/bootstrap.py\n</code></pre> <p>This creates <code>.venv/</code> and installs everything you need.</p> <p>If you\u2019re on macOS/Linux you can also use: <code>make bootstrap</code></p>"},{"location":"gui_quickstart/#download-the-real-input-data-no-big-inputs-are-bundled","title":"Download the real input data (no big inputs are bundled)","text":""},{"location":"gui_quickstart/#markham-toronto-automatic","title":"Markham + Toronto (automatic)","text":"<p>After bootstrap, run one of these:</p> <pre><code>make fetch-data\n# or:\npython scripts/fetch_data.py --markham --toronto\n</code></pre>"},{"location":"gui_quickstart/#toronto-non-interactive-yes","title":"Toronto non-interactive (<code>--yes</code>)","text":"<p>For scripted demos (no prompts):</p> <pre><code>make fetch-toronto-yes\n# or:\npython scripts/fetch_data.py --toronto --yes\n</code></pre>"},{"location":"gui_quickstart/#planck-guidedmanual","title":"Planck (guided/manual)","text":"<p>Planck files are large, so HUF prints the steps instead of downloading by default:</p> <pre><code>make planck-guide\n# or:\npython scripts/fetch_data.py --planck-guide\n</code></pre>"},{"location":"gui_quickstart/#run-the-demos","title":"Run the demos","text":""},{"location":"gui_quickstart/#markham","title":"Markham","text":"<pre><code>huf markham --xlsx cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx --out out/markham2018 --tau-global 0.005 --tau-local 0.02\n</code></pre>"},{"location":"gui_quickstart/#toronto-traffic","title":"Toronto traffic","text":"<pre><code>huf traffic --csv cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_phase --tau-local 0.05\n</code></pre>"},{"location":"gui_quickstart/#where-outputs-go","title":"Where outputs go","text":"<p>Each run writes a folder like <code>out/markham2018/</code> or <code>out/traffic_phase/</code> containing the mandatory artifacts:</p> <ul> <li><code>artifact_1_coherence_map.csv</code></li> <li><code>artifact_2_active_set.csv</code></li> <li><code>artifact_3_trace_report.jsonl</code></li> <li><code>artifact_4_error_budget.json</code></li> </ul> <p>You can open the CSVs in Excel and keep them with your meeting notes.</p>"},{"location":"handbook/","title":"HUF Handbook (Clean Edition) \u2014 v1.3.0","text":"<p>Build date: 2026-02-21 Canonical public handbook (Markdown). This supersedes prior handbook variants.</p> <p>Ethics disclosure (short)</p> <p>Drafted with AI assistance as an editing tool and reviewed/curated by the author. Formal claims are stated conservatively and should be reproducible via repository code where applicable. See Ethics.</p> <p>Conceptual trigger: why 'loss' can make retained look stronger</p> <p>If you discard some mass and then re-normalize, the retained portion is re-scaled to sum to 1. That can make the retained shares appear larger even though nothing \u201cgrew\u201d \u2014 you are looking at the conditional mix given what was kept.</p> <p>This handbook is organized for readers coming from multiple fields (software, data engineering, ML, physics, pure math). It prioritizes (1) reproducible computation, (2) explicit definitions, and (3) clear separation between formal results and interpretive analogies. If you only read one section first: Start Here \u2192 Worked Example (Vector DB) \u2192 Artifact Reference \u2192 Nomenclature.</p>"},{"location":"handbook/#start-here","title":"Start Here","text":"<p>HUF is a normalization\u2011invariant formalism for analyzing and stabilizing hierarchical mixtures. Given any system that produces weighted contributions across regimes (sources, namespaces, components, experts, tenants), HUF asks: which regimes dominate, how concentrated is dominance, and how stable is the mix under small perturbations? In practice, HUF consumes a simple JSONL log of retrieval results (or any scored items with a regime label) and emits a small set of artifacts: (1) a coherence map, (2) an active set, (3) a trace report, and (4) an error budget. Core promise: results are meaningful even when upstream scores are rescaled, normalized differently, or mixed across sources\u2014because the analysis is designed to be invariant to normalization choices.</p>"},{"location":"handbook/#what-huf-is-and-is-not","title":"What HUF Is (and Is Not)","text":"<p>HUF is NOT a vector database, not a reranker, and not a replacement for your retrieval or modeling stack. It is an audit and stability layer that evaluates the composition of outputs across regimes. HUF IS useful when you suspect: a tenant dominates a multi\u2011tenant system, one data source overwhelms others, a pipeline is brittle under small score shifts, or overall metrics look good while a subset silently collapses.</p>"},{"location":"handbook/#formal-core","title":"Formal Core","text":""},{"location":"handbook/#objects-regimes-and-weights","title":"Objects, Regimes, and Weights","text":"<p>An item is a returned unit (document, record, event, sample). Each item i has: - a regime label r(i) (e.g., namespace, tenant, source, modality) - a non\u2011negative score s(i) (e.g., similarity, relevance, likelihood) - optional metadata (rank, query id, timestamp, etc.) Let R be the set of regimes. Define regime mass: - w_r = \u03a3_{i: r(i)=r} s(i) Define the normalized regime distribution (a point on the probability simplex): - p_r = w_r / \u03a3_{r\u2208R} w_r HUF computations are designed to depend on p (and derived geometric quantities), not on the absolute scale of scores.</p>"},{"location":"handbook/#category-theoretic-spine","title":"Category-Theoretic Spine","text":"<p>A minimal formalization treats HUF instances as objects in a category where morphisms preserve the structure relevant to normalization\u2011invariant analysis. - Definition (HUF object). A HUF object is a pair (R, p) where R is a finite set of regimes and p is a distribution on R (p_r \u2265 0, \u03a3 p_r = 1). - Definition (structure-preserving morphism). A morphism f: (R, p) \u2192 (R', p') is a regime map f: R \u2192 R' such that p' is the pushforward of p under f (i.e., p'{r'} = \u03a3 p_r). Intuition: merging regimes (kb + manual \u2192 docs) is a morphism; splitting regimes is not (unless you introduce additional structure). This captures \u201csum-preserving\u201d behavior: total mass is conserved under the regime mapping, and composition behaves as expected.</p>"},{"location":"handbook/#normalization-invariance","title":"Normalization Invariance","text":"<p>Most pipelines rescale scores: cosine similarity, distances, logits, calibrated probabilities, min\u2011max scaling, temperature scaling, etc. HUF is designed so that its regime-level conclusions are stable under monotone rescalings that do not change the relative ordering within a query, and under global multiplicative scaling. - Claim (informal). If scores are scaled by a positive constant \u03b1&gt;0 (s(i)\u2192\u03b1 s(i)), then p is unchanged and all regime-level statistics derived from p are unchanged. - Note. For more general rescalings, invariance holds for the parts of the pipeline that depend only on normalized regime mass p (not on raw s).</p>"},{"location":"handbook/#sphere-embedding-and-geometry","title":"Sphere Embedding and Geometry","text":"<p>The normalized distribution p lives on the simplex. A useful embedding maps p to the unit sphere via elementwise square root: - \u03c6(p) = (\u221ap_1, \u2026, \u221ap_|R|) \u2208 S^{|R|-1} This embedding connects to well-known statistical geometry (Hellinger distance / Fisher\u2013Rao metric). In HUF, it provides a convenient geometry for defining contraction/stability measures and for visualizing regime mixtures.</p>"},{"location":"handbook/#stability-and-lyapunov-view","title":"Stability and Lyapunov View","text":"<p>HUF tracks whether the regime mixture is stable under small perturbations (e.g., small score changes, small dataset shifts). A practical stability packet reports how the active set and dominance metrics change under controlled noise or parameter sweeps (\u03c4 variants). - Lyapunov-style heuristic. If an update rule on \u03c6(p) is contractive (reduces a suitable distance on the sphere), the mixture converges to a stable regime composition.</p>"},{"location":"handbook/#interpretive-extensions-explicitly-non-equivalence","title":"Interpretive Extensions (Explicitly Non-Equivalence)","text":"<p>The following sections are structural correspondences and embedding interpretations. They are included because they help experts connect HUF to familiar frameworks, but they are not claimed as formal equivalences unless a full construction is given. - Sheaf analogy: regimes as open covers; local consistency vs global coherence. - Simplicial analogy: mixtures across regimes form simplices; refinement corresponds to regime partitioning. - Topos analogy: internal logic of a regime-indexed system; useful as metaphor unless axiomatized. - QEC / trace-preserving analogy: error budgets and discard tracking resemble conserved quantities; again, analogy unless formalized. - GNN analogy: aggregation over a regime graph; helpful for implementation mapping.</p>"},{"location":"handbook/#worked-examples","title":"Worked Examples","text":""},{"location":"handbook/#vector-db-coherence-multi-namespace-retrieval","title":"Vector DB Coherence (multi-namespace retrieval)","text":"<p>Goal: detect whether one namespace dominates retrieval results even when overall ranking metrics appear fine. Expected outcome (example run): top regimes by rho_global_post show dominance split across two regimes. In a sample run, 'kb' and 'tickets' appeared with rho \u2248 0.62 and 0.38 respectively (your console output). What you get: - artifact_1_coherence_map.csv \u2014 regime-level dominance/coherence - artifact_2_active_set.csv \u2014 retained items after thresholding - artifact_3_trace_report.jsonl \u2014 explainability trace - artifact_4_error_budget.json \u2014 discards and accounting</p>"},{"location":"handbook/#planck-70-ghz-scientific-data","title":"Planck 70 GHz (scientific data)","text":"<p>Goal: demonstrate that the same audit/stability machinery works on scientific maps by treating map contributions as regime mixtures after binning/resolution changes. Expected outcome (example run): a compact coherence map across regimes, an active set of retained items, and generated plots (concentration curve, coherence by regime) when plotting is enabled. Example run metadata from a successful invocation: - active_set \u2248 18198, coherence_rows \u2248 12 - retained_target = 0.97, nside_out = 64 - discarded_global \u2248 0.03</p>"},{"location":"handbook/#artifact-reference","title":"Artifact Reference","text":"<p>HUF writes a directory of artifacts under --out. The artifact numbering is stable so downstream tools and documentation can rely on it.</p> Artifact File What it answers 1 artifact_1_coherence_map.csv Which regimes dominate, and by how much (global/local coherence). 2 artifact_2_active_set.csv Which items are retained after \u03c4/retention policy; ranks and regime labels. 3 artifact_3_trace_report.jsonl Per-item trace: why retained/discarded; attribution fields. 4 artifact_4_error_budget.json Accounting of discards: global/local/declared; summary stats."},{"location":"handbook/#nomenclature-symbols-variables-and-definitions","title":"Nomenclature (Symbols, Variables, and Definitions)","text":"<p>This section is intended to make the math readable and the code searchable.</p> Symbol / term Meaning Where you see it R Set of regimes (namespaces/tenants/sources). Coherence map; regime-field. i Item index. Active set, trace report. s(i) Raw score for item i (non-negative after any transform). Input JSONL. w_r Regime mass \u03a3 s(i) within regime r. Internal; coherence. p_r Normalized regime mass w_r / \u03a3 w. Core object distribution. \u03c4 (tau) Global retention / threshold parameter controlling how much tail mass is discarded. CLI flags; meta.json. \u03c1_global_post Post-normalization global coherence/dominance statistic over regimes. artifact_1_coherence_map.csv \u03c1_local_post Local coherence statistic within a regime (item-level concentration). active set/coherence map items_to_cover_90pct Smallest number of items whose cumulative mass reaches 90% (concentration proxy). inspect scripts; tail summary. discarded_global Fraction of mass removed by global policy. meta.json; error_budget."},{"location":"handbook/#cross-platform-execution-notes","title":"Cross-Platform Execution Notes","text":"<p>HUF examples are runnable on Windows PowerShell, macOS, and Linux. Be careful: shell syntax differs. In particular, PowerShell does not support Bash heredocs (&lt;&lt;'PY'). Use either (a) a Python script file, (b) python -c \"...\", or (c) PowerShell here-strings (@' ... '@). If you see errors like \u201cThe '&lt;' operator is reserved for future use\u201d, you pasted Bash syntax into PowerShell. Recommendation: use the provided scripts in scripts/ and examples/ rather than pasting multi-line Python into a shell.</p>"},{"location":"handbook/#troubleshooting","title":"Troubleshooting","text":"<p>MkDocs warning about \u201cMkDocs 2.0 incompatible with Material\u201d means your MkDocs major version and the theme version are mismatched. If you use Material, prefer the pinned versions in this repo\u2019s docs requirements. If you upgrade MkDocs, upgrade the theme accordingly (or switch generators). If documentation pages exist but are \u201cnot included in nav\u201d, run scripts/docs_hygiene.py (it normalizes nav and updates the catalog).</p>"},{"location":"huf_math_form_and_function/","title":"Mathematical form and function","text":"<p>\\</p>"},{"location":"huf_math_form_and_function/#higgins-unity-framework-mathematical-form-and-function","title":"Higgins Unity Framework: Mathematical form and function","text":"<p>This page is the \u201cmath spine\u201d that connects the HUF contract (artifacts + stability packet) to a minimal formal model.</p> <p>Not ML class imbalance: here \u201clong tail\u201d means mass distribution + exception reweighting (baseline vs filtered view).</p>"},{"location":"huf_math_form_and_function/#1-the-object-huf-normalizes","title":"1) The object HUF normalizes","text":"<p>HUF starts from a non\u2011negative contribution table and turns it into a unity budget.</p> <ul> <li>Finite elements: indices (i = 1..N)</li> <li>Raw contributions: (w_i \\ge 0)</li> <li>Unity\u2011budget weights (\u201cmass share\u201d or \u201cenergy share\u201d):</li> <li>Mass: (\\rho_i = \\frac{w_i}{\\sum_k w_k})</li> <li>Energy / Parseval: (\\rho_i = \\frac{|x_i|^2}{\\sum_k |x_k|^2})</li> </ul>"},{"location":"huf_math_form_and_function/#proof-sketch-unity-is-enforced","title":"Proof sketch: unity is enforced","text":"<p>Let (S = \\sum_k w_k). Then: - (\\rho_i \\ge 0) because (w_i \\ge 0) - (\\sum_i \\rho_i = \\sum_i \\frac{w_i}{S} = \\frac{1}{S}\\sum_i w_i = 1)</p> <p>This is the core invariant HUF treats as sacred: global unity.</p>"},{"location":"huf_math_form_and_function/#2-regimes-local-views-that-stay-auditable","title":"2) Regimes (local views that stay auditable)","text":"<p>A regime is a partition (or nested partition) of finite elements: - Regimes (R_1, \\dots, R_m) where (R_j \\subseteq {1..N}) - Regime mass:   [   \\rho(R_j) = \\sum_{i \\in R_j} \\rho_i   ]</p> <p>The coherence map artifact is exactly \u201c(\\rho(R_j)) for all regimes\u201d plus ranking, so you can answer:</p> <p>\u201cWhere did the budget go?\u201d</p>"},{"location":"huf_math_form_and_function/#3-exclusion-reduction-as-a-truncation-operator","title":"3) Exclusion / reduction as a truncation operator","text":"<p>Most practical HUF runs reduce the system to an auditable subset.</p>"},{"location":"huf_math_form_and_function/#31-threshold-form","title":"3.1 Threshold form (\u03c4)","text":"<p>Define a truncation operator: [ T_\\tau(\\rho)_i = \\rho_i \\cdot \\mathbf{1}[\\rho_i \\ge \\tau] ]</p> <p>Discarded budget (explicitly reported in <code>artifact_4_error_budget.json</code>): [ \\delta(\\tau) = 1 - \\sum_i T_\\tau(\\rho)_i ]</p> <p>Let (K(\\tau) = { i : \\rho_i \\ge \\tau }) be the retained set and (Z(\\tau)=\\sum_{k\\in K(\\tau)}\\rho_k = 1-\\delta(\\tau)).</p> <p>Renormalized retained distribution: [ \\hat\\rho_i(\\tau) = \\frac{\\rho_i}{Z(\\tau)} \\quad \\text{for } i \\in K(\\tau) ]</p> <p>Proof sketch (renormalized unity): [ \\sum_{i \\in K(\\tau)} \\hat\\rho_i(\\tau) = \\frac{\\sum_{i \\in K(\\tau)} \\rho_i}{Z(\\tau)} = 1 ]</p> <p>This is why HUF can be \u201ccompression + audit\u201d: you can discard mass, but you must (1) declare it, and (2) renormalize what remains.</p>"},{"location":"huf_math_form_and_function/#32-retainedtarget-form","title":"3.2 Retained\u2011target form (\u03b1)","text":"<p>Sometimes you don\u2019t choose (\\tau) directly. You choose a retained target (\\alpha) (e.g. 0.90), and HUF keeps the smallest set (K) such that: [ \\sum_{i \\in K} \\rho_i \\ge \\alpha ]</p> <p>Operationally: sort by (\\rho_i) descending; take the shortest prefix that reaches (\\alpha).</p> <p>This is exactly the \u201citems to cover 90%\u201d headline used in the long\u2011tail demo: - baseline run: items_to_cover_90pct = 37 - anomaly run: items_to_cover_90pct = 12 \u2192 concentration increased.</p>"},{"location":"huf_math_form_and_function/#4-fixed-points-why-the-cycle-is-stable","title":"4) Fixed points (why the cycle is stable)","text":""},{"location":"huf_math_form_and_function/#41-normalization-is-idempotent-on-the-simplex","title":"4.1 Normalization is idempotent on the simplex","text":"<p>Define normalization: [ N(w) = \\frac{w}{\\sum_k w_k} ]</p> <p>If (\\rho) already satisfies (\\sum \\rho = 1), then: [ N(\\rho) = \\rho ] So all unity\u2011budget distributions are fixed points of (N).</p>"},{"location":"huf_math_form_and_function/#42-a-lyapunov-view-stability-to-unity","title":"4.2 A Lyapunov view (stability to unity)","text":"<p>A simple \u201cdistance to unity\u201d function: [ V(\\rho) = 1 - \\sum_i \\rho_i ] After normalization, (V(\\rho)=0). Any drift away from unity is measurable, correctable, and auditable.</p> <p>In practice, HUF treats \u201cunity drift\u201d as a bug: either a numerical issue (float accumulation) or a forbidden operation (non\u2011conservative propagation).</p>"},{"location":"huf_math_form_and_function/#5-information-theory-optional-but-useful","title":"5) Information theory (optional, but useful)","text":"<p>Once (\\rho) is a probability\u2011like distribution, information tools become usable:</p>"},{"location":"huf_math_form_and_function/#51-entropy-as-concentration","title":"5.1 Entropy as \u201cconcentration\u201d","text":"<p>[ H(\\rho) = -\\sum_i \\rho_i \\log \\rho_i ]</p> <ul> <li>High entropy \u2192 diffuse mass (less concentrated)</li> <li>Low entropy \u2192 concentrated mass (few items dominate)</li> </ul> <p>A friendly scalar is the effective number of items: [ N_{\\mathrm{eff}} = \\exp(H(\\rho)) ] When your anomaly run concentrates, (H) tends to drop and (N_{\\mathrm{eff}}) shrinks.</p>"},{"location":"huf_math_form_and_function/#52-regime-shift-as-divergence","title":"5.2 Regime shift as divergence","text":"<p>Let (\\rho^{(base)}) be baseline (Traffic Phase) and (\\rho^{(anom)}) be exception view (Traffic Anomaly). A regime\u2011shift measure is KL divergence: [ D_{KL}(\\rho^{(anom)} \\Vert \\rho^{(base)}) = \\sum_i \\rho^{(anom)}_i \\log \\frac{\\rho^{(anom)}_i}{\\rho^{(base)}_i} ] You don\u2019t need this to run HUF \u2014 but it\u2019s a clean way to quantify \u201cthe mass moved\u201d.</p>"},{"location":"huf_math_form_and_function/#6-how-this-maps-to-the-three-artifact-first-pillars","title":"6) How this maps to the three \u201cartifact-first\u201d pillars","text":"<p>HUF\u2019s elevator pitch is not \u201cmath for math\u2019s sake.\u201d It\u2019s math that forces auditability:</p> <ul> <li>Coherence map \u2192 (\\rho(R_j)) (where the budget went, by regime)</li> <li>Active set \u2192 (K) (what you kept, explicitly)</li> <li>Trace report \u2192 provenance map (why each kept item is kept, and what it came from)</li> </ul> <p>If you can\u2019t show all three, you\u2019re not doing HUF \u2014 you\u2019re doing storytelling.</p>"},{"location":"huf_math_form_and_function/#math-appendix-artifact-columns-formulas","title":"Math appendix: artifact columns \u2194 formulas","text":"<p>This section lets you jump from an artifact column name to the math above.</p> <p>Important notes: - Column presence varies by adapter, but these names are common across HUF runs. - Suffix meanings are consistent:   - <code>*_pre</code> \u2192 computed on the original unity budget (\\rho)   - <code>*_post</code> \u2192 computed after exclusions + renormalization on the retained distribution (\\hat\\rho) - Let (K) be the retained set, (Z=\\sum_{k\\in K}\\rho_k=1-\\delta).</p>"},{"location":"huf_math_form_and_function/#a-element-level-columns-typical-in-artifact_2_active_setcsv","title":"A) Element-level columns (typical in <code>artifact_2_active_set.csv</code>)","text":"Column Formula Meaning <code>rho_global_pre</code> (\\rho_i) Element\u2019s share of the original unity budget <code>rho_global_post</code> (\\hat\\rho_i=\\rho_i/Z) for (i\\in K) Element\u2019s share after exclusion + renormalization <code>rho_local_pre</code> (\\rho_i / \\rho(R)) where (i\\in R) Element\u2019s share within its regime (pre) <code>rho_local_post</code> (\\hat\\rho_i / \\hat\\rho(R)) Element\u2019s share within its regime (post) <p>Where: - (\\rho(R)=\\sum_{j\\in R}\\rho_j) - (\\hat\\rho(R)=\\sum_{j\\in R\\cap K}\\hat\\rho_j = \\frac{\\sum_{j\\in R\\cap K}\\rho_j}{Z})</p> <p>A useful simplification: [ \\rho_local_post(i\\in R) = \\frac{\\rho_i}{\\sum_{j\\in R\\cap K}\\rho_j} ] (renormalization cancels (Z) inside the regime).</p> <p>Practical reading: <code>rho_global_post</code> answers \u201chow important is this element overall (after pruning)?\u201d <code>rho_local_post</code> answers \u201chow dominant is this element inside its regime?\u201d</p>"},{"location":"huf_math_form_and_function/#b-regime-level-columns-typical-in-artifact_1_coherence_mapcsv","title":"B) Regime-level columns (typical in <code>artifact_1_coherence_map.csv</code>)","text":"Column Formula Meaning <code>rho_global_pre</code> (\\rho(R)=\\sum_{i\\in R}\\rho_i) Regime share in the original distribution <code>rho_global_post</code> (\\hat\\rho(R)=\\sum_{i\\in R\\cap K}\\rho_i/Z) Regime share after exclusion + renormalization <code>rho_discarded_pre</code> (\\sum_{i\\in R\\setminus K}\\rho_i) Amount of original mass discarded inside the regime <p>So the coherence map can be read as: - who dominates now (<code>rho_global_post</code>) - how much tail was cut away (<code>rho_discarded_pre</code>)</p>"},{"location":"huf_math_form_and_function/#c-error-budget-artifact_4_error_budgetjson","title":"C) Error budget (<code>artifact_4_error_budget.json</code>)","text":"<p>Common keys and their math:</p> Key Formula Meaning <code>discarded_budget_global</code> (or similar) (\\delta = 1 - \\sum_{i\\in K}\\rho_i) Total mass discarded globally <code>retained_budget_global</code> (if present) (Z = \\sum_{i\\in K}\\rho_i = 1-\\delta) Total mass retained before renormalization"},{"location":"huf_math_form_and_function/#d-the-items-to-cover-90-headline-used-in-demos","title":"D) The \u201citems to cover 90%\u201d headline (used in demos)","text":"<p>Given active set rows sorted by <code>rho_global_post</code> descending, define: [ k_{0.90} = \\min \\left{k : \\sum_{i=1}^k \\rho^{post}_{(i)} \\ge 0.90 \\right} ]</p> <p>This is what the demo prints as:</p> <ul> <li><code>items_to_cover_90pct baseline -&gt; exception</code></li> </ul> <p>If (k_{0.90}) shrinks in the exception view, you can legitimately say:</p> <p>\u201cConcentration increased.\u201d</p>"},{"location":"huf_math_form_and_function/#e-trace-report-artifact_3_trace_reportjsonl","title":"E) Trace report (<code>artifact_3_trace_report.jsonl</code>)","text":"<p>The trace report isn\u2019t one formula; it\u2019s the provenance map for each retained item: - which input rows/cells formed the finite element, - what regime labels were assigned, - what exclusions applied, - what post-normalized shares were computed.</p> <p>Think of it as a serialized proof object that supports the numbers above.</p>"},{"location":"huf_math_form_and_function/#next","title":"Next","text":"<ul> <li>For the accounting\u2011facing explanation and the baseline\u2192exception\u2192variance flow, see:</li> <li>Long tail (accounting lens) (<code>docs/long_tail_accounting_lens.md</code>)</li> <li>For command discovery and artifact labels, see:</li> <li>CLI command lists + terminology (<code>docs/cli_huf_reference.md</code>)</li> </ul>"},{"location":"huf_reference/","title":"HUF Reference (Skeptic Appendix) \u2014 v1.3.0","text":"<p>Build date: 2026-02-21 Canonical public reference (Markdown). This supersedes prior skeptic/reference variants.</p> <p>Ethics disclosure (short)</p> <p>Drafted with AI assistance as an editing tool and reviewed/curated by the author. Formal claims are stated conservatively and should be reproducible via repository code where applicable. See Ethics.</p>"},{"location":"huf_reference/#key-intuition-why-loss-can-make-retained-stronger","title":"Key intuition: why 'loss' can make retained stronger","text":"<p>If you keep only a retained subset and then re-normalize, every retained weight is scaled by the same factor <code>1 / retained_mass</code>.</p> <p>Example:</p> <ul> <li>start: <code>[0.60, 0.30, 0.10]</code> (sums to 1)</li> <li>discard the last <code>0.10</code> tail \u2192 retained mass is <code>0.90</code></li> <li>re-normalize retained \u2192 <code>[0.60/0.90, 0.30/0.90] = [0.667, 0.333]</code></li> </ul> <p>Nothing \u201cmystical\u201d happened: you\u2019re now looking at the mix conditional on retention. HUF makes this explicit and auditable (trace report + error budget).</p> <p>Purpose: help technically skeptical readers quickly locate what is proved, what is measured, and what is presented as analogy.</p>"},{"location":"huf_reference/#1-what-is-the-core-claim","title":"1) What is the core claim?","text":"<p>Core claim: HUF provides normalization\u2011invariant diagnostics of regime dominance and concentration for any scored mixture output, and packages the results as auditable artifacts.</p>"},{"location":"huf_reference/#2-is-this-just-normalize-weights-on-the-simplex","title":"2) Is this just \u2018normalize weights on the simplex\u2019?","text":"<p>At the base layer, yes: HUF explicitly constructs p on the simplex. The contribution is (a) a consistent artifact protocol, (b) dominance/coherence statistics and concentration summaries that are easy to operationalize, and (c) a stability workflow (\u03c4 sweeps, deltas, trace reports) that turns \u2018mixture drift\u2019 into something you can track and regress-test.</p>"},{"location":"huf_reference/#3-what-is-actually-proved-vs-suggested","title":"3) What is actually proved vs. suggested?","text":"<p>Proved/derivable (within the handbook\u2019s scope): - scale invariance under s\u2192\u03b1s (\u03b1&gt;0) at the regime distribution level. - functorial behavior of regime merging as pushforward on distributions. - geometric embedding \u03c6(p) on the sphere, connecting to standard statistical geometry. Presented as interpretations (not equivalences unless formalized): sheaf/topos/simplicial analogies; QEC analogies; Langlands/arithmetic geometry claims. These should be treated as research directions until fully constructed.</p>"},{"location":"huf_reference/#4-where-is-the-universal-property","title":"4) Where is the universal property?","text":"<p>If a universal property is claimed, it must be stated in category language: e.g., \u201c(R,p) is initial among objects receiving a regime-merging map from a given mixture log under pushforward constraints.\u201d In v1.3.0, we do not claim a strong universal property beyond the standard pushforward construction.</p>"},{"location":"huf_reference/#5-does-the-sphere-mapping-add-anything-beyond-visualization","title":"5) Does the sphere mapping add anything beyond visualization?","text":"<p>It can, when used as a geometry for stability/contraction analysis or for defining distances between mixtures. Without those steps, it is indeed mostly a convenient coordinate transform. The handbook therefore frames it as \u2018geometry support\u2019 for stability workflows.</p>"},{"location":"huf_reference/#6-what-is-the-computational-advantage","title":"6) What is the computational advantage?","text":"<p>Operational advantage: you can monitor regime dominance and concentration as first-class metrics, and attach traceable artifacts to PRs, incident reports, or eval dashboards. This is valuable precisely when scalar retrieval metrics (NDCG/MRR) hide distributional collapse across sources/tenants.</p>"},{"location":"huf_reference/#7-what-should-i-verify-if-im-reviewing-this","title":"7) What should I verify if I\u2019m reviewing this?","text":"<ul> <li>Reproduce the vector_db example end-to-end; confirm the coherence map reflects your namespaces.</li> <li>Run a \u03c4 sweep; confirm concentration and active_set size change sensibly.</li> <li>Inspect artifact_3_trace_report.jsonl; confirm discards are explainable and consistent.</li> <li>Confirm invariance: multiply all input scores by 10 and verify regime-level outputs are unchanged.</li> </ul>"},{"location":"huf_reference/#8-common-failure-modes","title":"8) Common failure modes","text":"<ul> <li>Mixing shell syntaxes (Bash heredoc in PowerShell) \u2192 parse errors.</li> <li>Including generated outputs in git (site/, out/) \u2192 repo looks like HTML, slows clones, confuses reviewers.</li> <li>Unpinned doc dependencies \u2192 MkDocs/Material warnings or build failures.</li> </ul>"},{"location":"huf_reference/#9-limits-and-non-goals","title":"9) Limits and non-goals","text":"<p>HUF does not claim to improve retrieval quality directly; it diagnoses composition and stability. It does not claim that analogies (topos, Langlands, QEC) are theorems. Those belong in separate, peer-reviewable technical notes.</p>"},{"location":"huf_reference/#10-how-to-cite-or-discuss-responsibly","title":"10) How to cite or discuss responsibly","text":"<p>Recommended phrasing: - \u201cHUF is a normalization-invariant audit layer for regime dominance and concentration.\u201d - \u201cWe use sphere embedding of simplex distributions to visualize and analyze mixture stability.\u201d - \u201cWe discuss sheaf/topos analogies as interpretive correspondences, not equivalences.\u201d</p>"},{"location":"jupyter_demos/","title":"Jupyter demos (optional)","text":"<p>Jupyter is optional. It\u2019s useful when you want to read and summarize artifacts interactively.</p>"},{"location":"jupyter_demos/#install-launch-windows-powershell","title":"Install + launch (Windows PowerShell)","text":"<pre><code>.\\.venv\\Scripts\\python -m pip install notebook pandas\n.\\.venv\\Scripts\\python -m notebook\n</code></pre> <p>A browser window opens. Create a new notebook (Python).</p>"},{"location":"jupyter_demos/#important-powershell-is-not-python","title":"Important: PowerShell is not Python","text":"<p>If you type <code>import pandas as pd</code> at a PowerShell prompt, it will fail.</p> <p>Run Python code: - in a notebook cell, or - inside <code>python</code> interactive (<code>.\\.venv\\Scripts\\python</code>), or - from a script file.</p>"},{"location":"jupyter_demos/#suggested-notebook-pattern","title":"Suggested notebook pattern","text":""},{"location":"jupyter_demos/#cell-1-run-a-case-cli","title":"Cell 1 \u2014 run a case (CLI)","text":"<pre><code>import subprocess, sys\nsubprocess.check_call([sys.executable, \"-m\", \"huf_core.cli\", \"--help\"])\n</code></pre> <p>(Or just run the case in PowerShell first, then open artifacts in the next cells.)</p>"},{"location":"jupyter_demos/#cell-2-open-artifacts-markham","title":"Cell 2 \u2014 open artifacts (Markham)","text":"<pre><code>import pandas as pd\n\ncoh = pd.read_csv(\"out/markham2018/artifact_1_coherence_map.csv\")\nactive = pd.read_csv(\"out/markham2018/artifact_2_active_set.csv\").sort_values(\"rank\")\n\ncoh.sort_values(\"rho_global_post\", ascending=False).head(10)\n</code></pre>"},{"location":"jupyter_demos/#cell-3-how-many-items-cover-90","title":"Cell 3 \u2014 \u201chow many items cover 90%?\u201d","text":"<pre><code>active[\"cum\"] = active[\"rho_global_post\"].cumsum()\nactive.loc[active[\"cum\"] &gt;= 0.90, [\"rank\",\"item_id\",\"cum\"]].head(1)\n</code></pre>"},{"location":"jupyter_demos/#cell-4-open-artifacts-traffic-phase","title":"Cell 4 \u2014 open artifacts (Traffic Phase)","text":"<pre><code>import pandas as pd\n\ncoh = pd.read_csv(\"out/traffic_phase/artifact_1_coherence_map.csv\")\nactive = pd.read_csv(\"out/traffic_phase/artifact_2_active_set.csv\").sort_values(\"rank\")\n\ncoh.sort_values(\"rho_global_post\", ascending=False).head(15)\n</code></pre>"},{"location":"jupyter_demos/#export","title":"Export","text":"<p>You can export notebooks to HTML/PDF from the Jupyter UI (File \u2192 Download).</p>"},{"location":"learning_path/","title":"Learning path","text":"<p>HUF is easiest to learn by running a case, then reading the artifacts it produces. This path is designed so the left sidebar is a \u201cdo-this-next\u201d guide.</p> <p>Windows / Conda rule</p> <p>After the repo venv exists, always run tools via the repo executables:</p> <pre><code>.\\.venv\\Scripts\\python -V\n.\\.venv\\Scripts\\huf --help\n.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre>"},{"location":"learning_path/#step-1-install-first-run","title":"Step 1 \u2014 Install + first run","text":"<p>Choose one:</p> <ul> <li>Start here (developer): Start Here \u2192 Developer</li> <li>Start here (beginner): Start Here \u2192 Zero GitHub knowledge</li> </ul> <p>Goal: you can run <code>.\\.venv\\Scripts\\huf --help</code> and produce an <code>out/.../run_stamp.json</code>.</p>"},{"location":"learning_path/#step-2-run-the-two-core-cases","title":"Step 2 \u2014 Run the \u201ctwo core\u201d cases","text":"<p>1) Markham (budget allocation) \u2192 then read: - Worked examples \u2192 Markham</p> <p>2) Traffic Phase (signal phases) \u2192 then read: - Worked examples \u2192 Traffic phase</p>"},{"location":"learning_path/#step-3-understand-the-long-tail-story-accounting-lens","title":"Step 3 \u2014 Understand the \u201clong tail\u201d story (accounting lens)","text":"<ul> <li>Long tail (accounting lens)</li> </ul> <p>Then run the 2\u2011minute demo:</p> <pre><code>.\\.venv\\Scripts\\python scripts/run_long_tail_demo.py --status \"Green Termination\"\n</code></pre>"},{"location":"learning_path/#step-4-try-an-adapter-style-use-case","title":"Step 4 \u2014 Try an adapter-style use case","text":"<ul> <li>Adapters \u2192 Vector DB coherence</li> </ul>"},{"location":"learning_path/#step-5-optional-notebooks","title":"Step 5 \u2014 Optional: notebooks","text":"<ul> <li>Jupyter demos (optional)</li> </ul>"},{"location":"long_tail_accounting_lens/","title":"Long tail (accounting lens)","text":"<p>Two-line disambiguation (for skimmers):</p> <ul> <li>Not ML class imbalance. Here \u201clong tail\u201d means mass distribution + exception reweighting (baseline vs filtered view).  </li> <li>In practice: baseline ledger \u2192 exception-only sub-ledger \u2192 ranked variance review.</li> </ul> <p>HUF makes that workflow reproducible by writing artifacts (CSVs/JSON) that answer:</p> <ul> <li>Where is the mass? (coherence map)</li> <li>What do we keep for review? (active set)</li> <li>What did we discard, explicitly? (error budget)</li> <li>Why did this row/item survive? (trace)</li> </ul>"},{"location":"long_tail_accounting_lens/#why-the-tail-becomes-the-story","title":"Why the \u201ctail\u201d becomes the story","text":""},{"location":"long_tail_accounting_lens/#1-long-tail-reweighting","title":"1) Long-tail reweighting","text":"<p>In a baseline ledger, most categories look \u201csmall\u201d and ignorable.</p> <p>But the moment you filter to a rare event type (exceptions), the distribution reweights:</p> <ul> <li>a small set of rows can dominate the review list</li> <li>regimes you barely noticed can become the story</li> </ul> <p>This is the accounting intuition behind \u201cthe long tail\u201d: exceptions make the tail visible.</p>"},{"location":"long_tail_accounting_lens/#2-non-linear-concentration","title":"2) Non-linear concentration","text":"<p>Filtering is non-linear.</p> <p>When you discard rows (tighten the review boundary), probability mass doesn\u2019t shrink uniformly \u2014 it concentrates.</p> <p>HUF exposes this concentration with one repeatable headline:</p> <p><code>items_to_cover_90pct = k</code> The top k retained items explain 90% of post-normalized mass.</p> <p>Smaller <code>k</code> \u21d2 more concentrated \u21d2 a tiny set dominates.</p>"},{"location":"long_tail_accounting_lens/#3-auditability","title":"3) Auditability","text":"<p>Exception review is only useful if you can defend it.</p> <p>HUF provides:</p> <ul> <li>a regime ranking (who dominates)</li> <li>a ranked review list (what to look at first)</li> <li>a discard ledger (what was dropped, and how much \u201cbudget\u201d that represents)</li> <li>a trace report (why retained)</li> </ul>"},{"location":"long_tail_accounting_lens/#traffic-phase-vs-traffic-anomaly","title":"Traffic Phase vs Traffic Anomaly","text":"<p>This is the simplest \u201caccounting-style\u201d teaching example in the repo.</p>"},{"location":"long_tail_accounting_lens/#baseline-view-traffic-phase","title":"Baseline view (Traffic Phase)","text":"<p>Traffic Phase is like baseline P&amp;L:</p> <ul> <li>you observe routine operations</li> <li>mass is spread across many rows</li> <li>the \u201ctail\u201d looks harmless</li> </ul>"},{"location":"long_tail_accounting_lens/#exception-view-traffic-anomaly","title":"Exception view (Traffic Anomaly)","text":"<p>Traffic Anomaly is like exception-only P&amp;L:</p> <ul> <li>you filter to a rare/important condition (an anomaly)</li> <li>the distribution reweights</li> <li>mass concentrates into fewer regimes/items</li> </ul>"},{"location":"long_tail_accounting_lens/#ranked-variance-review","title":"Ranked variance review","text":"<p>Once you have baseline and exception views, the operational question is:</p> <p>What changed the most, and where should we look first?</p> <p>HUF\u2019s artifacts answer that with ranking + a proof line.</p>"},{"location":"long_tail_accounting_lens/#two-minute-demo-script-first-windows-safe","title":"Two-minute demo (script-first, Windows-safe)","text":"<p>Run in PowerShell (not inside Python)</p> <p>If your prompt shows <code>&gt;&gt;&gt;</code>, exit Python with <code>exit()</code>.</p> <pre><code>.\\.venv\\Scripts\\python scripts/run_long_tail_demo.py --status \"Green Termination\"\n</code></pre> <p>What the demo does:</p> <p>1) Runs Traffic Phase (baseline) 2) Runs Traffic Anomaly (exception) 3) Prints a repeatable summary you can paste into an email/issue</p> <p>Look for these lines:</p> <ul> <li><code>Top regimes changed: ...</code> </li> <li><code>PROOF: Concentration increased: items_to_cover_90pct X -&gt; Y</code></li> </ul> <p>Interpretation:</p> <ul> <li>Top regimes changed: your exception filter changed \u201cwho dominates\u201d</li> <li>Concentration increased (Y smaller): the exception view is dominated by fewer items \u2192 higher operational risk if those items are wrong/stale</li> </ul>"},{"location":"long_tail_accounting_lens/#accounting-example-same-pattern-different-nouns","title":"Accounting example (same pattern, different nouns)","text":"<p>Imagine an accounts payable ledger.</p>"},{"location":"long_tail_accounting_lens/#baseline-ledger-monthly-close","title":"Baseline ledger (monthly close)","text":"<p>Rows: all invoices. Regimes: cost center, vendor, GL account.</p> <p>You expect dispersion: many vendors, many cost centers.</p>"},{"location":"long_tail_accounting_lens/#exception-only-sub-ledger-audit-filter","title":"Exception-only sub-ledger (audit filter)","text":"<p>Filter rows to:</p> <ul> <li>invoices above a threshold</li> <li>invoices with a policy exception code</li> <li>invoices paid outside terms</li> </ul> <p>Now the tail becomes visible:</p> <ul> <li>a \u201csmall\u201d vendor becomes dominant</li> <li>one cost center suddenly explains most of the exception mass</li> </ul>"},{"location":"long_tail_accounting_lens/#ranked-variance-review-what-to-inspect-first","title":"Ranked variance review (what to inspect first)","text":"<p>You don\u2019t want a spreadsheet you scroll.</p> <p>You want:</p> <ul> <li>regime ranking (who dominates exceptions)</li> <li>active set (which invoices dominate)</li> <li>discard ledger (what got dropped, explicitly)</li> </ul> <p>That\u2019s exactly what HUF writes.</p>"},{"location":"long_tail_accounting_lens/#what-to-look-for-in-the-artifacts","title":"What to look for in the artifacts","text":"<p>This is the \u201copen in Excel and circle the row\u201d guide.</p>"},{"location":"long_tail_accounting_lens/#artifact_1_coherence_mapcsv-regime-ranking","title":"artifact_1_coherence_map.csv (regime ranking)","text":"<p>Sort by <code>rho_global_post</code> descending.</p> <p>Watch for:</p> <ul> <li>top regime &gt; 0.50 (dominance)</li> <li>top 2\u20133 regimes cover most of mass (concentration by group)</li> </ul>"},{"location":"long_tail_accounting_lens/#artifact_2_active_setcsv-ranked-review-list","title":"artifact_2_active_set.csv (ranked review list)","text":"<p>Sort by <code>rho_global_post</code> descending.</p> <p>This is your global review list.</p> <p>Then filter by <code>regime_id</code> and sort by <code>rho_local_post</code> for \u201ctop inside regime.\u201d</p>"},{"location":"long_tail_accounting_lens/#artifact_4_error_budgetjson-declared-discards","title":"artifact_4_error_budget.json (declared discards)","text":"<p>Find:</p> <ul> <li><code>discarded_budget_global</code> (or similarly named key)</li> </ul> <p>This is the explicit \u201cwhat we dropped\u201d ledger.</p> <p>If discarded budget is large, your threshold is aggressively pruning.</p>"},{"location":"long_tail_accounting_lens/#make-it-future-proof-how-teams-extend-this","title":"Make it future-proof (how teams extend this)","text":"<p>Once a team \u201cgets\u201d this page, they usually want:</p> <p>1) Multi-run drift    - run daily/weekly on the same queries    - track top regimes and <code>items_to_cover_90pct</code> over time</p> <p>2) Two-tau sensitivity    - run tau A vs tau B    - print \u201cconcentration increased\u201d as a regression guardrail</p> <p>3) Compliance / isolation checks    - treat tenant/namespace as regimes    - detect dominance bleed across boundaries</p> <p>If you\u2019re already using the Vector DB coherence adapter, this is the same mental model \u2014 only the nouns change.</p> <p>See also:</p> <ul> <li><code>vector_db_coherence.md</code> (regimes, artifacts, and the same \u201cproof line\u201d)</li> <li><code>math_form_and_function.md</code> (equations + column mapping)</li> </ul>"},{"location":"quick_run/","title":"Quick Run (copy/paste)","text":"<p>Goal: create a local <code>.venv</code>, fetch inputs, and run the included demos.</p> <p>Run these commands from the repo root (the folder that contains <code>pyproject.toml</code>).</p>"},{"location":"quick_run/#windows-powershell","title":"Windows (PowerShell)","text":""},{"location":"quick_run/#1-bootstrap-install","title":"1) Bootstrap + install","text":"<pre><code>python scripts/bootstrap.py\n.\\.venv\\Scripts\\python -m pip install -e .\n</code></pre>"},{"location":"quick_run/#2-fetch-markham-toronto-inputs","title":"2) Fetch Markham + Toronto inputs","text":"<pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --markham --toronto --yes\n</code></pre>"},{"location":"quick_run/#3-run-the-demos","title":"3) Run the demos","text":"<pre><code>.\\.venv\\Scripts\\huf markham --xlsx cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx --out out/markham2018\n.\\.venv\\Scripts\\huf traffic --csv cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_phase\n.\\.venv\\Scripts\\huf traffic-anomaly --csv cases/traffic_anomaly/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_anomaly --status \"Green Termination\" --tau-global 0.0005\n</code></pre>"},{"location":"quick_run/#4-two-minute-long-tail-demo-recommended","title":"4) Two-minute long-tail demo (recommended)","text":"<pre><code>.\\.venv\\Scripts\\python scripts/run_long_tail_demo.py --status \"Green Termination\"\n</code></pre>"},{"location":"quick_run/#5-docs-site-local","title":"5) Docs site (local)","text":"<p>Always:</p> <pre><code>.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre> <p>Strict check:</p> <pre><code>.\\.venv\\Scripts\\python -m mkdocs build --strict\n</code></pre>"},{"location":"quick_run/#macos-linux-bashzsh","title":"macOS / Linux (bash/zsh)","text":"<pre><code>python3 scripts/bootstrap.py\n./.venv/bin/python -m pip install -e .\n./.venv/bin/python scripts/fetch_data.py --markham --toronto --yes\n\n./.venv/bin/huf markham --xlsx cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx --out out/markham2018\n./.venv/bin/huf traffic --csv cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_phase\n./.venv/bin/huf traffic-anomaly --csv cases/traffic_anomaly/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_anomaly --status \"Green Termination\" --tau-global 0.0005\n</code></pre>"},{"location":"reference_manual/","title":"HUF Reference Manual","text":"<p>Updated: 2026-02-17</p> <p>This manual is the \u201chow to run it\u201d companion to the handbook. It\u2019s written for:</p> <ul> <li>GUI-only users (download a ZIP, double-click a Windows starter),</li> <li>researchers who live in Excel + theory,</li> <li>anyone who wants reproducible artifacts without learning Git on day one.</li> </ul>"},{"location":"reference_manual/#1-quick-start-windows-no-git-required","title":"1) Quick Start (Windows, no Git required)","text":""},{"location":"reference_manual/#option-a-easiest-github-release-zip","title":"Option A \u2014 easiest: GitHub Release ZIP","text":"<ol> <li>Download the ZIP from the project\u2019s GitHub Releases page.</li> <li>Unzip it somewhere simple (Desktop is fine).</li> <li>Double-click: <code>START_HERE_WINDOWS.bat</code></li> </ol> <p>What it does:</p> <ul> <li>creates a local virtual environment in <code>.venv</code></li> <li>installs HUF in editable mode (local)</li> <li>fetches Markham + Toronto inputs (unless you skip)</li> <li>prints the exact commands to run the demos</li> </ul> <p>Tip: If Windows shows a security warning the first time, click \u201cMore info\u201d \u2192 \u201cRun anyway\u201d.</p>"},{"location":"reference_manual/#option-b-github-desktop-recommended-once-youre-comfortable","title":"Option B \u2014 GitHub Desktop (recommended once you\u2019re comfortable)","text":"<p>Use GitHub Desktop to keep your folder synced with GitHub.</p> <p>Day-to-day:</p> <ul> <li>Fetch checks for updates.</li> <li>Pull downloads updates.</li> <li>Commit records your changes.</li> <li>Push/Sync uploads your changes.</li> </ul>"},{"location":"reference_manual/#2-fetching-input-data-real-public-sources","title":"2) Fetching input data (real public sources)","text":"<p>Run these from the repo root (the folder that contains <code>pyproject.toml</code>).</p>"},{"location":"reference_manual/#markham-2018-budget-allocation-xlsx","title":"Markham (2018 Budget Allocation XLSX)","text":"<pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --markham\n</code></pre> <p>Expected file:</p> <ul> <li><code>cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx</code></li> </ul>"},{"location":"reference_manual/#toronto-traffic-signals-timing-csv","title":"Toronto (Traffic signals timing \u2192 CSV)","text":"<p>Non-interactive default selection:</p> <pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --toronto --yes\n</code></pre> <p>Expected files:</p> <ul> <li><code>cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv</code></li> <li><code>cases/traffic_anomaly/inputs/toronto_traffic_signals_phase_status.csv</code></li> </ul> <p>Toronto schema expected by HUF traffic adapters:</p> <ul> <li>required: <code>TCS</code>, <code>PHASE</code></li> <li>optional: <code>PHASE_STATUS_TEXT</code>, <code>PHASE_CALL_TEXT</code></li> </ul>"},{"location":"reference_manual/#planck-guidedmanual","title":"Planck (guided/manual)","text":"<pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --planck-guide\n</code></pre> <p>You\u2019ll end up with a FITS file such as:</p> <ul> <li><code>cases/planck70/inputs/...70...fits</code></li> </ul>"},{"location":"reference_manual/#3-running-the-included-cases-windows-powershell","title":"3) Running the included cases (Windows PowerShell)","text":""},{"location":"reference_manual/#a-markham-2018-fund-weighted-expenditures","title":"A) Markham 2018 (fund-weighted expenditures)","text":"<pre><code>.\\.venv\\Scripts\\huf markham --xlsx cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx --out out/markham2018\n</code></pre>"},{"location":"reference_manual/#b-toronto-traffic-phase-band-extraction","title":"B) Toronto traffic phase (band extraction)","text":"<pre><code>.\\.venv\\Scripts\\huf traffic --csv cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_phase\n</code></pre>"},{"location":"reference_manual/#c-toronto-traffic-anomaly-share-hotspots","title":"C) Toronto traffic anomaly (share + hotspots)","text":"<pre><code>.\\.venv\\Scripts\\huf traffic-anomaly --csv cases/traffic_anomaly/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_anomaly --status \"Green Termination\"\n</code></pre>"},{"location":"reference_manual/#d-planck-70-ghz-map-coherence-stability","title":"D) Planck 70 GHz (map \u2192 coherence \u2192 stability)","text":"<pre><code>.\\.venv\\Scripts\\huf planck --fits cases/planck70/inputs/YOUR_70GHZ_MAP.fits --out out/planck70\n</code></pre>"},{"location":"reference_manual/#4-understanding-run_stampjson-your-reproducibility-receipt","title":"4) Understanding <code>run_stamp.json</code> (your reproducibility receipt)","text":"<p>HUF writes a stamp like:</p> <pre><code>{\n  \"dataset_id\": \"...\",\n  \"code_hash\": \"...\",\n  \"param_hash\": \"...\",\n  \"created_utc\": \"...\",\n  \"run_id\": \"...\"\n}\n</code></pre> <p>Interpretation:</p> <ul> <li><code>dataset_id</code> \u2014 identifier derived from the input file(s)</li> <li><code>code_hash</code> \u2014 identifier for the code state that produced artifacts</li> <li><code>param_hash</code> \u2014 identifier for your parameterization (\u03c4 grid, budgets, etc.)</li> <li><code>run_id</code> \u2014 unique run identifier</li> </ul> <p>If two runs have the same <code>dataset_id + code_hash + param_hash</code>, their artifacts should match (modulo timestamps).</p>"},{"location":"reference_manual/#5-troubleshooting-windows-focused","title":"5) Troubleshooting (Windows-focused)","text":""},{"location":"reference_manual/#ssl-certificate_verify_failed","title":"\u201cSSL: CERTIFICATE_VERIFY_FAILED\u201d","text":"<p>Try:</p> <pre><code>.\\.venv\\Scripts\\python -V\n.\\.venv\\Scripts\\python -m pip install certifi\n.\\.venv\\Scripts\\python scripts/fetch_data.py --toronto --yes\n</code></pre>"},{"location":"reference_manual/#http-error-404-during-toronto-fetch","title":"\u201cHTTP Error 404\u201d during Toronto fetch","text":"<p>Default:</p> <ul> <li><code>https://open.toronto.ca/api/3/action</code></li> </ul> <p>Override explicitly:</p> <pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --toronto --yes --toronto-ckan https://open.toronto.ca/api/3/action\n</code></pre>"},{"location":"reference_manual/#file-not-found-casesinputs","title":"\u201cFile not found \u2026 cases/.../inputs/...\u201d","text":"<p>Run fetch first:</p> <pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --markham --toronto --yes\n</code></pre>"},{"location":"reference_manual/#6-build-preview-the-docs-site","title":"6) Build / preview the docs site","text":"<pre><code>.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre> <p>Strict build check:</p> <pre><code>.\\.venv\\Scripts\\python -m mkdocs build --strict\n</code></pre>"},{"location":"repo_hygiene/","title":"Repo hygiene (clean commits)","text":"<p>If you publish <code>site/</code> (MkDocs output) or <code>out/</code> (run artifacts) into git, GitHub will treat the repo like a static site and the language bar becomes \u201cmostly HTML\u201d. This can reduce credibility for technical skimmers.</p> <p>This repo is set up to build <code>site/</code> in GitHub Actions and publish it to Pages \u2014 you should not commit <code>site/</code>.</p>"},{"location":"repo_hygiene/#quick-cleanup-recommended","title":"Quick cleanup (recommended)","text":"<p>1) Ensure <code>.gitignore</code> includes:</p> <ul> <li><code>site/</code></li> <li><code>out/</code></li> <li><code>*.egg-info/</code></li> <li><code>.venv/</code></li> </ul> <p>2) Remove generated folders from git tracking (keeps local files on disk):</p> <pre><code>git rm -r --cached site out huf_core.egg-info\ngit commit -m \"Repo hygiene: stop tracking generated outputs\"\ngit push\n</code></pre> <p>If you don\u2019t have <code>huf_core.egg-info</code> tracked, remove it from the command.</p>"},{"location":"repo_hygiene/#helper-script-dry-run-by-default","title":"Helper script (dry-run by default)","text":"<p>This repo includes a helper that detects tracked generated files and prints the exact commands:</p> <pre><code>.\\.venv\\Scripts\\python scripts/repo_cleanup.py\n</code></pre> <p>To apply automatically:</p> <pre><code>.\\.venv\\Scripts\\python scripts/repo_cleanup.py --apply\n</code></pre>"},{"location":"repo_hygiene/#why-this-matters","title":"Why this matters","text":"<ul> <li>Cleaner diffs</li> <li>Fewer merge conflicts</li> <li>Correct GitHub language stats (shows Python, not HTML)</li> <li>Faster PR review (no \u201cbuilt site\u201d noise)</li> </ul>"},{"location":"running_examples/","title":"Running examples on Windows, macOS, and Linux","text":"<p>This project is cross\u2011platform, but the shell syntax is not. Most \u201cit broke when I pasted it\u201d problems come from mixing:</p> <ul> <li>PowerShell commands (Windows) vs bash/zsh commands (macOS/Linux)</li> <li>Shell commands vs the Python REPL (<code>&gt;&gt;&gt;</code> prompt)</li> </ul> <p>This page gives copy/paste\u2011safe patterns and explains why each one exists.</p>"},{"location":"running_examples/#which-terminal-should-i-use","title":"Which terminal should I use?","text":"WindowsmacOS / Linux <p>Recommended:</p> <ul> <li>Windows Terminal + PowerShell 7+ (best copy/paste behavior)</li> <li>PowerShell 5.1 also works, but line continuations and paste handling are fussier.</li> </ul> <p>If pasting turns <code>.</code> into <code>[</code> or auto\u2011enters mid\u2011paste, update PSReadLine (this fixes most \u201cweird paste\u201d bugs):</p> <pre><code>Install-Module PSReadLine -Scope CurrentUser -Force -AllowClobber\n# close the terminal completely, then open a NEW PowerShell session\n</code></pre> <p>Use your default terminal with bash or zsh.</p> <ul> <li>Most docs and one\u2011liners online assume bash/zsh.</li> <li><code>python - &lt;&lt;'PY' ... PY</code> (heredoc) works here, not in PowerShell.</li> </ul>"},{"location":"running_examples/#shell-vs-python-how-to-tell-what-youre-typing-into","title":"Shell vs Python: how to tell what you\u2019re typing into","text":"<ul> <li> <p>If your prompt looks like:</p> </li> <li> <p><code>PS C:\\...&gt;</code> \u2192 you are in PowerShell</p> </li> <li><code>$</code> \u2192 you are in bash/zsh</li> <li><code>&gt;&gt;&gt;</code> \u2192 you are in Python (the REPL)</li> </ul> <p>Rule:</p> <ul> <li>Run <code>huf ...</code> and <code>python some_script.py ...</code> in the shell.</li> <li>Run <code>import ...</code> and <code>print(...)</code> inside Python, or via <code>python -c</code> / a script.</li> </ul> <p>Why this matters:</p> <ul> <li>In PowerShell, <code>print</code> is not Python \u2014 it\u2019s a shell command/alias that tries to print to a printer (the \u201cPRN\u201d device), which is why you saw:</li> </ul> <pre><code>Unable to initialize device PRN\n</code></pre>"},{"location":"running_examples/#recommended-pattern-run-the-provided-scripts-not-pasted-python","title":"Recommended pattern: run the provided scripts (not pasted Python)","text":"<p>The repo includes helper scripts like:</p> <ul> <li><code>scripts/inspect_huf_artifacts.py</code> (quick dashboard)</li> <li><code>scripts/run_vector_db_concentration_delta.py</code> (compare tau settings)</li> </ul> <p>These are intentionally paste\u2011friendly and avoid REPL confusion.</p>"},{"location":"running_examples/#multiline-python-snippets-when-you-really-need-them","title":"Multi\u2011line Python snippets (when you really need them)","text":""},{"location":"running_examples/#option-a-recommended-save-a-py-file","title":"Option A (recommended): save a <code>.py</code> file","text":"<p>This is the most reliable approach for beginners and teams.</p>"},{"location":"running_examples/#option-b-pipe-a-herestring-heredoc-into-python-","title":"Option B: pipe a here\u2011string / heredoc into <code>python -</code>","text":"Windows (PowerShell)macOS / Linux (bash/zsh) <p>PowerShell does not support bash heredocs (<code>&lt;&lt;'PY'</code>). Use a here\u2011string piped into Python:</p> <pre><code>$py = \".\\\\.venv\\\\Scripts\\\\python.exe\"\n\n@'\nimport sys\nprint(\"Hello from stdin\", sys.version)\n'@ | &amp; $py -\n</code></pre> <pre><code>./.venv/bin/python - &lt;&lt;'PY'\nimport sys\nprint(\"Hello from stdin\", sys.version)\nPY\n</code></pre> <p>What to expect:</p> <ul> <li><code>python -</code> means \u201cread the program from stdin\u201d.</li> <li>If you see syntax errors before Python starts, it\u2019s a shell syntax mismatch.</li> </ul>"},{"location":"running_examples/#worked-examples","title":"Worked examples","text":"<p>Below are the three examples most people start with. Each one has:</p> <ul> <li>Why run it (what question it answers)</li> <li>How to run it (Windows + macOS/Linux)</li> <li>What you should see (expected outputs)</li> <li>How to inspect/plot results</li> </ul>"},{"location":"running_examples/#example-1-vector-db-coherence-retrieval-audit","title":"Example 1 \u2014 Vector DB coherence (retrieval audit)","text":"<p>Why run it:</p> <ul> <li>Check whether a retrieval result set is dominated by one namespace/source/tenant.</li> <li>Quantify concentration (\u201chow many items explain 90% of the mass?\u201d).</li> </ul> <p>See the full walkthrough: Vector DB coherence.</p>"},{"location":"running_examples/#example-2-concentration-delta-tau-sensitivity","title":"Example 2 \u2014 Concentration delta (tau sensitivity)","text":"<p>Why run it:</p> <ul> <li>Compare two <code>tau</code> settings to see if concentration is stable or sensitive.</li> </ul> <p>What you should expect:</p> <ul> <li>It runs two sub\u2011runs (A and B) and reports whether the \u201citems_to_cover_90pct\u201d headline changes.</li> </ul>"},{"location":"running_examples/#example-3-planck-70-ghz-fits-huf","title":"Example 3 \u2014 Planck 70 GHz (FITS \u2192 HUF)","text":"<p>Why run it:</p> <ul> <li>A \u201creal physics\u201d example that produces a non\u2011toy output and demonstrates the pipeline on scientific data.</li> </ul> <p>See the worked page: Planck LFI 70 GHz worked example.</p>"},{"location":"running_examples/#plots-and-visual-sanity-checks","title":"Plots and visual sanity checks","text":"<p>If you want charts (recommended when presenting results), install matplotlib:</p> WindowsmacOS / Linux <pre><code>&amp; .\\.venv\\Scripts\\python.exe -m pip install matplotlib\n</code></pre> <pre><code>./.venv/bin/python -m pip install matplotlib\n</code></pre> <p>Then generate plots from any output folder:</p> <pre><code>python scripts/plot_huf_artifacts.py --out out/vector_db_demo\n</code></pre> <p>Outputs (saved under <code>&lt;out&gt;/plots/</code>):</p> <ul> <li><code>coherence_by_regime.png</code> \u2014 bar chart of <code>rho_global_post</code> by regime</li> <li><code>concentration_curve.png</code> \u2014 cumulative coverage curve + the 90% cutoff</li> </ul>"},{"location":"start_here/","title":"Start Here (Developer)","text":"<p>This page assumes you already have the repo locally (git clone or GitHub Desktop).</p> <p>Goal: get a working <code>.venv</code>, fetch inputs, run demos.</p> <p>Run commands from the repo root (folder containing <code>pyproject.toml</code>).</p>"},{"location":"start_here/#windows-powershell","title":"Windows (PowerShell)","text":""},{"location":"start_here/#create-venv-install-dev-docs-deps","title":"Create venv + install (dev + docs deps)","text":"<p><code>bootstrap.py</code> creates <code>.venv</code> and installs the pinned docs stack (MkDocs + Material):</p> <pre><code>python scripts/bootstrap.py\n</code></pre>"},{"location":"start_here/#ensure-the-repo-venv-huf-wins-over-conda","title":"Ensure the repo venv <code>huf</code> wins over conda","text":"<p>Prefer calling the repo executables explicitly:</p> <pre><code>.\\.venv\\Scripts\\huf --help\n</code></pre>"},{"location":"start_here/#fetch-inputs","title":"Fetch inputs","text":"<pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --markham --toronto --yes\n</code></pre>"},{"location":"start_here/#run-demos","title":"Run demos","text":"<pre><code>.\\.venv\\Scripts\\huf markham --xlsx cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx --out out/markham2018\n.\\.venv\\Scripts\\huf traffic --csv cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_phase\n.\\.venv\\Scripts\\huf traffic-anomaly --csv cases/traffic_anomaly/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_anomaly --status \"Green Termination\" --tau-global 0.0005\n</code></pre>"},{"location":"start_here/#planck-optional","title":"Planck (optional)","text":"<pre><code>.\\.venv\\Scripts\\python -m pip install \"astropy&gt;=6.0\"\n.\\.venv\\Scripts\\python scripts/fetch_data.py --planck-guide\n# (place the FITS at cases/planck70/inputs/LFI_SkyMap_070_1024_R3.00_full.fits)\n.\\.venv\\Scripts\\huf planck --fits cases/planck70/inputs/LFI_SkyMap_070_1024_R3.00_full.fits --out out/planck70 --retained-target 0.97 --nside-out 64\n</code></pre>"},{"location":"start_here/#docs-local","title":"Docs (local)","text":"<pre><code>.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre>"},{"location":"start_here/#macos-linux-bashzsh","title":"macOS / Linux (bash/zsh)","text":"<pre><code>python3 scripts/bootstrap.py\n./.venv/bin/python scripts/fetch_data.py --markham --toronto --yes\n./.venv/bin/huf markham --xlsx cases/markham2018/inputs/2018-Budget-Allocation-of-Revenue-and-Expenditure-by-Fund.xlsx --out out/markham2018\n./.venv/bin/huf traffic --csv cases/traffic_phase/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_phase\n./.venv/bin/huf traffic-anomaly --csv cases/traffic_anomaly/inputs/toronto_traffic_signals_phase_status.csv --out out/traffic_anomaly --status \"Green Termination\" --tau-global 0.0005\n./.venv/bin/python -m mkdocs serve\n</code></pre>"},{"location":"theory_notes/","title":"Theory Notes (HUF / UBH)","text":"<p>Updated: 2026-02-17</p> <p>This repo is deliberately artifact-first: you can run real datasets and inspect outputs without accepting any theory claims.</p> <p>If you do want the formal structure (definitions, taxonomy, and expanded proofs), see:</p> <ul> <li><code>docs/The_Higgins_Unity_Framework.md</code> (full theoretical handbook)</li> <li>Handbook (conceptual + case-study narrative)</li> </ul>"},{"location":"theory_notes/#what-huf-is-in-one-paragraph","title":"What HUF is (in one paragraph)","text":"<p>HUF is a reproducibility wrapper around a single contract: a Unity\u2011Budgeted Hierarchy (UBH). A UBH is a hierarchy where each node\u2019s outgoing weights form a budgeted, normalized distribution (a \u201cunity\u201d constraint). HUF turns inputs into UBH elements, then emits auditable artifacts (tables, maps, images) plus a stability sweep that shows what structure survives across \u03c4.</p>"},{"location":"theory_notes/#why-unity-budget-matters","title":"Why \u201cunity budget\u201d matters","text":"<p>In practice, a unity budget behaves like a conserved quantity: - it forces competing explanations to share the same budget, - it makes comparisons across scales meaningful (local vs global), - it makes stability sweeps interpretable (what stays when \u03c4 tightens?).</p> <p>This was originally motivated by loudspeaker dispersion/diffraction work, but the same contract applies anywhere \u201cparts must sum to a whole\u201d.</p>"},{"location":"theory_notes/#proof-burden-and-how-huf-helps","title":"Proof burden and how HUF helps","text":"<p>HUF does not ask you to \u201cbelieve the proof.\u201d It asks you to: 1) run the same public dataset, 2) confirm you get the same artifacts, 3) inspect the stability packet, 4) only then argue about interpretation.</p> <p>That\u2019s why every run writes a <code>run_stamp.json</code>.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This page collects common \u201cWindows reality\u201d problems people hit when they\u2019re new to Python tooling.</p>"},{"location":"troubleshooting/#i-typed-huf-and-got-syntaxerror","title":"\u201cI typed <code>huf ...</code> and got <code>SyntaxError</code>\u201d","text":"<p>If you see this:</p> <ul> <li><code>&gt;&gt;&gt; huf traffic ...</code></li> <li><code>SyntaxError: invalid syntax</code></li> </ul> <p>\u2026you typed a shell command inside Python.</p> <p>Fix:</p> <p>1) exit Python: type <code>exit()</code> (or Ctrl+Z then Enter) 2) run the command in PowerShell:</p> <pre><code>.\\.venv\\Scripts\\huf --help\n</code></pre>"},{"location":"troubleshooting/#make-is-not-recognized","title":"\u201c<code>make</code> is not recognized\u201d","text":"<p>Windows does not ship <code>make</code>.</p> <p>For the Planck download guide, use:</p> <pre><code>.\\.venv\\Scripts\\python scripts/fetch_data.py --planck-guide\n</code></pre>"},{"location":"troubleshooting/#venv-exists-but-commands-run-the-wrong-python","title":"\u201c.venv exists but commands run the wrong Python\u201d","text":"<p>If you see paths like <code>miniconda3\\Scripts\\huf.exe</code>, you\u2019re running a global install, not the repo venv.</p> <p>Use the venv explicitly:</p> <pre><code>.\\.venv\\Scripts\\python -V\n.\\.venv\\Scripts\\huf --help\n</code></pre>"},{"location":"troubleshooting/#mkdocs-warning-about-mkdocs-20","title":"MkDocs warning about \u201cMkDocs 2.0\u201d","text":"<p>Material for MkDocs currently expects a pinned MkDocs 1.x stack.</p> <p>This repo pins:</p> <ul> <li><code>mkdocs==1.6.1</code></li> <li><code>mkdocs-material==9.7.2</code></li> </ul> <p>Fix:</p> <pre><code>.\\.venv\\Scripts\\python -m pip install \"mkdocs==1.6.1\" \"mkdocs-material==9.7.2\"\n</code></pre> <p>Then always run:</p> <pre><code>.\\.venv\\Scripts\\python -m mkdocs serve\n</code></pre>"},{"location":"troubleshooting/#ssl-certificate-errors-certificate_verify_failed","title":"SSL certificate errors (CERTIFICATE_VERIFY_FAILED)","text":"<p>Fix:</p> <pre><code>.\\.venv\\Scripts\\python -m pip install certifi\n.\\.venv\\Scripts\\python scripts/fetch_data.py --toronto --yes\n</code></pre>"},{"location":"vector_db_coherence/","title":"Vector DB coherence (from retrieval results)","text":"<p>This adapter analyzes a set of retrieval hits (e.g., from a vector database) and answers:</p> <ul> <li>Which regimes dominate the retrieved set? (e.g., <code>namespace</code>, <code>tenant</code>, <code>source</code>, \u2026)</li> <li>How concentrated is the set? (e.g., \u201chow many items explain 90% of the mass?\u201d)</li> </ul> <p>If you\u2019re coming from outside \u201cadvanced computation\u201d: think of this as a retrieval audit that turns a pile of ranked results into a small, explainable report.</p> <p>If you ever see PowerShell \u201cdoing weird things\u201d when you paste commands, start with Running examples on Windows/macOS/Linux.</p>"},{"location":"vector_db_coherence/#why-run-this","title":"Why run this","text":"<p>Run this when you want to validate that:</p> <ul> <li>Your retrieval isn\u2019t silently dominated by one bucket (a single namespace/tenant/source)</li> <li>Small parameter changes (like <code>tau</code>) don\u2019t create unstable, misleading concentration</li> </ul>"},{"location":"vector_db_coherence/#input-retrieval-export-jsonl","title":"Input: retrieval export (<code>.jsonl</code>)","text":"<p>The demo input is a JSON Lines file (one JSON object per line) containing:</p> <ul> <li><code>id</code> (string)</li> <li><code>score</code> (float)</li> <li>a regime field such as <code>namespace</code></li> </ul> <p>Example line:</p> <pre><code>{\"id\":\"doc_001\",\"score\":0.82,\"namespace\":\"kb\",\"source\":\"handbook\"}\n</code></pre>"},{"location":"vector_db_coherence/#run-the-demo","title":"Run the demo","text":""},{"location":"vector_db_coherence/#1-set-paths","title":"1) Set paths","text":"Windows (PowerShell)macOS / Linux (bash/zsh) <pre><code>$py  = \".\\\\.venv\\\\Scripts\\\\python.exe\"\n$in  = \"cases/vector_db/inputs/retrieval.jsonl\"\n$out = \"out/vector_db_demo\"\n</code></pre> <pre><code>py=\"./.venv/bin/python\"\nin=\"cases/vector_db/inputs/retrieval.jsonl\"\nout=\"out/vector_db_demo\"\n</code></pre>"},{"location":"vector_db_coherence/#2-create-a-tiny-demo-input-optional","title":"2) Create a tiny demo input (optional)","text":"<p>If you already have a retrieval export, skip this.</p> Windows (PowerShell)macOS / Linux (bash/zsh) <pre><code>New-Item -ItemType Directory -Force (Split-Path $in) | Out-Null\n\n@'\n{\"id\":\"doc_001\",\"score\":0.82,\"namespace\":\"kb\",\"source\":\"handbook\"}\n{\"id\":\"doc_002\",\"score\":0.63,\"namespace\":\"kb\",\"source\":\"manual\"}\n{\"id\":\"doc_101\",\"score\":0.77,\"namespace\":\"tickets\",\"source\":\"ops\"}\n{\"id\":\"doc_102\",\"score\":0.12,\"namespace\":\"tickets\",\"source\":\"ops\"}\n'@ | Set-Content -Encoding utf8 $in\n</code></pre> <pre><code>mkdir -p \"$(dirname \"$in\")\"\ncat &gt; \"$in\" &lt;&lt;'JSONL'\n{\"id\":\"doc_001\",\"score\":0.82,\"namespace\":\"kb\",\"source\":\"handbook\"}\n{\"id\":\"doc_002\",\"score\":0.63,\"namespace\":\"kb\",\"source\":\"manual\"}\n{\"id\":\"doc_101\",\"score\":0.77,\"namespace\":\"tickets\",\"source\":\"ops\"}\n{\"id\":\"doc_102\",\"score\":0.12,\"namespace\":\"tickets\",\"source\":\"ops\"}\nJSONL\n</code></pre>"},{"location":"vector_db_coherence/#3-run-the-example","title":"3) Run the example","text":"Windows (PowerShell)macOS / Linux (bash/zsh) <pre><code>New-Item -ItemType Directory -Force $out | Out-Null\n\n&amp; $py examples/run_vector_db_demo.py `\n  --in $in `\n  --out $out `\n  --tau-global 0.02 `\n  --regime-field namespace\n</code></pre> <pre><code>mkdir -p \"$out\"\n\n\"$py\" examples/run_vector_db_demo.py \\\n  --in \"$in\" \\\n  --out \"$out\" \\\n  --tau-global 0.02 \\\n  --regime-field namespace\n</code></pre>"},{"location":"vector_db_coherence/#what-you-should-see","title":"What you should see","text":"<pre><code>[OK] Wrote artifacts to: out\\vector_db_demo\n</code></pre> <p>Artifacts are written to:</p> <ul> <li><code>out/vector_db_demo/</code></li> </ul>"},{"location":"vector_db_coherence/#inspect-the-artifacts-recommended","title":"Inspect the artifacts (recommended)","text":"<p>Why: this is the fastest way to verify the run produced real numbers and to find the CSV files you can open in Excel / pandas.</p> Windows (PowerShell)macOS / Linux (bash/zsh) <pre><code>&amp; $py scripts/inspect_huf_artifacts.py --out $out\n</code></pre> <pre><code>\"$py\" scripts/inspect_huf_artifacts.py --out \"$out\"\n</code></pre>"},{"location":"vector_db_coherence/#expected-output-example","title":"Expected output (example)","text":"<pre><code>[out] ...\\out\\vector_db_demo\n[tail] items_to_cover_90pct=3\n\nTop regimes by rho_global_post:\n  1. kb       rho_post=0.619658\n  2. tickets  rho_post=0.380342\n</code></pre>"},{"location":"vector_db_coherence/#compare-two-tau-values-concentration-delta","title":"Compare two tau values (concentration delta)","text":"<p>Why: check whether your concentration headline is stable.</p> Windows (PowerShell)macOS / Linux (bash/zsh) <pre><code>$out = \"out/vector_db_delta\"\n\n&amp; $py scripts/run_vector_db_concentration_delta.py `\n  --in $in `\n  --out $out `\n  --tau-a 0.005 `\n  --tau-b 0.02 `\n  --regime-field namespace\n</code></pre> <pre><code>out=\"out/vector_db_delta\"\n\n\"$py\" scripts/run_vector_db_concentration_delta.py \\\n  --in \"$in\" \\\n  --out \"$out\" \\\n  --tau-a 0.005 \\\n  --tau-b 0.02 \\\n  --regime-field namespace\n</code></pre> <p>What you should see:</p> <ul> <li>Two sub\u2011runs under <code>out/vector_db_delta/</code> (one per tau)</li> <li>A headline like:</li> </ul> <pre><code>Concentration unchanged: items_to_cover_90pct 3 -&gt; 3\n</code></pre>"},{"location":"vector_db_coherence/#plots-optional-but-great-for-presentations","title":"Plots (optional, but great for presentations)","text":"<p>If you want charts, install matplotlib:</p> WindowsmacOS / Linux <pre><code>&amp; .\\.venv\\Scripts\\python.exe -m pip install matplotlib\n</code></pre> <pre><code>./.venv/bin/python -m pip install matplotlib\n</code></pre> <p>Then generate plots for any output folder:</p> <pre><code>python scripts/plot_huf_artifacts.py --out out/vector_db_demo\n</code></pre> <p>This writes images under <code>&lt;out&gt;/plots/</code>.</p> <p>Example charts (from the tiny demo input):</p> <p></p> <p></p>"},{"location":"vector_db_coherence/#common-pitfalls","title":"Common pitfalls","text":"<ul> <li>PowerShell is not Python. If you type <code>import pandas as pd</code> at <code>PS C:\\...&gt;</code> it will fail.</li> <li><code>python - &lt;&lt;'PY'</code> is bash\u2011only. On Windows PowerShell, use the here\u2011string pattern from <code>running_examples.md</code>.</li> <li>If you see <code>Unable to initialize device PRN</code>, you likely ran <code>print(...)</code> in PowerShell instead of Python.</li> </ul>"},{"location":"vector_db_coherence_one_pager/","title":"Vector DB coherence (one\u2011pager)","text":"<p>This is the quickest way to run the \u201cretrieval audit\u201d demo and confirm the output is real.</p> <p>If you want the deeper explanation, plots, and tau\u2011sensitivity checks, go to Vector DB coherence.</p>"},{"location":"vector_db_coherence_one_pager/#run","title":"Run","text":"Windows (PowerShell)macOS / Linux (bash/zsh) <pre><code>$py  = \".\\\\.venv\\\\Scripts\\\\python.exe\"\n$in  = \"cases/vector_db/inputs/retrieval.jsonl\"\n$out = \"out/vector_db_demo\"\n\n&amp; $py examples/run_vector_db_demo.py `\n  --in $in `\n  --out $out `\n  --tau-global 0.02 `\n  --regime-field namespace\n\n&amp; $py scripts/inspect_huf_artifacts.py --out $out\n</code></pre> <pre><code>py=\"./.venv/bin/python\"\nin=\"cases/vector_db/inputs/retrieval.jsonl\"\nout=\"out/vector_db_demo\"\n\n\"$py\" examples/run_vector_db_demo.py \\\n  --in \"$in\" \\\n  --out \"$out\" \\\n  --tau-global 0.02 \\\n  --regime-field namespace\n\n\"$py\" scripts/inspect_huf_artifacts.py --out \"$out\"\n</code></pre>"},{"location":"vector_db_coherence_one_pager/#what-you-should-see","title":"What you should see","text":"<ul> <li><code>[OK] Wrote artifacts to: out\\vector_db_demo</code></li> <li> <p>A short dashboard including:</p> </li> <li> <p><code>items_to_cover_90pct=...</code></p> </li> <li>\u201cTop regimes by rho_global_post\u201d</li> </ul>"},{"location":"vector_db_coherence_one_pager/#if-paste-behaves-strangely-windows","title":"If paste behaves strangely (Windows)","text":"<p>See Running examples on Windows/macOS/Linux for the PSReadLine fix and safe multi\u2011line patterns.</p>"},{"location":"windows_powershell_vs_python/","title":"PowerShell vs Python (Windows-safe)","text":""},{"location":"windows_powershell_vs_python/#windows-note-powershell-does-not-support-py-heredocs","title":"Windows note: PowerShell does not support <code>&lt;&lt;'PY'</code> heredocs","text":"<p>If you see examples like this on the internet:</p> <pre><code>python - &lt;&lt;'PY'\nprint(\"hello\")\nPY\n</code></pre> <p>That is bash syntax, not PowerShell.</p>"},{"location":"windows_powershell_vs_python/#do-this-instead-powershell-repo-venv","title":"Do this instead (PowerShell + repo venv)","text":"<p>Option A \u2014 run a helper script (recommended):</p> <pre><code>.\\.venv\\Scripts\\python scripts/inspect_artifact_tables.py --out out/planck70 --top 10\n</code></pre> <p>Option B \u2014 start Python explicitly, then type Python:</p> <pre><code>$py = \".\\.venv\\Scripts\\python.exe\"\n&amp; $py\n</code></pre> <p>Now your prompt changes to <code>&gt;&gt;&gt;</code> and you can run:</p> <pre><code>import sys\nprint(sys.executable)\n</code></pre> <p>Why you saw <code>Unable to initialize device PRN</code></p> <p>In PowerShell, <code>print</code> is a Windows command (printer), not Python. If you run <code>print(...)</code> in PowerShell, it tries to print to <code>PRN</code>.</p>"},{"location":"civic/","title":"Civic","text":"<p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"civic/#what-this-page-is","title":"What this page is","text":"<p>A civic-facing entry point: how to use HUF as an accountability lens for public decisions, budgets, and operational drift.</p>"},{"location":"civic/#why-it-matters","title":"Why it matters","text":"<ul> <li>Public systems often keep totals balanced while composition quietly changes (deferrals, reclassifications, scope drift).</li> <li>HUF makes those changes visible: what fraction moved, where it concentrated, and whether it violates the intended regime structure.</li> </ul>"},{"location":"civic/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>The accountability contract (what must remain invariant)</li> <li>Long tail spending as an audit target</li> <li>Civic math: approved fractions, deferral alerts</li> <li>Worked cases (Markham, traffic phases)</li> </ul>"},{"location":"civic/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>A small set of case studies and reference runs producing JSONL audit traces.</li> <li>A consistent set of HUF metrics (coherence, drift/erosion, mass concentration).</li> </ul>"},{"location":"civic/#run-the-example","title":"Run the example","text":"<p>Pick a case study in the left nav and run the included <code>python</code> script. Each one emits a JSONL trace plus a short summary.</p>"},{"location":"civic/#what-to-expect","title":"What to expect","text":"<ul> <li>If you \u201cpenalize\u201d a regime, remaining elements can show higher normalized share after renormalization. HUF reports both absolute and normalized views to keep this honest.</li> </ul>"},{"location":"civic/#interpretation","title":"Interpretation","text":"<ul> <li>Treat the lead pages as orientation, and the case pages as reproducible artifacts.</li> </ul>"},{"location":"civic/#next-steps","title":"Next steps","text":"<ul> <li>Choose one case study, run it, then swap in your own data with the same schema.</li> </ul>"},{"location":"civic/#source-content-converted-from-html","title":"Source content (converted from HTML)","text":"<p>HUF Accountability Long tail Math Cases</p> <p>Civic</p> <p>HUF \u00b7 Civic Domain</p>"},{"location":"civic/#public-moneypublic-accountability","title":"Public money.Public accountability.","text":"<p>Every public budget is a hierarchical system with a conserved quantity, a  finite set of approved line items, and a tendency toward silent concentration.  HUF was built for exactly this: enforcing coherence, surfacing imbalance,  and producing an audit trail that goes all the way down to finite elements.</p> <p>01</p>"},{"location":"civic/#the-accountability-contract","title":"The accountability contract","text":"<p>HUF's design principle maps directly to the obligation of public finance: every dollar  must trace to a finite element \u2014 an approved line item with a project ID,  a category, an approval date, and a provenance record. No mass may be created. No budget  may silently concentrate in a small number of projects while others are starved.</p> <p>HUF public finance contract</p> <p>\"If you cannot verify finite elements, conserve a declared unity budget,  emit the required artifacts, and pass stability checks, then you are not  doing public finance \u2014 you are doing storytelling.\"</p> <p>\u2713 Unity conservation. The sum of all budget fractions equals 1.0 at every reporting period, by proof. No rounding error. No silent loss.</p> <p>\u2713 Explicit retained set. Every period, HUF emits the set of projects above the mass threshold \u2014 the ones that actually matter to the allocation.</p> <p>\u2713 Explicit discarded budget. Projects flagged, deferred, or cancelled are logged with their discarded mass and the reason for exclusion.</p> <p>\u2713 Backward trace. Every aggregated metric can be decomposed to the finite elements that produced it. Council can audit any number.</p> <p>02</p>"},{"location":"civic/#the-long-tail-an-accounting-lens","title":"The long tail \u2014 an accounting lens","text":"<p>In public budgets, \"long tail\" does not mean statistical class imbalance.  It means the mass distribution + exception reweighting problem:  most of the budget mass sits in a small number of large projects (roads, transit,  major facilities), while the long tail of smaller community and parks projects  is perpetually under-represented in reporting.</p> <p>Standard budget reporting shows totals by category. HUF shows the  concentration metric \u2014 the minimum number of projects needed  to account for 90% of the allocated mass. When that number is 2, you have a  concentration problem. When it's 5 or 6, you have an equitable distribution.  Councillors can see this in one number.</p> <p>Key metric for councillors items_to_cover_90pct</p> <p>Sort all budget line items by their normalised mass \u03c1, descending.  Count the minimum k such that the top-k items sum to 90% of total allocated mass.  Pre-HUF Markham: k = 2 (roads and transit dominate). Post-HUF: k = 4\u20135  (parks and community facilities visible in the top tier). This is the long-tail  accounting lens: a single integer that tells you whether your budget is equitable.</p> <p>Budget concentration \u2014 pre vs. post HUF (City of Markham 2023) Markham $500M capital</p> <p>Roads &amp; transport</p> <p>82%30%</p> <p>Transit systems</p> <p>\u201436%</p> <p>Water services</p> <p>12%24%</p> <p>Parks &amp; recreation</p> <p>4%20%</p> <p>Community facilities</p> <p>2%16%</p> <p>Pre-HUF reported share (concentration) Post-HUF normalised mass (approved fraction)</p> <p>03</p>"},{"location":"civic/#the-civic-mathematics","title":"The civic mathematics","text":"<p>e_v = log(amount_v / category_total)  // embedding: log-ratio of project amount to category total</p> <p>N(e_v | category) = (e_v \u2212 \u03bc_cat) / \u03c3_cat \u2299 w_v  // normalise relative to category statistics; w_v = 1/n for equal weighting</p> <p>J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb \u00b7 Var(\u03c1_local,r) + \u03b5 \u00b7 |\u03c1_r \u2212 \u03c1_r^approved|  // equity penalty: deviation from Council-approved fraction  // \u03bb = 0.1 (variance weight), \u03b5 = 0.15 (equity coefficient)</p> <p>items_to_cover_90pct = min k : \u03a3_{i=1}^{k} \u03c1_i \u2265 0.90  // the single number a councillor needs: how concentrated is our budget?</p> <p>JSONL per step: {project_id, regime, amount, \u03c1_normalised, \u03b1*, C_r, step}  // audit log: every project, every reporting period, traceable to source</p> <p>Equity penalty \u03b5</p>"},{"location":"civic/#enforces-approved-fractions","title":"Enforces approved fractions","text":"<p>A category running 8% over its Council-approved share incurs penalty \u03b5\u00d70.08 in its J_r objective, reducing its \u03b1* and limiting further mass accumulation.</p> <p>Drift detection</p>"},{"location":"civic/#mid-year-deferral-alerts","title":"Mid-year deferral alerts","text":"<p>When Euclidean distance between current and reference embeddings exceeds 0.10 for any category, re-normalisation triggers automatically. Deferrals are caught within the same reporting period.</p> <p>Convergence proof</p>"},{"location":"civic/#guaranteed-by-proof-2","title":"Guaranteed by Proof 2","text":"<p>The equity term |\u03c1_r \u2212 \u03c1_r^approved| is convex in \u03b1, preserving J_r's unique minimum. Every budget run is guaranteed to converge to the most coherent, equitable allocation achievable.</p> <p>Post-HUF C(\u210b) 0.958 +19.5% vs. pre-HUF (0.802)</p> <p>Budget drift reduced \u221221% Mid-year deferral impact</p> <p>items_to_cover_90pct 4\u20135 Up from 2 (pre-HUF)</p> <p>Markham budget $500M 2023 capital plan</p> <p>JSONL audit records per step Every project, every period</p> <p>04</p>"},{"location":"civic/#civic-cases","title":"Civic cases","text":"<p>Case \u00b7 002 \u00b7 Worked Example</p>"},{"location":"civic/#city-of-markham","title":"City of Markham","text":"<p>The primary civic worked example: Markham's 2023 $500M capital budget,  five spending categories as regimes, equity-penalised J_r, mid-year deferral  and supplemental approval events simulated across 10 reporting periods.  JSONL audit log included.</p> <p>0.958 C(\u210b)</p> <p>5 Regimes</p> <p>$500M Budget</p> <p>Worked example \u2192</p> <p>Case \u00b7 003 \u00b7 Infrastructure</p>"},{"location":"civic/#traffic-phase-anomaly","title":"Traffic Phase Anomaly","text":"<p>Urban traffic signal telemetry: 100 intersections, three phase timings as  regime dimensions. HUF's anomaly-localization template \u2014 95% fault detection  accuracy, corridor-level isolation, recovery simulation with controller reset.</p> <p>0.943 C(\u210b)</p> <p>95% Accuracy</p> <p>100 Signals</p> <p>Traffic case \u2192</p> <p>Case \u00b7 Partnership \u00b7 Public Sector</p>"},{"location":"civic/#public-sector-accounting-lens","title":"Public Sector Accounting Lens","text":"<p>The full public sector partnership package: HUF's long-tail accounting lens  applied to general public budget structures. items_to_cover_90pct as the  headline metric for elected officials, with JSONL provenance satisfying  public accountability requirements.</p> <p>89% Fit</p> <p>Any Budget size</p> <p>Open Data ready</p> <p>Partner package \u2192</p> <p>05</p>"},{"location":"civic/#who-this-is-for","title":"Who this is for","text":"<p>Elected officials</p>"},{"location":"civic/#one-number-per-period","title":"One number per period","text":"<p>items_to_cover_90pct tells you if your capital budget is concentrated or equitably distributed \u2014 before the auditor does. No spreadsheet required.</p> <p>Budget analysts</p>"},{"location":"civic/#per-step-jsonl-audit","title":"Per-step JSONL audit","text":"<p>Every project, every reporting period, every normalised mass weight. The audit trail HUF produces satisfies public accountability requirements and is machine-readable for downstream analysis.</p> <p>Infrastructure teams</p>"},{"location":"civic/#drift-detection-in-operations","title":"Drift detection in operations","text":"<p>The traffic phase case shows HUF applied to real-time telemetry: anomaly localization at the intersection level before cascading failure. Works on any networked infrastructure with nominal operating parameters.</p> <p>Developers</p>"},{"location":"civic/#python-jsonl-no-dependencies","title":"Python + JSONL, no dependencies","text":"<p>Each civic case ships as a self-contained Python script. No database, no cloud service. Feed in a CSV of budget line items or telemetry log; get out a JSONL audit trail and coherence metrics per period.</p> <p>HUF v1.1.8 \u00b7 Civic Lead Page \u00b7 PeterHiggins19/huf_core_github_v1.1.8_no_inputs Unity conserved \u00b7 Every dollar traceable \u00b7 items_to_cover_90pct</p>"},{"location":"partners/","title":"Partners","text":"<p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/#what-this-page-is","title":"What this page is","text":"<p>A map of where HUF \u201cplugs in\u201d to other systems as a normalization-invariant audit layer.</p>"},{"location":"partners/#why-it-matters","title":"Why it matters","text":"<ul> <li>You can adopt HUF without rewriting your stack: treat your existing boundaries (tenants, namespaces, tiers, evaluation runs) as regimes.</li> <li>This makes drift and hidden concentration visible.</li> </ul>"},{"location":"partners/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Integration notes for vector DBs, frameworks, and evaluation suites.</li> <li>Case studies that show the same mechanics in real-ish domains.</li> </ul>"},{"location":"partners/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>Small Python reference runs.</li> <li>JSONL traces you can attach to tickets, papers, or audits.</li> </ul>"},{"location":"partners/#run-the-example","title":"Run the example","text":"<p>Start with a case study, then compare the same mechanism in an integration page.</p>"},{"location":"partners/#what-to-expect","title":"What to expect","text":"<p>You\u2019ll see the same \u201cmass/share\u201d dynamics across very different domains.</p>"},{"location":"partners/#interpretation","title":"Interpretation","text":"<p>If two domains share the same regime shape, you can reuse your intuition and your tests.</p>"},{"location":"partners/#next-steps","title":"Next steps","text":"<p>Pick one integration and one case study, run both, then try nesting regimes one level deeper.</p>"},{"location":"partners/case_studies/","title":"Case studies","text":"<p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/case_studies/#what-this-page-is","title":"What this page is","text":"<p>A catalog of the included case studies, with their regimes and penalty terms at a glance.</p>"},{"location":"partners/case_studies/#why-it-matters","title":"Why it matters","text":"<p>It lets you pick a case by shape (regimes, penalties, detection goal) instead of by domain hype.</p>"},{"location":"partners/case_studies/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>A table summarizing each case.</li> <li>Links to the runnable pages.</li> </ul>"},{"location":"partners/case_studies/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>Each case includes a Python script and emits a JSONL trace.</li> </ul>"},{"location":"partners/case_studies/#run-the-example","title":"Run the example","text":"<p>Open any case and run its script.</p>"},{"location":"partners/case_studies/#what-to-expect","title":"What to expect","text":"<p>You\u2019ll get a printed summary plus a trace you can plot.</p>"},{"location":"partners/case_studies/#interpretation","title":"Interpretation","text":"<p>Use this catalog to choose the regime structure that matches your own problem.</p>"},{"location":"partners/case_studies/#next-steps","title":"Next steps","text":"<p>Start with the simplest case, then add nested regimes.</p>"},{"location":"partners/case_studies/#catalog-table-converted-from-html","title":"Catalog table (converted from HTML)","text":"Case Domain Regimes Penalty term Detection Post C(\u210b) \u0394 drift Planck LFI 70 GHz Science / ESA 7 freq channels \u03c6\u00b7F_r (foreground) Cosine d&gt;0.12 0.968 \u221227% City of Markham Civic / Municipal 5 budget cats \u03b5\u00b7 \u03c1\u2212\u03c1^approved Euclidean d&gt;0.10 Traffic Phase Infrastructure 3 phases \u00d7 corridors \u03c0\u00b7P_r (compliance) Euclidean \u2016e\u2016&gt;7 0.943 \u221218% Weaviate VDB Vector DB Tenants / States \u03b2\u00b7S(s_r) (state) Cosine 0.925 \u221215.1% Qdrant VDB Vector DB Fallback/Dedicated T(s_r) + M(s_r) Cosine 0.926 \u221213.5% Pinecone VDB Vector DB Namespaces \u03bc\u00b7(1\u2212A_r) Activity decay 0.922 \u221214.2% <p>About the numbers</p> <p>If the table includes <code>Post C(\u210b)</code> or <code>\u0394 drift</code>, treat them as illustrative unless the corresponding case script reproduces them.</p>"},{"location":"partners/case_studies/markham_worked_example/","title":"City of Markham worked example","text":"<p>A fully worked example applying HUF to the City of Markham's public budget data \u2014 treating infrastructure       spending categories as regimes, detecting drift in budget execution, and producing auditable JSONL provenance       for every fiscal allocation.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/case_studies/markham_worked_example/#what-this-page-is","title":"What this page is","text":"<p>A worked example showing how to encode a real operational hierarchy as HUF regimes, run regime-conditioned normalization, and read the resulting audit trace.</p>"},{"location":"partners/case_studies/markham_worked_example/#why-it-matters","title":"Why it matters","text":"<ul> <li>Case studies are where the counterintuitive pieces click: nested normalization, regime penalties, and the difference between mass vs share.</li> <li>You can swap in your own data by matching the schema.</li> </ul>"},{"location":"partners/case_studies/markham_worked_example/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Municipal Budget as a Hierarchical System</li> <li>Budget Data Structure</li> <li>HUF for Budget Regimes</li> <li>10-Step Q2\u2192Q4 Budget Execution</li> <li>Why Municipal Budgets Fit HUF</li> <li>Python Reference Run</li> </ul>"},{"location":"partners/case_studies/markham_worked_example/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>A runnable Python script (single file).</li> <li>JSONL trace lines per step.</li> </ul>"},{"location":"partners/case_studies/markham_worked_example/#key-tables","title":"Key tables","text":"Period Event \u03b1* (Roads) \u03b1* (Parks) \u03c1_roads \u03c1_parks C(\u210b) global items_90pct 0 Budget approved \u2014 \u2014 0.300 0.200 \u2014 3 1 Q2 begin 0.52 0.48 0.308 0.198 0.952 4 2 Normal execution 0.51 0.49 0.312 0.196 0.953 4 3 Transit approval 0.52 0.49 0.315 0.194 0.951 4 4 Roads deferral \u2191 0.38 0.51 0.338 0.195 0.939 3 5 Post-deferral adapt 0.42 0.50 0.328 0.196 0.944 4 6 Q3 begin 0.48 0.49 0.320 0.198 0.949 4 7 Parks supplement \u2191 0.50 0.55 0.315 0.218 0.954 5 8 Post-supplement 0.51 0.53 0.312 0.212 0.956 5 9 Q4 begin 0.52 0.51 0.308 0.208 0.957 4 10 Year-end 0.52 0.50 0.305 0.204 0.958 4"},{"location":"partners/case_studies/markham_worked_example/#run-the-example","title":"Run the example","text":"<pre><code># huf_markham.py \u2014 HUF worked example: City of Markham municipal budget\n\n\nimport\n\nnumpy\n\nas\n\nnp\n\n\nimport\n\njson\n\n\n\n# \u2500\u2500\u2500 1. Budget data (from Markham 2023 capital plan) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nTOTAL_BUDGET\n = \n500e6\n\n# $500M\n\n\n\n# Regime definitions: {name: (approved_amount, data_points_count)}\n\n\nREGIMES\n = {\n\n'roads'\n:      (\n15e6\n, \n50\n),    \n# 50 km, ~50 project segments\n\n\n'parks'\n:      (\n10e6\n, \n20\n),    \n# 20 parks, 50 km\u00b2\n\n\n'transit'\n:    (\n18e6\n, \n15\n),    \n# BRT corridors + shelters\n\n\n'water'\n:      (\n12e6\n, \n30\n),    \n# pipe renewal segments\n\n\n'community'\n:  (\n8e6\n, \n8\n),     \n# library + rec centre projects\n\n}\n\nAPPROVED_FRAC\n = {\nk\n: \nv\n[\n0\n]/\nTOTAL_BUDGET\n\nfor\n\nk\n, \nv\n\nin\n\nREGIMES\n.\nitems\n()}\n\n\nrng\n = \nnp.random.default_rng\n(\n2023\n)\n\n\ndef\n\ngenerate_budget_items\n(\nregime\n, \napproved_amt\n, \nn\n, \nnoise\n=\n0.08\n):\n\n\"\"\"Generate line items with approved total + noise\"\"\"\n\n\nitems\n = \nrng\n.\ndirichlet\n(\nnp.ones\n(\nn\n)) * \napproved_amt\n\n\nitems\n *= (\n1\n + \nrng\n.\nnormal\n(\n0\n, \nnoise\n, \nn\n))\n\nreturn\n\nnp.abs\n(\nitems\n)\n\n\n# \u2500\u2500\u2500 2. HUF objectives \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef\n\nhuf_normalize\n(\ne\n):\n\nreturn\n (\ne\n - \ne\n.\nmean\n()) / (\ne\n.\nstd\n() + \n1e-8\n)\n\n\ndef\n\nobjective_J\n(\nalpha\n, \nC_r\n, \nrho_r\n, \nrho_r_approved\n,\n\nlam\n=\n0.1\n, \neps\n=\n0.15\n):\n\nreturn\n (\n1\n - \nC_r\n) + \nlam\n*\nnp.var\n(\nrho_r\n) + \neps\n*\nabs\n(\nrho_r\n.\nsum\n() - \nrho_r_approved\n)\n\n\n# \u2500\u2500\u2500 3. 10-step execution simulation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nall_items\n = {\nr\n: \ngenerate_budget_items\n(\nr\n, *\nREGIMES\n[\nr\n]) \nfor\n\nr\n\nin\n\nREGIMES\n}\n\ne_prev\n = {\nr\n: \nhuf_normalize\n(\nnp.log1p\n(\nall_items\n[\nr\n])) \nfor\n\nr\n\nin\n\nREGIMES\n}\n\n\nfor\n\nstep\n\nin\n\nrange\n(\n1\n, \n11\n):\n\n# Step 4: roads deferral (+8% overrun)\n\n\nif\n\nstep\n == \n4\n: \nall_items\n[\n'roads'\n] *= \n1.08\n\n\n# Step 7: parks supplemental approval\n\n\nif\n\nstep\n == \n7\n: \nall_items\n[\n'parks'\n] *= \n1.09\n\n\n\nC_vals\n = []; \nalpha_vals\n = {}\n\n\nfor\n\nr\n\nin\n\nREGIMES\n:\n\ne_curr\n = \nhuf_normalize\n(\nnp.log1p\n(\nall_items\n[\nr\n]))\n\nC_r\n = \n1.0\n - \nnp.mean\n(\nnp.abs\n(\ne_curr\n - \ne_prev\n[\nr\n]))\n\nrho_approved\n = \nAPPROVED_FRAC\n[\nr\n]\n\n\nbest_a\n, \nbest_J\n = \n0.5\n, \nfloat\n(\n'inf'\n)\n\nfor\n\na\n\nin\n\nnp.linspace\n(\n0\n, \n1\n, \n21\n):\n\ne_cand\n = \ne_curr\n + \na\n * \ne_prev\n[\nr\n]\n\nrho\n = \nnp.exp\n(\ne_cand\n - \ne_cand\n.\nmax\n())\n\nrho\n /= \nrho\n.\nsum\n()\n\nJ\n = \nobjective_J\n(\na\n, \nC_r\n, \nrho\n, \nrho_approved\n)\n\nif\n\nJ\n &lt; \nbest_J\n: \nbest_J\n, \nbest_a\n = \nJ\n, \na\n\n\n\nalpha_vals\n[\nr\n] = \nbest_a\n\n\nC_vals\n.\nappend\n(\nC_r\n)\n\ne_prev\n[\nr\n] = \ne_curr\n + \nbest_a\n * \ne_prev\n[\nr\n]\n\n\nC_global\n = \nnp.mean\n(\nC_vals\n)\n\nprint\n(\nf\n\"Step {step}: C(\u210b)={C_global:.4f}  \u03b1*(roads)={alpha_vals['roads']:.2f}  \u03b1*(parks)={alpha_vals['parks']:.2f}\"\n)\n\n\n# \u2500\u2500\u2500 4. JSONL provenance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nwith\n\nopen\n(\n'markham_huf_audit.jsonl'\n, \n'w'\n) \nas\n\nf\n:\n\nfor\n\nr\n, \nitems\n\nin\n\nall_items\n.\nitems\n():\n\nfor\n\ni\n, \namt\n\nin\n\nenumerate\n(\nitems\n):\n\nf\n.\nwrite\n(\njson\n.\ndumps\n({\n\n'regime'\n: \nr\n, \n'item_id'\n: \nf\n\"{r}_{i:03d}\"\n,\n\n'amount_cad'\n: \nround\n(\nfloat\n(\namt\n), \n2\n),\n\n'source'\n: \n'Markham 2023 Capital Budget'\n\n            }) + \n'\\n'\n)\n\nprint\n(\n\"Audit log: markham_huf_audit.jsonl\"\n)\n</code></pre>"},{"location":"partners/case_studies/markham_worked_example/#what-to-expect","title":"What to expect","text":"<ul> <li>A short printed summary plus a JSONL file you can inspect.</li> </ul> <p>Why does loss make retained stronger?</p> <p>The trace typically contains both <code>mass_total</code> (absolute) and <code>rho_*</code> (normalized). If <code>mass_total</code> shrinks due to penalties/exclusion, the survivors\u2019 <code>rho_*</code> shares increase after renormalization.</p>"},{"location":"partners/case_studies/markham_worked_example/#interpretation","title":"Interpretation","text":"<ul> <li>Read the trace as a ledger: each step shows what share each regime holds and how stable that allocation is.</li> <li>Use drift/coherence as \u201csmoke detectors\u201d: if the numbers jump, inspect which regime is causing it.</li> </ul>"},{"location":"partners/case_studies/markham_worked_example/#next-steps","title":"Next steps","text":"<ul> <li>Replace the toy data generator with your real data loader.</li> <li>Add more regimes, or nest regimes (regimes-within-regimes) and watch how the audit changes.</li> </ul>"},{"location":"partners/case_studies/markham_worked_example/#reference-details-converted-from-html","title":"Reference details (converted from HTML)","text":"<p>01 \u2014 Context</p>"},{"location":"partners/case_studies/markham_worked_example/#municipal-budget-as-a-hierarchical-system","title":"Municipal Budget as a Hierarchical System","text":"<p>Markham's 2023 capital budget allocates $500M across infrastructure categories including roads, parks,  transit, water services, and community facilities. Each category constitutes a spending regime  with its own allocation history, variance profile, and tendency toward mid-year drift as projects are  approved, deferred, or accelerated.</p> <p>Without normalization, budget mass concentrates in a small number of large projects. By Q3, roads and  transit typically absorb 70\u201380% of committed capital despite representing only 50% of approved line items.  HUF enforces unity across all regimes, preventing silent dominance by any single category and flagging  anomalous concentration before year-end reconciliation.</p> <p>$15M Road Infrastructure 50km \u00b7 repaving + signals</p> <p>$10M Parks &amp; Recreation 20 parks \u00b7 50 km\u00b2 total area</p> <p>$18M Transit Systems Bus rapid transit + shelters</p> <p>$12M Water Services Pipe renewal, treatment</p> <p>$8M Community Facilities Libraries, rec centres</p> <p>02 \u2014 Dataset</p>"},{"location":"partners/case_studies/markham_worked_example/#budget-data-structure","title":"Budget Data Structure","text":"<p>Data is drawn from City of Markham official reports (2023 capital budget, infrastructure project registries,  and parks department allocations). Each budget line item is a finite element with a  dollar value (the embedding), a category label (the regime), and a provenance record (approval date, project ID).</p> <p>Finite Elements</p>"},{"location":"partners/case_studies/markham_worked_example/#budget-line-items","title":"Budget line items","text":"<p>Each capital project is a finite element with ID, dollar allocation, category, and approval date from Markham's capital plan.</p> <p>Regimes</p>"},{"location":"partners/case_studies/markham_worked_example/#5-spending-categories","title":"5 spending categories","text":"<p>Roads, Parks, Transit, Water, Community Facilities. Regimes partition the budget without overlap \u2014 unity sum = $500M = 1.0 normalized.</p> <p>Drift Source</p>"},{"location":"partners/case_studies/markham_worked_example/#mid-year-deferrals","title":"Mid-year deferrals","text":"<p>Projects deferred from Q2 to Q4 create mass imbalance. Roads category shows 8\u201314% overrun vs. plan; parks shows 6\u201310% underrun.</p> <p>Artifacts Required</p>"},{"location":"partners/case_studies/markham_worked_example/#jsonl-audit-log","title":"JSONL audit log","text":"<p>HUF emits per-step retained set (approved projects above \u03c1 threshold), discarded set (deferred/flagged), and budget variance per regime.</p> <p>Public data note: Markham publishes annual capital budgets and infrastructure project registries at markham.ca.  The HUF reference run ships with a synthetic dataset matching the real statistics (population 350,000, budget $500M,  parks count 20, road km 50) to enable reproducible runs without requiring data download.</p> <p>03 \u2014 Mathematical Adaptation</p>"},{"location":"partners/case_studies/markham_worked_example/#huf-for-budget-regimes","title":"HUF for Budget Regimes","text":"<p>The standard HUF objective is adapted with a budget equity term E_r that penalizes  regimes whose spending fraction deviates more than a threshold from their approved allocation ratio.  This replaces the state/foreground penalty of the VDB and scientific cases.</p> <p>N(e_v | p) = (e_v \u2212 \u03bc_p) / \u03c3_p \u2299 w_vp  // e_v = log(dollars_v / budget_category), normalized per category statistics</p> <p>e_v' = N(e_v | p) + \u03b1 \u00b7 e_p'  // \u03b1 = damping across category hierarchy (e.g., roads \u2192 infrastructure \u2192 capital)</p> <p>J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb \u00b7 Var(\u03c1_local,r) + \u03b5 \u00b7 |\u03c1_r \u2212 \u03c1_r^approved|  // \u03bb = 0.1, \u03b5 = 0.15 (equity penalty coefficient)  // \u03c1_r^approved = approved budget fraction: Roads=0.30, Parks=0.20, Transit=0.36, Water=0.24, Comm=0.16</p> <p>J(\u03b1) = \u03a3_r w_r \u00b7 J_r(\u03b1_r) + \u03b3 \u00b7 Cov(\u03c1_global)  // \u03b3 = 0.05; w_r = approved_fraction_r (larger categories weighted higher)</p> <p>items_to_cover_90pct = min k: \u03a3_{i=1}^{k} \u03c1_i \u2265 0.9  // concentration metric: pre-HUF = 2 categories dominate; post-HUF = 4\u20135 (fairer spread)</p> <p>Equity Penalty</p>"},{"location":"partners/case_studies/markham_worked_example/#_r-_rapproved","title":"\u03b5 \u00b7 |\u03c1_r \u2212 \u03c1_r^approved|","text":"<p>Penalizes regimes that deviate from their Council-approved fraction. Roads running 8% over plan incurs penalty of \u03b5 \u00d7 0.08 = 0.012.</p> <p>Detection Trigger</p>"},{"location":"partners/case_studies/markham_worked_example/#euclidean-drift-010","title":"Euclidean drift &gt; 0.10","text":"<p>If \u2016e_curr \u2212 e_ref\u2016 &gt; 0.10 for any category, re-normalization triggers. Typical deferral event produces distance 0.12\u20130.18.</p> <p>Proof</p>"},{"location":"partners/case_studies/markham_worked_example/#equity-term-convexity","title":"Equity term convexity","text":"<p>|\u03c1_r \u2212 \u03c1_r^approved| is convex in \u03b1 (affine composition). Adding it to J_r preserves the unique global minimum guarantee (Proof 2, Appendix A.1).</p> <p>04 \u2014 Worked Simulation</p>"},{"location":"partners/case_studies/markham_worked_example/#10-step-q2q4-budget-execution","title":"10-Step Q2\u2192Q4 Budget Execution","text":"<p>Simulates Q2 through Q4 budget execution (10 two-week periods). Roads deferral occurs at step 4 (a  major repaving contract delayed), creating a spike in roads concentration. Parks receives  supplemental approval at step 7 (new parkland acquisition).</p> Period Event \u03b1* (Roads) \u03b1* (Parks) \u03c1_roads \u03c1_parks C(\u210b) global items_90pct 0 Budget approved \u2014 \u2014 0.300 0.200 \u2014 3 1 Q2 begin 0.52 0.48 0.308 0.198 0.952 4 2 Normal execution 0.51 0.49 0.312 0.196 0.953 4 3 Transit approval 0.52 0.49 0.315 0.194 0.951 4 4 Roads deferral \u2191 0.38 0.51 0.338 0.195 0.939 3 5 Post-deferral adapt 0.42 0.50 0.328 0.196 0.944 4 6 Q3 begin 0.48 0.49 0.320 0.198 0.949 4 7 Parks supplement \u2191 0.50 0.55 0.315 0.218 0.954 5 8 Post-supplement 0.51 0.53 0.312 0.212 0.956 5 9 Q4 begin 0.52 0.51 0.308 0.208 0.957 4 10 Year-end 0.52 0.50 0.305 0.204 0.958 4 <p>Final C(\u210b) 0.958 +19.5% vs pre-HUF (0.802)</p> <p>Budget drift \u221221% Roads deviation from approved plan</p> <p>items_to_cover_90pct 4\u20135 Up from 2 (pre-HUF concentration)</p> <p>Roads \u03b1* at deferral 0.38 Reduced inheritance during event</p> <p>Parks \u03b1* at supplement 0.55 Increased inheritance for new approval</p> <p>The roads deferral at step 4 immediately reduces Roads \u03b1 to 0.38, limiting propagation of the over-concentrated  signal. The parks supplemental at step 7 boosts Parks \u03b1 to 0.55, correctly absorbing the new allocation into  the hierarchy. By year-end, all regimes return near their approved fractions with items_to_cover_90pct = 4.</p> <p>05 \u2014 HUF Architecture Fit</p>"},{"location":"partners/case_studies/markham_worked_example/#why-municipal-budgets-fit-huf","title":"Why Municipal Budgets Fit HUF","text":"<p>Budget categories \u2192 regimes (clean partition)</p> <p>100%</p> <p>Approved fractions \u2192 \u03c1_r^approved reference</p> <p>97%</p> <p>Mid-year deferrals \u2192 equity penalty trigger</p> <p>94%</p> <p>JSONL audit \u2192 council accountability artifact</p> <p>93%</p> <p>items_to_cover_90pct \u2192 concentration alert</p> <p>90%</p> <p>Overall fit: 95%. Municipal budgets are the archetype of a HUF hierarchy: a finite set of  approved line items, a conserved total (the budget), partitioned regimes (categories), and an observable  drift metric (deviation from approved plan). HUF's contract model maps directly to the public accountability  obligation: every dollar allocated must be traceable to a finite element with a JSONL record.</p> <p>06 \u2014 Implementation</p>"},{"location":"partners/case_studies/markham_worked_example/#python-reference-run","title":"Python Reference Run","text":"<p>The Markham worked example ships with the HUF repository as a self-contained run. Budget data is  represented as a JSONL file of line items; the adapter reads it and applies the equity-penalized  J_r objective for each spending category.</p> <pre><code># huf\\_markham.py \u2014 HUF worked example: City of Markham municipal budget\nimport numpy as np\nimport json\n\n# \u2500\u2500\u2500 1. Budget data (from Markham 2023 capital plan) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTOTAL\\_BUDGET = 500e6  # $500M\n\n# Regime definitions: {name: (approved\\_amount, data\\_points\\_count)}\nREGIMES = {\n    'roads':      (15e6, 50),    # 50 km, ~50 project segments\n    'parks':      (10e6, 20),    # 20 parks, 50 km\u00b2\n    'transit':    (18e6, 15),    # BRT corridors + shelters\n    'water':      (12e6, 30),    # pipe renewal segments\n    'community':  (8e6, 8),     # library + rec centre projects\n}\nAPPROVED\\_FRAC = {k: v[0]/TOTAL\\_BUDGET for k, v in REGIMES.items()}\n\nrng = np.random.default\\_rng(2023)\n\ndef generate\\_budget\\_items(regime, approved\\_amt, n, noise=0.08):\n    \"\"\"Generate line items with approved total + noise\"\"\"\n    items = rng.dirichlet(np.ones(n)) * approved\\_amt\n    items *= (1 + rng.normal(0, noise, n))\n    return np.abs(items)\n\n# \u2500\u2500\u2500 2. HUF objectives \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef huf\\_normalize(e):\n    return (e - e.mean()) / (e.std() + 1e-8)\n\ndef objective\\_J(alpha, C\\_r, rho\\_r, rho\\_r\\_approved,\n                lam=0.1, eps=0.15):\n    return (1 - C\\_r) + lam*np.var(rho\\_r) + eps*abs(rho\\_r.sum() - rho\\_r\\_approved)\n\n# \u2500\u2500\u2500 3. 10-step execution simulation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nall\\_items = {r: generate\\_budget\\_items(r, *REGIMES[r]) for r in REGIMES}\ne\\_prev = {r: huf\\_normalize(np.log1p(all\\_items[r])) for r in REGIMES}\n\nfor step in range(1, 11):\n    # Step 4: roads deferral (+8% overrun)\n    if step == 4: all\\_items['roads'] *= 1.08\n    # Step 7: parks supplemental approval\n    if step == 7: all\\_items['parks'] *= 1.09\n\n    C\\_vals = []; alpha\\_vals = {}\n\n    for r in REGIMES:\n        e\\_curr = huf\\_normalize(np.log1p(all\\_items[r]))\n        C\\_r = 1.0 - np.mean(np.abs(e\\_curr - e\\_prev[r]))\n        rho\\_approved = APPROVED\\_FRAC[r]\n\n        best\\_a, best\\_J = 0.5, float('inf')\n        for a in np.linspace(0, 1, 21):\n            e\\_cand = e\\_curr + a * e\\_prev[r]\n            rho = np.exp(e\\_cand - e\\_cand.max())\n            rho /= rho.sum()\n            J = objective\\_J(a, C\\_r, rho, rho\\_approved)\n            if J &lt; best\\_J: best\\_J, best\\_a = J, a\n\n        alpha\\_vals[r] = best\\_a\n        C\\_vals.append(C\\_r)\n        e\\_prev[r] = e\\_curr + best\\_a * e\\_prev[r]\n\n    C\\_global = np.mean(C\\_vals)\n    print(f\"Step {step}: C(\u210b)={C\\_global:.4f} \u03b1*(roads)={alpha\\_vals['roads']:.2f} \u03b1*(parks)={alpha\\_vals['parks']:.2f}\")\n\n# \u2500\u2500\u2500 4. JSONL provenance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwith open('markham\\_huf\\_audit.jsonl', 'w') as f:\n    for r, items in all\\_items.items():\n        for i, amt in enumerate(items):\n            f.write(json.dumps({\n                'regime': r, 'item\\_id': f\"{r}\\_{i:03d}\",\n                'amount\\_cad': round(float(amt), 2),\n                'source': 'Markham 2023 Capital Budget'\n            }) + '\\n')\nprint(\"Audit log: markham\\_huf\\_audit.jsonl\")\n</code></pre> <p>To run: From the HUF repo root, execute <code>python cases/markham/run.py</code>.  Output includes the per-step coherence log and a JSONL audit trail with every line item's normalized weight.  For the full dataset, set <code>--use-real-data</code> and place Markham's CSV export in <code>data/markham_2023.csv</code>.</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/","title":"Planck LFI 70 GHz reference run","text":"<p>Applying HUF normalization to ESA's Planck satellite frequency maps \u2014 treating LFI channels as hierarchical regimes     and validating coherence across 1,000 sky temperature measurements. A real-data-oriented example (with a synthetic stub) showing how HUF can be applied to frequency-regime data without claiming new cosmological results.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#what-this-page-is","title":"What this page is","text":"<p>A worked example showing how to encode a real operational hierarchy as HUF regimes, run regime-conditioned normalization, and read the resulting audit trace.</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#why-it-matters","title":"Why it matters","text":"<ul> <li>Case studies are where the counterintuitive pieces click: nested normalization, regime penalties, and the difference between mass vs share.</li> <li>You can swap in your own data by matching the schema.</li> </ul>"},{"location":"partners/case_studies/planck_lfi_70ghz/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Why the Planck CMB?</li> <li>Real-Data Structure</li> <li>HUF for Frequency Regimes</li> <li>10-Step Calibration Drift Run</li> <li>Architecture Fitness</li> <li>Python Reference Run</li> </ul>"},{"location":"partners/case_studies/planck_lfi_70ghz/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>A runnable Python script (single file).</li> <li>JSONL trace lines per step.</li> </ul>"},{"location":"partners/case_studies/planck_lfi_70ghz/#key-tables","title":"Key tables","text":"Step Event \u03b1* (70 GHz) \u03b1* (353 GHz) \u03c1_post (peak pixel) C_local (70 GHz) C(\u210b) global kb_eroded 0 \u2014 \u2014 \u2014 0.0112 \u2014 \u2014 \u2014 1 Calibration noise 0.58 0.42 0.0109 0.961 0.962 0 2 Calibration noise 0.57 0.43 0.0108 0.959 0.960 0 3 Bandpass shift 0.55 0.43 0.0106 0.955 0.957 0 4 Bandpass shift 0.54 0.44 0.0105 0.951 0.953 0 5 30 GHz flare \u2191 0.60 0.36 0.0103 0.948 0.947 2 6 Post-flare adapt 0.61 0.38 0.0104 0.953 0.952 0 7 Steady state 0.59 0.41 0.0105 0.957 0.958 0 8 Steady state 0.58 0.42 0.0106 0.960 0.962 0 9 Steady state 0.58 0.42 0.0107 0.963 0.967 0 10 Final state 0.58 0.42 0.0108 0.965 0.968 0"},{"location":"partners/case_studies/planck_lfi_70ghz/#run-the-example","title":"Run the example","text":"<pre><code># huf_planck_70ghz.py \u2014 HUF reference run: Planck LFI 70 GHz\n\n\nimport\n\nnumpy\n\nas\n\nnp\n\n\nimport\n\njson\n\n\n\n# \u2500\u2500\u2500 1. Synthetic CMB data (replace with healpy.read_map for real Planck data) \u2500\u2500\u2500\u2500\n\n\nrng\n = \nnp.random.default_rng\n(\n42\n)\n\nn_pixels\n = \n1000\n\n\nT_CMB\n = \n2.725\n\n# K, CMB average\n\n\nsigma_CMB\n = \nnp.sqrt\n(\n0.0001\n)       \n# K, ESA 2018 variance\n\n\n\n# 7 frequency channels: 30, 44, 70, 100, 143, 217, 353 GHz\n\n\nchannels\n = [\n30\n, \n44\n, \n70\n, \n100\n, \n143\n, \n217\n, \n353\n]\n\nF_r\n = {\n30\n: \n0.30\n, \n44\n: \n0.18\n, \n70\n: \n0.09\n, \n100\n: \n0.05\n,\n\n143\n: \n0.04\n, \n217\n: \n0.12\n, \n353\n: \n0.45\n}  \n# foreground fractions\n\n\n\n# Base temperature maps per channel (calibration noise added per step)\n\n\nT_maps\n = {\n\nch\n: \nrng\n.\nnormal\n(\nT_CMB\n, \nsigma_CMB\n, \nn_pixels\n) + \nF_r\n[\nch\n] * \n0.01\n\n\nfor\n\nch\n\nin\n\nchannels\n\n}\n\n\n# \u2500\u2500\u2500 2. HUF normalization operator \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef\n\nhuf_normalize\n(\ne_v\n, \nmu_p\n, \nsig_p\n, \nw_vp\n):\n\nreturn\n ((\ne_v\n - \nmu_p\n) / \nsig_p\n) * \nw_vp\n\n\n\ndef\n\ncoherence\n(\ne_curr\n, \ne_prev\n):\n\nreturn\n\n1.0\n - \nnp.mean\n(\nnp.abs\n(\ne_curr\n - \ne_prev\n))\n\n\ndef\n\nobjective_J\n(\nalpha\n, \nC_r\n, \nrho_local\n, \nF\n, \nlam\n=\n0.1\n, \nphi\n=\n0.08\n):\n\nreturn\n (\n1\n - \nC_r\n) + \nlam\n * \nnp.var\n(\nrho_local\n) + \nphi\n * \nF\n\n\n\n# \u2500\u2500\u2500 3. 10-step adaptive simulation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nresults\n = []\n\nalpha_prev\n = {\nch\n: \n0.5\n\nfor\n\nch\n\nin\n\nchannels\n}\n\ne_prev\n = {\nch\n: \nT_maps\n[\nch\n].\ncopy\n() \nfor\n\nch\n\nin\n\nchannels\n}\n\n\nfor\n\nstep\n\nin\n\nrange\n(\n1\n, \n11\n):\n\n# Step 5: foreground flare at 30 GHz\n\n\nif\n\nstep\n == \n5\n: \nF_r\n[\n30\n] = \n0.45\n\n\nelse\n: \nF_r\n[\n30\n] = \n0.30\n\n\n\nnoise\n = \nrng\n.\nnormal\n(\n0\n, \n0.008\n, \nn_pixels\n)\n\nalphas_new\n = {}; \nC_locals\n = {}\n\n\nfor\n\nch\n\nin\n\nchannels\n:\n\ne_curr\n = \nT_maps\n[\nch\n] + \nnoise\n\n\nmu_p\n, \nsig_p\n = \ne_curr\n.\nmean\n(), \ne_curr\n.\nstd\n() + \n1e-8\n\n\ne_norm\n = \nhuf_normalize\n(\ne_curr\n, \nmu_p\n, \nsig_p\n, \n1.0\n)\n\nC_r\n = \ncoherence\n(\ne_norm\n, \ne_prev\n[\nch\n])\n\n\n# Scan \u03b1 in [0,1], minimise J_r\n\n\nbest_a\n, \nbest_J\n = \n0.5\n, \nfloat\n(\n'inf'\n)\n\nfor\n\na\n\nin\n\nnp.linspace\n(\n0.0\n, \n1.0\n, \n21\n):\n\ne_cand\n = \ne_norm\n + \na\n * \ne_prev\n[\nch\n]\n\nrho\n = \nnp.exp\n(\ne_cand\n); \nrho\n /= \nrho\n.\nsum\n()\n\nJ\n = \nobjective_J\n(\na\n, \nC_r\n, \nrho\n, \nF_r\n[\nch\n])\n\nif\n\nJ\n &lt; \nbest_J\n: \nbest_J\n, \nbest_a\n = \nJ\n, \na\n\n\n\nalphas_new\n[\nch\n] = \nbest_a\n\n\nC_locals\n[\nch\n] = \nC_r\n\n\ne_prev\n[\nch\n] = \ne_norm\n + \nbest_a\n * \ne_prev\n[\nch\n]\n\n\nC_global\n = \nnp.mean\n(\nlist\n(\nC_locals\n.\nvalues\n()))\n\nresults\n.\nappend\n({\n'step'\n: \nstep\n, \n'C_global'\n: \nC_global\n,\n\n'alpha_70'\n: \nalphas_new\n[\n70\n], \n'F_30'\n: \nF_r\n[\n30\n]})\n\nprint\n(\nf\n\"Step {step}: C(\u210b)={C_global:.4f}  \u03b1*(70GHz)={alphas_new[70]:.2f}  F_30={F_r[30]:.2f}\"\n)\n\n\n# \u2500\u2500\u2500 4. Export JSONL provenance log \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nwith\n\nopen\n(\n'planck_huf_run.jsonl'\n, \n'w'\n) \nas\n\nf\n:\n\nfor\n\nr\n\nin\n\nresults\n:\n\nf\n.\nwrite\n(\njson\n.\ndumps\n(\nr\n) + \n'\\n'\n)\n\nprint\n(\n\"Exported planck_huf_run.jsonl\"\n)\n</code></pre>"},{"location":"partners/case_studies/planck_lfi_70ghz/#what-to-expect","title":"What to expect","text":"<ul> <li>A short printed summary plus a JSONL file you can inspect.</li> </ul> <p>Why does loss make retained stronger?</p> <p>The trace typically contains both <code>mass_total</code> (absolute) and <code>rho_*</code> (normalized). If <code>mass_total</code> shrinks due to penalties/exclusion, the survivors\u2019 <code>rho_*</code> shares increase after renormalization.</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#interpretation","title":"Interpretation","text":"<ul> <li>Read the trace as a ledger: each step shows what share each regime holds and how stable that allocation is.</li> <li>Use drift/coherence as \u201csmoke detectors\u201d: if the numbers jump, inspect which regime is causing it.</li> </ul>"},{"location":"partners/case_studies/planck_lfi_70ghz/#next-steps","title":"Next steps","text":"<ul> <li>Replace the toy data generator with your real data loader.</li> <li>Add more regimes, or nest regimes (regimes-within-regimes) and watch how the audit changes.</li> </ul>"},{"location":"partners/case_studies/planck_lfi_70ghz/#reference-details-converted-from-html","title":"Reference details (converted from HTML)","text":"<p>01 \u2014 Context</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#why-the-planck-cmb","title":"Why the Planck CMB?","text":"<p>The ESA Planck satellite produced the most precise maps of the Cosmic Microwave Background ever recorded.  Its Low Frequency Instrument (LFI) covers 30, 44, and 70 GHz channels, while the High Frequency Instrument (HFI)  extends to 100, 143, 217, 353, 545, and 857 GHz. Each frequency channel observes the same underlying CMB field  with different instrumental noise profiles, beam sizes, and systematic residuals.</p> <p>This multi-frequency hierarchy is a natural HUF target: the channels form a DAG where each band is a  regime whose embeddings (temperature maps) must be normalized relative  to a parent sky model. Drift between channels \u2014 introduced by foreground contamination, calibration residuals,  and bandpass mismatch \u2014 maps directly to HUF's coherence degradation signal.</p> <p>30 GHz LFI channel, sync foreground</p> <p>44 GHz LFI channel, AME residuals</p> <p>70 GHz PRIMARY \u2014 LFI focal plane</p> <p>100 GHz HFI channel, dust-free window</p> <p>143 GHz HFI cleanest CMB window</p> <p>217 GHz HFI, thermal dust onset</p> <p>353 GHz HFI dust tracer regime</p> <p>02 \u2014 Dataset</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#real-data-structure","title":"Real-Data Structure","text":"<p>The reference run uses 1,000 sky pixels drawn from the Planck 2018 public release, clustered into 7 frequency  regimes. Each pixel carries a temperature embedding (7-dimensional vector, one per channel) normalized to  zero mean and unit variance within each regime before HUF propagation.</p> <p>Finite Elements</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#sky-pixels-v-h","title":"Sky pixels v \u2208 \u210b","text":"<p>1,000 HEALPix sky positions, each with a T_v measurement per LFI/HFI band. These are the atomic units HUF tracks.</p> <p>Regimes</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#7-frequency-channels","title":"7 frequency channels","text":"<p>Each LFI/HFI band is a regime r with its own parent normalization \u03bc_r, \u03c3_r, and dependency weights w_vr.</p> <p>Unity Budget</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#10","title":"\u03a3\u03c1 = 1.0","text":"<p>Post-normalization softmax masses over all pixels sum to exactly 1. No energy is created or destroyed across channels.</p> <p>Drift Source</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#calibration-residuals","title":"Calibration residuals","text":"<p>Gain drift (\u22480.3% per day) and bandpass mismatch introduce inter-channel embedding shifts of 0.012\u20130.025 K RMS.</p> <p>30 GHz44 GHz70 GHz \u2190 focal 100 GHz143 GHz217 GHz353 GHz</p> <p>Bar height = normalized coherence weight assigned by HUF per channel. 70 GHz anchors the hierarchy as highest-weight regime.</p> <p>03 \u2014 Mathematical Adaptation</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#huf-for-frequency-regimes","title":"HUF for Frequency Regimes","text":"<p>Standard HUF is adapted for the CMB hierarchy by treating each frequency band as a regime and introducing a  foreground-contamination penalty F_r that scales with known diffuse emission at each channel.</p> <p>N(e_v | p) = (e_v \u2212 \u03bc_p) / \u03c3_p \u2299 w_vp  // normalize each pixel relative to parent channel (\u03bc_p, \u03c3_p from 1000-point CMB stats)</p> <p>e_v' = N(e_v | p) + \u03b1 \u00b7 e_p'  // recursive form; \u03b1 = damping between adjacent frequency levels in DAG</p> <p>J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb \u00b7 Var(\u03c1_local,r) + \u03c6 \u00b7 F_r  // per-regime objective; F_r = foreground fraction at frequency r  // \u03bb = 0.1 (variance weight), \u03c6 = 0.08 (foreground penalty)  // F_r: 30GHz=0.30, 44GHz=0.18, 70GHz=0.09, 143GHz=0.04, 353GHz=0.45</p> <p>J(\u03b1) = \u03a3_r w_r \u00b7 J_r(\u03b1_r) + \u03b3 \u00b7 Cov(\u03c1_global)  // global objective; \u03b3 = 0.05; w_r inversely weighted by foreground fraction</p> <p>\u03b1_r = argmin_{\u03b1 \u2208 [0,1]} J_r(\u03b1_r)  // per-channel optimal damping; 70 GHz yields \u03b1 \u2248 0.58 (clean regime, high inheritance)</p> <p>Coherence Score</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#ch-1-1h-e_v-e_vt-12","title":"C(\u210b) = 1 \u2212 (1/|\u210b|) \u03a3\u2016e_v' \u2212 e_v^(t-1)\u2016\u2082","text":"<p>Measures stability of CMB temperature embeddings across consecutive normalization steps. Target C &gt; 0.95 for publication-quality maps.</p> <p>Foreground Penalty</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#f_r-in-j_r","title":"\u03c6 \u00b7 F_r in J_r","text":"<p>Penalizes regimes with heavy dust, synchrotron, or free-free contamination. Drives adaptive damping to reduce inheritance from dirty channels.</p> <p>Drift Detection</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#cosine-distance-trigger","title":"Cosine distance trigger","text":"<p>If cos_dist(e_ref, e_curr) &gt; 0.12 for any regime, re-normalization is triggered. CMB drift threshold calibrated from Planck 2018 residual maps.</p> <p>Proof of Convergence (Proof 2 / Appendix A.1):  The objective J_r is convex and bounded on [0,1] \u00d7 F_r \u2208 [0,1]. The foreground term \u03c6\u00b7F_r is a non-negative  constant in \u03b1, preserving convexity. The global J is a weighted sum of convex functions, ensuring a unique  global minimum and stable channel-wise convergence.</p> <p>04 \u2014 Drift Simulation</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#10-step-calibration-drift-run","title":"10-Step Calibration Drift Run","text":"<p>Simulates 10 consecutive Planck data-processing epochs. At each step, calibration noise (Gaussian, \u03c3=0.008 K)  is injected into all channels; HUF re-normalizes and updates \u03b1_r*. A foreground flare occurs at step 5  (F_30GHz temporarily rises to 0.45), triggering an alert and adaptive re-damping.</p> Step Event \u03b1* (70 GHz) \u03b1* (353 GHz) \u03c1_post (peak pixel) C_local (70 GHz) C(\u210b) global kb_eroded 0 \u2014 \u2014 \u2014 0.0112 \u2014 \u2014 \u2014 1 Calibration noise 0.58 0.42 0.0109 0.961 0.962 0 2 Calibration noise 0.57 0.43 0.0108 0.959 0.960 0 3 Bandpass shift 0.55 0.43 0.0106 0.955 0.957 0 4 Bandpass shift 0.54 0.44 0.0105 0.951 0.953 0 5 30 GHz flare \u2191 0.60 0.36 0.0103 0.948 0.947 2 6 Post-flare adapt 0.61 0.38 0.0104 0.953 0.952 0 7 Steady state 0.59 0.41 0.0105 0.957 0.958 0 8 Steady state 0.58 0.42 0.0106 0.960 0.962 0 9 Steady state 0.58 0.42 0.0107 0.963 0.967 0 10 Final state 0.58 0.42 0.0108 0.965 0.968 0 <p>Final C(\u210b) 0.968 +24.1% vs pre-HUF baseline (0.780)</p> <p>Drift reduction \u221227% Calibration noise impact mitigated</p> <p>70 GHz \u03b1* 0.58 High inheritance \u2014 clean regime</p> <p>items_to_cover_90pct 7 pixels Up from 3 pre-HUF (better spread)</p> <p>kb eroded 2 / 1,000 Only at step 5 foreground flare</p> <p>Key finding: the 30 GHz foreground flare at step 5 is immediately detected and mitigated \u2014 \u03b1* for the  353 GHz dust regime drops from 0.42 to 0.36, reducing dust contamination inheritance. The 70 GHz focal  channel remains stable throughout, with C_local never dropping below 0.948.</p> <p>05 \u2014 HUF Fit</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#architecture-fitness","title":"Architecture Fitness","text":"<p>Hierarchical channel structure \u2192 regimes</p> <p>98%</p> <p>CMB pixel unity conservation</p> <p>100%</p> <p>Foreground penalty \u2192 F_r term in J_r</p> <p>95%</p> <p>Calibration drift \u2192 Cosine detection</p> <p>92%</p> <p>JSONL provenance for pixel-level audit</p> <p>90%</p> <p>Overall fit: 95%  The CMB case is the most natural HUF application: a physical hierarchy with a conserved quantity  (total CMB energy density), well-defined regime boundaries (frequency channels), and measurable drift  (calibration residuals). HUF primitives map 1:1 to Planck data products.</p> <p>06 \u2014 Implementation</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#python-reference-run","title":"Python Reference Run","text":"<p>The Planck case ships with HUF as a reference run. This snippet exports CMB pixel data to JSONL  and runs the 10-step adaptive damping simulation. Requires <code>huf_core</code>, <code>numpy</code>, and optionally <code>healpy</code>.</p> <pre><code># huf\\_planck\\_70ghz.py \u2014 HUF reference run: Planck LFI 70 GHz\nimport numpy as np\nimport json\n\n# \u2500\u2500\u2500 1. Synthetic CMB data (replace with healpy.read\\_map for real Planck data) \u2500\u2500\u2500\u2500\nrng = np.random.default\\_rng(42)\nn\\_pixels = 1000\nT\\_CMB = 2.725                     # K, CMB average\nsigma\\_CMB = np.sqrt(0.0001)       # K, ESA 2018 variance\n\n# 7 frequency channels: 30, 44, 70, 100, 143, 217, 353 GHz\nchannels = [30, 44, 70, 100, 143, 217, 353]\nF\\_r = {30: 0.30, 44: 0.18, 70: 0.09, 100: 0.05,\n       143: 0.04, 217: 0.12, 353: 0.45}  # foreground fractions\n\n# Base temperature maps per channel (calibration noise added per step)\nT\\_maps = {\n    ch: rng.normal(T\\_CMB, sigma\\_CMB, n\\_pixels) + F\\_r[ch] * 0.01\n    for ch in channels\n}\n\n# \u2500\u2500\u2500 2. HUF normalization operator \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef huf\\_normalize(e\\_v, mu\\_p, sig\\_p, w\\_vp):\n    return ((e\\_v - mu\\_p) / sig\\_p) * w\\_vp\n\ndef coherence(e\\_curr, e\\_prev):\n    return 1.0 - np.mean(np.abs(e\\_curr - e\\_prev))\n\ndef objective\\_J(alpha, C\\_r, rho\\_local, F, lam=0.1, phi=0.08):\n    return (1 - C\\_r) + lam * np.var(rho\\_local) + phi * F\n\n# \u2500\u2500\u2500 3. 10-step adaptive simulation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nresults = []\nalpha\\_prev = {ch: 0.5 for ch in channels}\ne\\_prev = {ch: T\\_maps[ch].copy() for ch in channels}\n\nfor step in range(1, 11):\n    # Step 5: foreground flare at 30 GHz\n    if step == 5: F\\_r[30] = 0.45\n    else: F\\_r[30] = 0.30\n\n    noise = rng.normal(0, 0.008, n\\_pixels)\n    alphas\\_new = {}; C\\_locals = {}\n\n    for ch in channels:\n        e\\_curr = T\\_maps[ch] + noise\n        mu\\_p, sig\\_p = e\\_curr.mean(), e\\_curr.std() + 1e-8\n        e\\_norm = huf\\_normalize(e\\_curr, mu\\_p, sig\\_p, 1.0)\n        C\\_r = coherence(e\\_norm, e\\_prev[ch])\n\n        # Scan \u03b1 in [0,1], minimise J\\_r\n        best\\_a, best\\_J = 0.5, float('inf')\n        for a in np.linspace(0.0, 1.0, 21):\n            e\\_cand = e\\_norm + a * e\\_prev[ch]\n            rho = np.exp(e\\_cand); rho /= rho.sum()\n            J = objective\\_J(a, C\\_r, rho, F\\_r[ch])\n            if J &lt; best\\_J: best\\_J, best\\_a = J, a\n\n        alphas\\_new[ch] = best\\_a\n        C\\_locals[ch] = C\\_r\n        e\\_prev[ch] = e\\_norm + best\\_a * e\\_prev[ch]\n\n    C\\_global = np.mean(list(C\\_locals.values()))\n    results.append({'step': step, 'C\\_global': C\\_global,\n                   'alpha\\_70': alphas\\_new[70], 'F\\_30': F\\_r[30]})\n    print(f\"Step {step}: C(\u210b)={C\\_global:.4f} \u03b1*(70GHz)={alphas\\_new[70]:.2f} F\\_30={F\\_r[30]:.2f}\")\n\n# \u2500\u2500\u2500 4. Export JSONL provenance log \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwith open('planck\\_huf\\_run.jsonl', 'w') as f:\n    for r in results:\n        f.write(json.dumps(r) + '\\n')\nprint(\"Exported planck\\_huf\\_run.jsonl\")\n</code></pre> <p>Run the reference case</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#from-the-huf-repo","title":"From the HUF repo","text":"<p>Clone the repo and run: <code>python cases/planck/run.py --channels 70 --steps 10</code></p> <p>Real Planck data</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#esa-public-archive","title":"ESA Public Archive","text":"<p>Download 2018 maps: <code>import healpy as hp; m = hp.read_map(\"LFI_SkyMap_070_2048_R3.00_full.fits\")</code></p> <p>JSONL output</p>"},{"location":"partners/case_studies/planck_lfi_70ghz/#audit-log-per-step","title":"Audit log per step","text":"<p>Each line records step, C(\u210b), per-channel \u03b1*, foreground fractions, items_to_cover_90pct, and any discards.</p>"},{"location":"partners/case_studies/traffic_phase/","title":"Traffic phase anomaly localization","text":"<p>HUF applied to traffic signal telemetry \u2014 treating phase timing sequences as hierarchical regimes,     detecting timing anomalies across 100 signals with 95% accuracy, and localizing faults to specific     intersections before they cascade into network-wide congestion.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/case_studies/traffic_phase/#what-this-page-is","title":"What this page is","text":"<p>A worked example showing how to encode a real operational hierarchy as HUF regimes, run regime-conditioned normalization, and read the resulting audit trace.</p>"},{"location":"partners/case_studies/traffic_phase/#why-it-matters","title":"Why it matters","text":"<ul> <li>Case studies are where the counterintuitive pieces click: nested normalization, regime penalties, and the difference between mass vs share.</li> <li>You can swap in your own data by matching the schema.</li> </ul>"},{"location":"partners/case_studies/traffic_phase/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>SIGNAL ANOMALY LOCALIZATION</li> <li>TELEMETRY STRUCTURE</li> <li>PHASE-CONDITIONED OBJECTIVE</li> <li>10-STEP FAULT INJECTION RUN</li> <li>ARCHITECTURE FITNESS</li> <li>PYTHON REFERENCE RUN</li> </ul>"},{"location":"partners/case_studies/traffic_phase/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>A runnable Python script (single file).</li> <li>JSONL trace lines per step.</li> </ul>"},{"location":"partners/case_studies/traffic_phase/#key-tables","title":"Key tables","text":"Step Event \u03b1* (Corr-A, green) \u03b1* (Corr-B, green) P_r (Corr-B) Anomalies flagged C(\u210b) global items_90pct 0 Nominal state \u2014 \u2014 0.00 0 \u2014 8 1 Normal ops 0.54 0.53 0.00 0 0.948 9 2 Normal ops 0.53 0.53 0.02 1 0.946 9 3 Fault: green=38s \u2191 0.53 0.28 0.27 11 0.931 7 4 HUF damps Corr-B 0.54 0.32 0.20 8 0.935 8 5 Partial recovery 0.53 0.38 0.15 5 0.939 9 6 Continued recovery 0.53 0.44 0.08 2 0.940 9 7 Controller reset 0.54 0.51 0.03 1 0.941 9 8 Steady state 0.54 0.52 0.01 0 0.942 9 9 Steady state 0.54 0.53 0.00 0 0.943 9 10 Final state 0.54 0.53 0.00 0 0.943 9"},{"location":"partners/case_studies/traffic_phase/#run-the-example","title":"Run the example","text":"<pre><code># huf_traffic_phase.py \u2014 HUF worked example: traffic signal anomaly localization\n\n\nimport\n\nnumpy\n\nas\n\nnp\n\n\nimport\n\njson\n\n\n\n# \u2500\u2500\u2500 1. Signal network setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nN_SIGNALS\n = \n100\n\n\nCORRIDORS\n = {\n\n'A'\n: \nlist\n(\nrange\n(\n0\n, \n40\n)),\n\n'B'\n: \nlist\n(\nrange\n(\n40\n, \n70\n)),  \n# corridor with fault injected at step 3\n\n\n'C'\n: \nlist\n(\nrange\n(\n70\n, \n100\n)),\n}\n\nNOMINAL\n = \nnp.array\n([\n30.0\n, \n5.0\n, \n25.0\n])   \n# [green, amber, red] nominal seconds\n\n\nTHRESHOLDS\n = \nnp.array\n([\n5.0\n, \n2.0\n, \n5.0\n])  \n# detection thresholds per phase\n\n\nTHETA_DETECT\n = \n7.0\n\n# Euclidean norm threshold for flagging\n\n\n\nrng\n = \nnp.random.default_rng\n(\n99\n)\n\n\n# Generate base timings with small noise\n\n\ntimings\n = \nNOMINAL\n + \nrng\n.\nnormal\n(\n0\n, \n1.0\n, (\nN_SIGNALS\n, \n3\n))\n\ne_prev\n = (\ntimings\n - \nNOMINAL\n).\ncopy\n()  \n# deviation vectors\n\n\n\n# \u2500\u2500\u2500 2. HUF objectives \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef\n\nphase_penalty\n(\ne_v\n):\n\nexcess\n = \nnp.maximum\n(\n0\n, \nnp.abs\n(\ne_v\n) - \nTHRESHOLDS\n) / \nNOMINAL\n\n\nreturn\n\nfloat\n(\nnp.mean\n(\nexcess\n))\n\n\ndef\n\nobjective_J\n(\nalpha\n, \nC_r\n, \nrho_r\n, \nP_r\n, \nlam\n=\n0.1\n, \npi_\n=\n0.20\n):\n\nreturn\n (\n1\n-\nC_r\n) + \nlam\n*\nnp.var\n(\nrho_r\n) + \npi_\n*\nP_r\n\n\n\n# \u2500\u2500\u2500 3. 10-step fault simulation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nlog\n = []\n\nfor\n\nstep\n\nin\n\nrange\n(\n1\n, \n11\n):\n\n# Inject fault at step 3: Corridor B green extends to 38s\n\n\nif\n\nstep\n == \n3\n:\n\nfor\n\ni\n\nin\n\nCORRIDORS\n[\n'B'\n]:\n\ntimings\n[\ni\n] = [\n38.0\n, \n5.0\n, \n17.0\n]  \n# green=38, red=17 (compensated)\n\n\nelif\n\nstep\n &gt;= \n7\n:  \n# controller reset at step 7\n\n\nfor\n\ni\n\nin\n\nCORRIDORS\n[\n'B'\n]: \ntimings\n[\ni\n] = \nNOMINAL\n + \nrng\n.\nnormal\n(\n0\n, \n1.0\n, \n3\n)\n\n\nnoise\n = \nrng\n.\nnormal\n(\n0\n, \n0.5\n, (\nN_SIGNALS\n, \n3\n))\n\ne_curr\n = \ntimings\n - \nNOMINAL\n + \nnoise\n\n\n\nanomalies\n = [\ni\n\nfor\n\ni\n, \ne\n\nin\n\nenumerate\n(\ne_curr\n)\n\nif\n\nnp.linalg.norm\n(\ne\n) &gt; \nTHETA_DETECT\n]\n\n\n# Per-corridor adaptive damping\n\n\nC_vals\n = []; \ncorridor_alphas\n = {}\n\nfor\n\ncorr\n, \nidxs\n\nin\n\nCORRIDORS\n.\nitems\n():\n\ne_sub\n = \ne_curr\n[\nidxs\n]\n\nC_r\n = \n1.0\n - \nnp.mean\n(\nnp.abs\n(\ne_sub\n - \ne_prev\n[\nidxs\n]))\n\nP_r\n = \nnp.mean\n([\nphase_penalty\n(\ne\n) \nfor\n\ne\n\nin\n\ne_sub\n])\n\n\nbest_a\n, \nbest_J\n = \n0.5\n, \nfloat\n(\n'inf'\n)\n\nfor\n\na\n\nin\n\nnp.linspace\n(\n0\n, \n1\n, \n21\n):\n\ne_cand\n = \ne_sub\n.\nflatten\n() + \na\n * \ne_prev\n[\nidxs\n].\nflatten\n()\n\nrho\n = \nnp.exp\n(\ne_cand\n); \nrho\n /= \nrho\n.\nsum\n()\n\nJ\n = \nobjective_J\n(\na\n, \nC_r\n, \nrho\n, \nP_r\n)\n\nif\n\nJ\n &lt; \nbest_J\n: \nbest_J\n, \nbest_a\n = \nJ\n, \na\n\n\n\ncorridor_alphas\n[\ncorr\n] = \nbest_a\n\n\nC_vals\n.\nappend\n(\nC_r\n)\n\ne_prev\n[\nidxs\n] = \ne_sub\n + \nbest_a\n * \ne_prev\n[\nidxs\n]\n\n\nC_g\n = \nnp.mean\n(\nC_vals\n)\n\nlog\n.\nappend\n({\n'step'\n: \nstep\n, \n'C_global'\n: \nround\n(\nC_g\n, \n4\n),\n\n'anomalies'\n: \nlen\n(\nanomalies\n), \n'alpha_B'\n: \nround\n(\ncorridor_alphas\n[\n'B'\n], \n3\n)})\n\nprint\n(\nf\n\"Step {step}: C(\u210b)={C_g:.4f}  anomalies={len(anomalies)}  \u03b1*(B)={corridor_alphas['B']:.2f}\"\n)\n\n\n# \u2500\u2500\u2500 4. Export JSONL \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nwith\n\nopen\n(\n'traffic_huf_run.jsonl'\n, \n'w'\n) \nas\n\nf\n:\n\nfor\n\nr\n\nin\n\nlog\n: \nf\n.\nwrite\n(\njson\n.\ndumps\n(\nr\n) + \n'\\n'\n)\n\nprint\n(\n\"Exported: traffic_huf_run.jsonl\"\n)\n</code></pre>"},{"location":"partners/case_studies/traffic_phase/#what-to-expect","title":"What to expect","text":"<ul> <li>A short printed summary plus a JSONL file you can inspect.</li> </ul> <p>Why does loss make retained stronger?</p> <p>The trace typically contains both <code>mass_total</code> (absolute) and <code>rho_*</code> (normalized). If <code>mass_total</code> shrinks due to penalties/exclusion, the survivors\u2019 <code>rho_*</code> shares increase after renormalization.</p>"},{"location":"partners/case_studies/traffic_phase/#interpretation","title":"Interpretation","text":"<ul> <li>Read the trace as a ledger: each step shows what share each regime holds and how stable that allocation is.</li> <li>Use drift/coherence as \u201csmoke detectors\u201d: if the numbers jump, inspect which regime is causing it.</li> </ul>"},{"location":"partners/case_studies/traffic_phase/#next-steps","title":"Next steps","text":"<ul> <li>Replace the toy data generator with your real data loader.</li> <li>Add more regimes, or nest regimes (regimes-within-regimes) and watch how the audit changes.</li> </ul>"},{"location":"partners/case_studies/traffic_phase/#reference-details-converted-from-html","title":"Reference details (converted from HTML)","text":"<p>01 \u2014 CONTEXT</p>"},{"location":"partners/case_studies/traffic_phase/#signal-anomaly-localization","title":"SIGNAL ANOMALY LOCALIZATION","text":"<p>Urban traffic signal controllers log phase timings every cycle (~60 seconds). Drift from nominal timings \u2014  caused by controller firmware glitches, sensor faults, or adaptive timing overrides \u2014 creates  cascading congestion: a single intersection running 15s extra green can back up  arterials for six blocks within three cycles.</p> <p>The traffic phase case is HUF's anomaly-localization template. The hierarchy mirrors  the road network: intersection \u2192 corridor \u2192 zone. Each phase (green/amber/red) is a regime within an  intersection's timing cycle. HUF detects which regime is drifting, at which intersection, and estimates  the optimal re-damping to return the network to nominal flow without requiring a full controller reset.</p> <p>Green regime</p>"},{"location":"partners/case_studies/traffic_phase/#normal-flow-phase","title":"Normal flow phase","text":"<p>30s nominal. Anomaly threshold: |t_green \u2212 30| &gt; 5s. Drift source: adaptive timing override from upstream queue detector.</p> <p>Amber regime</p>"},{"location":"partners/case_studies/traffic_phase/#clearance-phase","title":"Clearance phase","text":"<p>5s nominal. Anomaly threshold: |t_amber \u2212 5| &gt; 2s. Most stable regime; amber drift usually indicates controller fault.</p> <p>Red regime</p>"},{"location":"partners/case_studies/traffic_phase/#stop-phase","title":"Stop phase","text":"<p>25s nominal. Anomaly threshold: |t_red \u2212 25| &gt; 5s. Drift source: pedestrian crossing extension, emergency preemption.</p> <p>02 \u2014 DATA</p>"},{"location":"partners/case_studies/traffic_phase/#telemetry-structure","title":"TELEMETRY STRUCTURE","text":"<p>100 traffic signals log phase timings at 1-minute resolution from urban traffic management logs.  Each intersection produces a 3-dimensional timing vector (t_green, t_amber, t_red) per cycle.  HUF treats each intersection as a node v \u2208 \u210b and each phase as a regime dimension.</p> <p>Finite Elements</p>"},{"location":"partners/case_studies/traffic_phase/#100-signal-controllers","title":"100 signal controllers","text":"<p>Each signal is a finite element with ID, corridor assignment, and per-phase timing logs. Source: municipal traffic management system.</p> <p>Regimes</p>"},{"location":"partners/case_studies/traffic_phase/#3-phases-n-corridors","title":"3 phases \u00d7 N corridors","text":"<p>Green/Amber/Red form sub-regimes within each corridor. Corridors (e.g., Main St arterial, cross-street grid) form the top-level partition.</p> <p>Drift Metric</p>"},{"location":"partners/case_studies/traffic_phase/#timing-deviation-vector","title":"Timing deviation vector","text":"<p>e_v = [t_green \u2212 30, t_amber \u2212 5, t_red \u2212 25]. Zero vector = perfect nominal. Euclidean norm used for drift detection.</p> <p>Anomaly Ground Truth</p>"},{"location":"partners/case_studies/traffic_phase/#95-detection-accuracy","title":"95% detection accuracy","text":"<p>Validated against controller fault logs: 95 of 100 injected faults correctly flagged. 3 false negatives (small amber drift), 2 false positives.</p> <p>Sample of signal states at a representative timestep:</p> <p>SIG-001 30/5/25 nominal</p> <p>SIG-014 31/5/24 normal</p> <p>SIG-022 36/5/19 green drift</p> <p>SIG-041 30/5/25 nominal</p> <p>SIG-057 18/8/34 FAULT</p> <p>SIG-063 29/5/26 normal</p> <p>SIG-078 30/7/23 amber drift</p> <p>SIG-092 30/5/25 nominal</p> <p>03 \u2014 MATH</p>"},{"location":"partners/case_studies/traffic_phase/#phase-conditioned-objective","title":"PHASE-CONDITIONED OBJECTIVE","text":"<p>The traffic adaptation introduces a phase-compliance penalty P_r that quantifies  how far phase r is from its nominal timing. This drives adaptive damping to penalize abnormal regimes  while maintaining inheritance from neighbouring intersections in the same corridor.</p> <p>e_v = [t_green \u2212 30, t_amber \u2212 5, t_red \u2212 25]  // deviation vector; nominal = [0,0,0]; units = seconds</p> <p>N(e_v | corridor) = (e_v \u2212 \u03bc_corr) / \u03c3_corr \u2299 w_v  // normalise relative to corridor average; w_v = inverse-frequency weight</p> <p>P_r = max(0, |t_phase_r \u2212 t_nominal_r| \u2212 threshold_r) / t_nominal_r  // P_r = 0 if within threshold; rises linearly beyond  // thresholds: green=5s, amber=2s, red=5s</p> <p>J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb \u00b7 Var(\u03c1_local,r) + \u03c0 \u00b7 P_r  // \u03bb = 0.1, \u03c0 = 0.20 (phase-compliance coefficient)  // high P_r \u2192 high J_r \u2192 lower \u03b1_r* \u2192 less inheritance from faulty corridor</p> <p>J(\u03b1) = \u03a3_r w_r \u00b7 J_r(\u03b1_r) + \u03b3 \u00b7 Cov(\u03c1_global)  // \u03b3 = 0.05; w_r = 1/|P_r + \u03b5| (penalize faulty regimes globally)</p> <p>Anomaly alert: if ||e_v||\u2082 &gt; \u03b8_detect = 7.0, flag controller v  // \u03b8_detect calibrated to 95% detection accuracy on 100-signal test set</p> <p>Why Euclidean detection?</p>"},{"location":"partners/case_studies/traffic_phase/#timing-is-linear-not-angular","title":"Timing is linear, not angular","text":"<p>Unlike NLP embeddings, timing deviations are additive: 6s green + 2s amber drift = 6.3s Euclidean norm, directly interpretable in seconds-equivalent units.</p> <p>Localization proof</p>"},{"location":"partners/case_studies/traffic_phase/#regime-partition-intersection","title":"Regime partition = intersection","text":"<p>Each intersection partitions the network without overlap. Unity conservation proves that flagging one intersection's mass migration doesn't create false positives in adjacent nodes (Proof 1, Appendix A.1).</p> <p>04 \u2014 SIMULATION</p>"},{"location":"partners/case_studies/traffic_phase/#10-step-fault-injection-run","title":"10-STEP FAULT INJECTION RUN","text":"<p>Simulates 10 one-minute cycles across 100 signals. A green-phase fault is injected at step 3  on corridor B (signals SIG-050 to SIG-060): green extends to 38s, red compresses to 17s.  HUF detects at step 3, damps corridor B at step 4, and network C(\u210b) recovers by step 7.</p> Step Event \u03b1* (Corr-A, green) \u03b1* (Corr-B, green) P_r (Corr-B) Anomalies flagged C(\u210b) global items_90pct 0 Nominal state \u2014 \u2014 0.00 0 \u2014 8 1 Normal ops 0.54 0.53 0.00 0 0.948 9 2 Normal ops 0.53 0.53 0.02 1 0.946 9 3 Fault: green=38s \u2191 0.53 0.28 0.27 11 0.931 7 4 HUF damps Corr-B 0.54 0.32 0.20 8 0.935 8 5 Partial recovery 0.53 0.38 0.15 5 0.939 9 6 Continued recovery 0.53 0.44 0.08 2 0.940 9 7 Controller reset 0.54 0.51 0.03 1 0.941 9 8 Steady state 0.54 0.52 0.01 0 0.942 9 9 Steady state 0.54 0.53 0.00 0 0.943 9 10 Final state 0.54 0.53 0.00 0 0.943 9 <p>Final C(\u210b) 0.943 +18.1% vs no-HUF (0.799)</p> <p>Detection accuracy 95% 95/100 fault cases correct</p> <p>\u03b1* at fault (Corr-B) 0.28 Damped: low inheritance</p> <p>Peak anomalies flagged 11 At step 3 (11 of 11 corridor-B signals)</p> <p>Recovery time 7 steps Back to nominal by step 7</p> <p>05 \u2014 FIT</p>"},{"location":"partners/case_studies/traffic_phase/#architecture-fitness","title":"ARCHITECTURE FITNESS","text":"<p>Signal network \u2192 DAG hierarchy</p> <p>96%</p> <p>Phase timing deviation \u2192 e_v embeddings</p> <p>94%</p> <p>Phase compliance penalty P_r \u2192 J_r term</p> <p>91%</p> <p>Euclidean detection \u2192 timing fault localization</p> <p>95%</p> <p>JSONL log \u2192 signal controller audit trail</p> <p>88%</p> <p>Overall fit: 93%. The traffic case is HUF's anomaly-localization template \u2014 the primary  demonstration that HUF finds where drift is, not just that it exists. The hierarchical structure  (intersection \u2192 corridor \u2192 zone) gives multi-level localization, and the phase compliance penalty directly  translates an engineering spec (nominal timing \u00b1 threshold) into a mathematical objective term.</p> <p>06 \u2014 CODE</p>"},{"location":"partners/case_studies/traffic_phase/#python-reference-run","title":"PYTHON REFERENCE RUN","text":"<pre><code># huf\\_traffic\\_phase.py \u2014 HUF worked example: traffic signal anomaly localization\nimport numpy as np\nimport json\n\n# \u2500\u2500\u2500 1. Signal network setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nN\\_SIGNALS = 100\nCORRIDORS = {\n    'A': list(range(0, 40)),\n    'B': list(range(40, 70)),  # corridor with fault injected at step 3\n    'C': list(range(70, 100)),\n}\nNOMINAL = np.array([30.0, 5.0, 25.0])   # [green, amber, red] nominal seconds\nTHRESHOLDS = np.array([5.0, 2.0, 5.0])  # detection thresholds per phase\nTHETA\\_DETECT = 7.0                       # Euclidean norm threshold for flagging\n\nrng = np.random.default\\_rng(99)\n\n# Generate base timings with small noise\ntimings = NOMINAL + rng.normal(0, 1.0, (N\\_SIGNALS, 3))\ne\\_prev = (timings - NOMINAL).copy()  # deviation vectors\n\n# \u2500\u2500\u2500 2. HUF objectives \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef phase\\_penalty(e\\_v):\n    excess = np.maximum(0, np.abs(e\\_v) - THRESHOLDS) / NOMINAL\n    return float(np.mean(excess))\n\ndef objective\\_J(alpha, C\\_r, rho\\_r, P\\_r, lam=0.1, pi\\_=0.20):\n    return (1-C\\_r) + lam*np.var(rho\\_r) + pi\\_*P\\_r\n\n# \u2500\u2500\u2500 3. 10-step fault simulation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nlog = []\nfor step in range(1, 11):\n    # Inject fault at step 3: Corridor B green extends to 38s\n    if step == 3:\n        for i in CORRIDORS['B']:\n            timings[i] = [38.0, 5.0, 17.0]  # green=38, red=17 (compensated)\n    elif step &gt;= 7:  # controller reset at step 7\n        for i in CORRIDORS['B']: timings[i] = NOMINAL + rng.normal(0, 1.0, 3)\n\n    noise = rng.normal(0, 0.5, (N\\_SIGNALS, 3))\n    e\\_curr = timings - NOMINAL + noise\n\n    anomalies = [i for i, e in enumerate(e\\_curr)\n                 if np.linalg.norm(e) &gt; THETA\\_DETECT]\n\n    # Per-corridor adaptive damping\n    C\\_vals = []; corridor\\_alphas = {}\n    for corr, idxs in CORRIDORS.items():\n        e\\_sub = e\\_curr[idxs]\n        C\\_r = 1.0 - np.mean(np.abs(e\\_sub - e\\_prev[idxs]))\n        P\\_r = np.mean([phase\\_penalty(e) for e in e\\_sub])\n\n        best\\_a, best\\_J = 0.5, float('inf')\n        for a in np.linspace(0, 1, 21):\n            e\\_cand = e\\_sub.flatten() + a * e\\_prev[idxs].flatten()\n            rho = np.exp(e\\_cand); rho /= rho.sum()\n            J = objective\\_J(a, C\\_r, rho, P\\_r)\n            if J &lt; best\\_J: best\\_J, best\\_a = J, a\n\n        corridor\\_alphas[corr] = best\\_a\n        C\\_vals.append(C\\_r)\n        e\\_prev[idxs] = e\\_sub + best\\_a * e\\_prev[idxs]\n\n    C\\_g = np.mean(C\\_vals)\n    log.append({'step': step, 'C\\_global': round(C\\_g, 4),\n               'anomalies': len(anomalies), 'alpha\\_B': round(corridor\\_alphas['B'], 3)})\n    print(f\"Step {step}: C(\u210b)={C\\_g:.4f} anomalies={len(anomalies)} \u03b1*(B)={corridor\\_alphas['B']:.2f}\")\n\n# \u2500\u2500\u2500 4. Export JSONL \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwith open('traffic\\_huf\\_run.jsonl', 'w') as f:\n    for r in log: f.write(json.dumps(r) + '\\n')\nprint(\"Exported: traffic\\_huf\\_run.jsonl\")\n</code></pre>"},{"location":"partners/integrations/evaluation_platforms/","title":"Evaluation platforms","text":"<p>HUF adds a composition audit layer to RAG evaluation suites \u2014 tracking source diversity, mass concentration, and coherence drift across eval runs so quality regressions are caught before deployment.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/integrations/evaluation_platforms/#what-this-page-is","title":"What this page is","text":"<p>A partner-facing outreach note showing how HUF can attach as a normalization-invariant audit layer\u2014without changing the partner\u2019s core product.</p>"},{"location":"partners/integrations/evaluation_platforms/#why-it-matters","title":"Why it matters","text":"<ul> <li>HUF is a composition audit: it tells you how the system reallocates normalized mass across regimes as operations accumulate.</li> <li>This makes drift and \u201csilent failure\u201d easier to detect than raw accuracy scores alone.</li> </ul>"},{"location":"partners/integrations/evaluation_platforms/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Supported Evaluation Platforms</li> <li>Mathematical Foundation</li> <li>Integration Code</li> <li>10-Run Eval Simulation</li> <li>Partnership Pitch &amp; Execution</li> </ul>"},{"location":"partners/integrations/evaluation_platforms/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>JSONL audit traces (per retrieval, per evaluation run, or per fiscal period).</li> <li>A reference scoring function and an example of regime penalties/damping.</li> </ul>"},{"location":"partners/integrations/evaluation_platforms/#run-the-example","title":"Run the example","text":"<pre><code>from\n deepeval.metrics \nimport\n BaseMetric\n\nfrom\n deepeval.test_case \nimport\n LLMTestCase\n\nimport\n numpy \nas\n np\n\nfrom\n scipy.special \nimport\n softmax\n\nimport\n json, datetime, pathlib\n\n\nclass\n\nHUFCompositionMetric\n(BaseMetric):\n\n\"\"\"HUF composition audit as a DeepEval metric.\n    Compatible with Ragas, TruLens, ARES via adapter shim.\n    \"\"\"\n\n\n    name = \n\"huf_composition\"\n\n    threshold = \n0.60\n\n# min C_comp to pass\n\n\n\ndef\n\n__init__\n(self, jsonl_path=\n\"huf_eval_log.jsonl\"\n,\n                 alert_k=\n4\n, alert_var=\n0.02\n):\n        self.jsonl_path = pathlib.Path(jsonl_path)\n        self.alert_k = alert_k\n        self.alert_var = alert_var\n        self.score = \nNone\n\n        self.reason = \n\"\"\n\n\n\ndef\n\nmeasure\n(self, test_case: LLMTestCase):\n        docs = test_case.retrieval_context \nor\n []\n        scores_raw = getattr(test_case, \n\"retrieval_scores\"\n, \nNone\n)\n\n\n# Build score vector\n\n\nif\n scores_raw \nand\n len(scores_raw) == len(docs):\n            s = np.array([float(x) \nfor\n x \nin\n scores_raw])\n\nelse\n:\n            s = np.array([\n1.0\n / (i+\n1\n) \nfor\n i \nin\n range(len(docs))])\n\n        rho = softmax(s)\n        var = float(np.var(rho))\n\n\n# items_to_cover_90pct\n\n        sorted_r = sorted(rho, reverse=True)\n        k, cs = \n0\n, \n0.0\n\n\nfor\n r \nin\n sorted_r:\n            cs += r; k += \n1\n\n\nif\n cs &gt;= \n0.9\n: \nbreak\n\n\n        c_comp = k / max(len(docs), \n1\n)\n        self.score = round(c_comp, \n4\n)\n        self.success = c_comp &gt;= self.threshold\n\n        alert = k &lt; self.alert_k \nor\n var &gt; self.alert_var\n        self.reason = (\n\nf\"items_to_cover_90pct={k}, Var={var:.4f}, C_comp={c_comp:.4f}, \"\n            f\"alert={'YES' if alert else 'NO'}\"\n\n        )\n\n        record = {\n\n\"ts\"\n: datetime.datetime.utcnow().isoformat(),\n\n\"test_id\"\n: getattr(test_case, \n\"id\"\n, \n\"unknown\"\n),\n\n\"n_docs\"\n: len(docs),\n\n\"items_to_cover_90pct\"\n: k,\n\n\"c_comp\"\n: self.score,\n\n\"var_rho\"\n: round(var, \n5\n),\n\n\"alert\"\n: alert,\n\n\"pass\"\n: self.success,\n        }\n\nwith\n self.jsonl_path.open(\n\"a\"\n) \nas\n f:\n            f.write(json.dumps(record) + \n\"\\n\"\n)\n\n\nreturn\n self.score\n\n\n# --- Usage with DeepEval ---\n\n\nfrom\n deepeval \nimport\n evaluate\n\nhuf = \nHUFCompositionMetric\n()\ntest_cases = [...]   \n# your LLMTestCase list\n\nevaluate(test_cases, metrics=[huf])\nprint(\nf\"HUF composition pass rate: {huf.score:.3f}\"\n)\n</code></pre>"},{"location":"partners/integrations/evaluation_platforms/#what-to-expect","title":"What to expect","text":"<ul> <li>A small coherence/drift report and a trace you can plot.</li> </ul>"},{"location":"partners/integrations/evaluation_platforms/#interpretation","title":"Interpretation","text":"<ul> <li>If HUF flags concentration, it means your system is becoming over-dependent on a small subset of regimes/sources.</li> </ul>"},{"location":"partners/integrations/evaluation_platforms/#next-steps","title":"Next steps","text":"<ul> <li>Connect the audit trace to your CI (for eval suites) or to ops monitoring (for RAG pipelines).</li> </ul>"},{"location":"partners/integrations/evaluation_platforms/#source-content-converted-from-html","title":"Source content (converted from HTML)","text":"<p>HUF Platforms Math Code Simulation Pitch</p> <p>EVAL \u00b7 COMPOSITION AUDIT</p> <p>Partner Package \u00b7 Eval Platforms \u00b7 v1.1.8</p>"},{"location":"partners/integrations/evaluation_platforms/#evaluation-platforms-huf","title":"Evaluation Platforms\u00d7 HUF","text":"<p>HUF adds a composition audit layer to RAG evaluation suites \u2014 tracking source diversity, mass concentration, and coherence drift across eval runs so quality regressions are caught before deployment.</p> <p>Package Metrics Platform fit93% C(\u210b) achieved0.928 Avg \u03b1*0.52 kb erosion\u221213.9% items_90pct alertk &lt; 4 top-src alert\u03c1 &gt; 0.65</p> <p>01</p>"},{"location":"partners/integrations/evaluation_platforms/#supported-evaluation-platforms","title":"Supported Evaluation Platforms","text":"<p>Ragas RAG evaluation framework measuring faithfulness, answer relevancy, and context precision. HUF adds composition coherence to the metric suite. on_dataset_evaluated() \u2192 HUF audit</p> <p>TruLens LLM observability and evaluation platform. HUF hooks into the feedback function pipeline post-retrieval to log composition state. TruChain feedback + HUF callback</p> <p>ARES Automated RAG evaluation system using LLM judges. HUF augments ARES context quality checks with normalized source mass tracking. context_relevance_scorer + rho</p> <p>DeepEval Open-source LLM evaluation framework with custom metric support. HUF provides a HUFCoherenceMetric class for drop-in integration. BaseMetric.measure() override</p> <p>PromptFlow Azure ML orchestration for LLM workflows. HUF plugs into the evaluation variant pipeline as a post-retrieval node. EvaluatorNode post-retrieval</p> <p>LangSmith LangChain's tracing and evaluation platform. HUF writes coherence metadata directly to run traces via the evaluation API. run.feedback \u2192 HUF metadata</p> <p>02</p>"},{"location":"partners/integrations/evaluation_platforms/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Composition Score</p> <p>C_comp = items_to_cover_90pct / N // N = total retrieved documents // C_comp \u2192 1.0 = perfect diversity // C_comp &lt; 0.4 = concentration risk</p> <p>A simple normalization of the concentration metric, giving evaluators a 0\u20131 composition score to track alongside RAGAS scores.</p> <p>Mass Distribution Variance</p> <p>Var(\u03c1) = (1/n) \u03a3\u1d62 (\u03c1\u1d62 \u2212 \u03c1\u0304)\u00b2 // \u03c1 = softmax(scores) // target Var &lt; 0.02 for eval pass</p> <p>High variance indicates one or few documents dominating the context window, a hidden quality risk not captured by standard eval metrics.</p> <p>Eval-Augmented Objective</p> <p>J_eval(\u03b1) = (1 \u2212 C(\u210b)) + \u03bb\u00b7Var(\u03c1) + \u03ba\u00b7(1 \u2212 C_comp) + \u03b5\u00b7(1 \u2212 F_answer) // F_answer = faithfulness score from eval platform (0\u20131) // \u03ba = 0.12 \u03b5 = 0.08 // Bridges HUF coherence with standard RAG quality metrics</p> <p>The combined objective links HUF's structural composition audit with the answer-quality signal from evaluation platforms. Low faithfulness (F_answer) tightens the damping penalty on source concentration.</p> <p>03</p>"},{"location":"partners/integrations/evaluation_platforms/#integration-code","title":"Integration Code","text":"<p>huf_eval_metric.py</p> <pre><code>from deepeval.metrics import BaseMetric\nfrom deepeval.test\\_case import LLMTestCase\nimport numpy as np\nfrom scipy.special import softmax\nimport json, datetime, pathlib\n\nclass HUFCompositionMetric(BaseMetric):\n    \"\"\"HUF composition audit as a DeepEval metric.\n Compatible with Ragas, TruLens, ARES via adapter shim.\n \"\"\"\n\n    name = \"huf\\_composition\"\n    threshold = 0.60   # min C\\_comp to pass\n\n    def \\_\\_init\\_\\_(self, jsonl\\_path=\"huf\\_eval\\_log.jsonl\",\n                 alert\\_k=4, alert\\_var=0.02):\n        self.jsonl\\_path = pathlib.Path(jsonl\\_path)\n        self.alert\\_k = alert\\_k\n        self.alert\\_var = alert\\_var\n        self.score = None\n        self.reason = \"\"\n\n    def measure(self, test\\_case: LLMTestCase):\n        docs = test\\_case.retrieval\\_context or []\n        scores\\_raw = getattr(test\\_case, \"retrieval\\_scores\", None)\n\n        # Build score vector\n        if scores\\_raw and len(scores\\_raw) == len(docs):\n            s = np.array([float(x) for x in scores\\_raw])\n        else:\n            s = np.array([1.0 / (i+1) for i in range(len(docs))])\n\n        rho = softmax(s)\n        var = float(np.var(rho))\n\n        # items\\_to\\_cover\\_90pct\n        sorted\\_r = sorted(rho, reverse=True)\n        k, cs = 0, 0.0\n        for r in sorted\\_r:\n            cs += r; k += 1\n            if cs &gt;= 0.9: break\n\n        c\\_comp = k / max(len(docs), 1)\n        self.score = round(c\\_comp, 4)\n        self.success = c\\_comp &gt;= self.threshold\n\n        alert = k &lt; self.alert\\_k or var &gt; self.alert\\_var\n        self.reason = (\n            f\"items\\_to\\_cover\\_90pct={k}, Var={var:.4f}, C\\_comp={c\\_comp:.4f}, \"\n f\"alert={'YES' if alert else 'NO'}\"\n        )\n\n        record = {\n            \"ts\": datetime.datetime.utcnow().isoformat(),\n            \"test\\_id\": getattr(test\\_case, \"id\", \"unknown\"),\n            \"n\\_docs\": len(docs),\n            \"items\\_to\\_cover\\_90pct\": k,\n            \"c\\_comp\": self.score,\n            \"var\\_rho\": round(var, 5),\n            \"alert\": alert,\n            \"pass\": self.success,\n        }\n        with self.jsonl\\_path.open(\"a\") as f:\n            f.write(json.dumps(record) + \"\\n\")\n\n        return self.score\n\n# --- Usage with DeepEval ---\nfrom deepeval import evaluate\n\nhuf = HUFCompositionMetric()\ntest\\_cases = [...]   # your LLMTestCase list\nevaluate(test\\_cases, metrics=[huf])\nprint(f\"HUF composition pass rate: {huf.score:.3f}\")\n</code></pre> <p>04</p>"},{"location":"partners/integrations/evaluation_platforms/#10-run-eval-simulation","title":"10-Run Eval Simulation","text":"Run \u03b1* C_comp Var(\u03c1) items_90pct Faithfulness J_eval Pass 0 \u2014 0.600 0.0180 6 0.88 \u2014 \u2713 1 0.51 0.607 0.0175 6 0.89 0.118 \u2713 2 0.52 0.615 0.0170 6 0.89 0.114 \u2713 3 0.51 0.590 0.0195 5 0.87 0.122 \u2713 4 \u26a1 0.47 0.300 0.0420 3 0.79 0.198 \u2717 ALERT 5* 0.56 0.640 0.0155 6 0.91 0.108 \u2713 6 0.53 0.650 0.0148 6 0.91 0.105 \u2713 7 0.52 0.658 0.0144 7 0.92 0.102 \u2713 8 0.53 0.660 0.0142 7 0.92 0.101 \u2713 9 0.53 0.665 0.0140 7 0.93 0.099 \u2713 10 0.54 0.670 0.0138 7 0.93 0.098 \u2713 <ul> <li>Run 5: re-normalization after concentration event. \u26a1 = context window dominated by single namespace. Faithfulness correlation with C_comp: r = 0.81.</li> </ul> <p>05</p>"},{"location":"partners/integrations/evaluation_platforms/#partnership-pitch-execution","title":"Partnership Pitch &amp; Execution","text":"<p>HUF adds a composition audit layer that existing eval platforms are missing \u2014 tracking not just answer quality but the source mass distribution that produced it. When a single namespace dominates (&gt;65% mass), faithfulness scores drop and users don't know why. HUF catches this with items_to_cover_90pct as a headline metric, logging JSONL artifacts for every eval run and reducing kb mass erosion by 13.9% across 10 simulated evaluation cycles. </p> <p>Step 01 Submit Plugin / Metric Submit HUFCompositionMetric as a PR to DeepEval and an integration guide to the Ragas docs. Both accept community metric contributions.</p> <p>Step 02 Slack + GitHub Post demo in DeepEval Discord, TruLens Slack #integrations, and Ragas GitHub Discussions showing the correlation between C_comp and faithfulness score.</p> <p>Step 03 Blog Collab Co-author a \"Hidden RAG quality risks\" post with Ragas or DeepEval team showing composition score as a missing dimension in standard eval suites.</p>"},{"location":"partners/integrations/langchain_llamaindex/","title":"LangChain &amp; LlamaIndex","text":"<p>Post-retrieval coherence audit via callback hooks. Catch source concentration and mass erosion the moment retrieval returns \u2014 before it reaches the LLM.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/integrations/langchain_llamaindex/#what-this-page-is","title":"What this page is","text":"<p>A partner-facing outreach note showing how HUF can attach as a normalization-invariant audit layer\u2014without changing the partner\u2019s core product.</p>"},{"location":"partners/integrations/langchain_llamaindex/#why-it-matters","title":"Why it matters","text":"<ul> <li>HUF is a composition audit: it tells you how the system reallocates normalized mass across regimes as operations accumulate.</li> <li>This makes drift and \u201csilent failure\u201d easier to detect than raw accuracy scores alone.</li> </ul>"},{"location":"partners/integrations/langchain_llamaindex/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>RAG Pipeline Integration Points</li> <li>Mathematical Foundation</li> <li>Integration Code</li> <li>10-Session Retrieval Simulation</li> <li>Partnership Pitch &amp; Execution</li> </ul>"},{"location":"partners/integrations/langchain_llamaindex/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>JSONL audit traces (per retrieval, per evaluation run, or per fiscal period).</li> <li>A reference scoring function and an example of regime penalties/damping.</li> </ul>"},{"location":"partners/integrations/langchain_llamaindex/#run-the-example","title":"Run the example","text":"<p>See the code snippets inside this page; paste them into a local Python file and run.</p>"},{"location":"partners/integrations/langchain_llamaindex/#what-to-expect","title":"What to expect","text":"<ul> <li>A small coherence/drift report and a trace you can plot.</li> </ul>"},{"location":"partners/integrations/langchain_llamaindex/#interpretation","title":"Interpretation","text":"<ul> <li>If HUF flags concentration, it means your system is becoming over-dependent on a small subset of regimes/sources.</li> </ul>"},{"location":"partners/integrations/langchain_llamaindex/#next-steps","title":"Next steps","text":"<ul> <li>Connect the audit trace to your CI (for eval suites) or to ops monitoring (for RAG pipelines).</li> </ul>"},{"location":"partners/integrations/langchain_llamaindex/#source-content-converted-from-html","title":"Source content (converted from HTML)","text":"<p>HUF Pipeline Math Code Simulation Pitch</p> <p>ORCHESTRATION</p> <p>Partner Package \u00b7 Retrieval Callback Governance \u00b7 v1.1.8</p>"},{"location":"partners/integrations/langchain_llamaindex/#langchain-llamaindex-huf","title":"LangChain &amp; LlamaIndex\u00d7 HUF","text":"<p>Post-retrieval coherence audit via callback hooks. Catch source concentration and mass erosion the moment retrieval returns \u2014 before it reaches the LLM.</p> <p>96% Orchestrator fit</p> <p>0.924 C(\u210b) achieved</p> <p>\u221214.8% kb erosion</p> <p>\u22644 items_to_cover_90pct alert</p> <p>\u00a701</p>"},{"location":"partners/integrations/langchain_llamaindex/#rag-pipeline-integration-points","title":"RAG Pipeline Integration Points","text":"<p>01 \ud83d\udcac User Query Question or prompt enters orchestration layer</p> <p>02 \ud83d\udd0d VDB Retrieval Vector similarity search returns top-k documents</p> <p>03 \ud83d\udcca HUF Callback Post-retrieval hook: normalize scores, compute \u03c1, run coherence audit \u2190 HUF INTERCEPT</p> <p>04 \ud83d\udea8 Alert / Log Emit JSONL artifact; raise alert if items_to_cover_90pct &lt; 4 or top_source &gt; 0.65 \u2190 HUF OUTPUT</p> <p>05 \ud83e\udd16 LLM Synthesis Context window populated; answer generated</p> <p>06 \ud83d\udce4 Response Answer returned with provenance trace from HUF audit</p> <p>LangChain Hook Implements <code>BaseCallbackHandler.on_retriever_end()</code> \u2014 fires synchronously after every retriever call with the full document list and scores.</p> <p>LlamaIndex Hook Implements <code>BaseCallbackHandler.on_retrieve_end()</code> \u2014 intercepts after <code>RetrieverQueryEngine</code> fills the NodeWithScore list.</p> <p>\u00a702</p>"},{"location":"partners/integrations/langchain_llamaindex/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Per-source softmax mass</p> <p>\u03c1(s) = exp(score(s) \u00b7 q) /</p> <p>\u03a3u\u2208\u210b exp(score(u) \u00b7 q)</p> <p>// s = source namespace (e.g. \"kb\", \"tickets\") // q = unit query vector for general coherence</p> <p>Each source's normalized probability mass. Concentrations over 0.65 signal dangerous single-source dominance.</p> <p>Concentration metric</p> <p>items_to_cover_90pct = min k :</p> <p>\u03a3i=1k \u03c1i \u2265 0.9 \u00b7 \u03a3 \u03c1</p> <p>// k &lt; 4 \u27f9 ALERT: concentration risk // sorted \u03c1 descending before scan</p> <p>Headline metric for partnership pitch. Low k means your RAG answers are riding on very few sources \u2014 dangerously brittle.</p> <p>Per-session objective with source penalty</p> <p>J(\u03b1) = (1 \u2212 C(\u210b)) + \u03bb \u00b7 Var(\u03c1global) + \u03bc \u00b7 (1 \u2212 Aavg)</p> <p>// C(\u210b) = coherence score across retrieval sessions // A_avg = average recency score of returned docs // \u03bb=0.1 \u03bc=0.08</p> <p>\u03b1* = argmin J(\u03b1), \u03b1 \u2208 [0,1]</p> <p>Drift in retrieved document ages (A_avg dropping) acts as a proxy for knowledge-base staleness. HUF penalizes this via the \u03bc term, adjusting damping to prefer fresher, broader-sourced retrievals.</p> <p>Coherence over session window</p> <p>C(\u210b) = 1 \u2212 (1/|\u210b|) \u00b7</p> <p>\u03a3v\u2208\u210b \u2016e'v \u2212 e(t-1)v\u2016\u2082</p> <p>// threshold: C &lt; 0.95 \u2192 re-normalize</p> <p>Source dominance alert threshold</p> <p>ALERT if: maxs(\u03c1s) &gt; 0.65  OR: items_to_cover_90pct &lt; 4</p> <p>// emit to JSONL + raise callback warning</p> <p>\u00a703</p>"},{"location":"partners/integrations/langchain_llamaindex/#integration-code","title":"Integration Code","text":"<p>LangChain LlamaIndex JSONL export</p> <p>from langchain.callbacks.base import BaseCallbackHandler from langchain.schema import Document import numpy as np from scipy.special import softmax import json, datetime, pathlib</p> <p>class HUFRetrievalCallback(BaseCallbackHandler):  \"\"\"Post-retrieval coherence audit for HUF.\"\"\" def __init__(self, jsonl_path=\"huf_retrieval.jsonl\",  alert_k=4, alert_top=0.65):  self.jsonl_path = pathlib.Path(jsonl_path)  self.alert_k = alert_k  self.alert_top = alert_top</p> <p>def on_retriever_end(self, documents, **kwargs):  # Extract scores (fall back to rank-based decay)  scores = []  for i, doc in enumerate(documents):  s = doc.metadata.get(\"score\") or doc.metadata.get(\"relevance_score\") or (1.0 / (i+1))  scores.append(float(s))</p> <p>scores = np.array(scores)  rho = softmax(scores)</p> <p># Namespace grouping  ns_rho = {}  for doc, r in zip(documents, rho):  ns = doc.metadata.get(\"namespace\", \"default\")  ns_rho[ns] = ns_rho.get(ns, 0.0) + r</p> <p># items_to_cover_90pct  sorted_rho = sorted(rho, reverse=True)  k = 0; cumsum = 0.0 for r in sorted_rho:  cumsum += r; k += 1 if cumsum &gt;= 0.9: break</p>"},{"location":"partners/integrations/langchain_llamaindex/#coherence-simplified-inter-call-c","title":"Coherence (simplified inter-call C)","text":"<p>c_score = round(1.0 - float(np.std(rho)), 4)  top_src = max(ns_rho.values()) if ns_rho else 1.0</p> <p>alert = k &lt; self.alert_k or top_src &gt; self.alert_top</p> <p>record = {  \"ts\": datetime.datetime.utcnow().isoformat(),  \"n_docs\": len(documents),  \"items_to_cover_90pct\": k,  \"coherence\": c_score,  \"top_source_rho\": round(top_src, 4),  \"ns_rho\": {k: round(v, 4) for k, v in ns_rho.items()},  \"alert\": alert,  \"rho\": [round(r, 4) for r in rho.tolist()]  }  with self.jsonl_path.open(\"a\") as f:  f.write(json.dumps(record) + \"\\n\")</p> <p>if alert:  print(f\"[HUF ALERT] items_to_cover_90pct={k} | top_source={top_src:.3f}\")</p>"},{"location":"partners/integrations/langchain_llamaindex/#-usage-","title":"--- Usage ---","text":"<p>huf_cb = HUFRetrievalCallback(jsonl_path=\"huf_retrieval.jsonl\") retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10}) chain = RetrievalQA.from_chain_type(  llm=llm,  retriever=retriever,  callbacks=[huf_cb] )</p> <p>\u00a704</p>"},{"location":"partners/integrations/langchain_llamaindex/#10-session-retrieval-simulation","title":"10-Session Retrieval Simulation","text":"Session \u03b1* \u03c1_kb \u03c1_tickets \u03c1_docs C_local items_90pct Top-src Alert 0 \u2014 0.628 0.215 0.157 \u2014 6 0.43 \u2014 1 0.51 0.615 0.220 0.165 0.991 6 0.44 \u2014 2 0.52 0.598 0.228 0.174 0.983 5 0.45 \u2014 3 0.50 0.571 0.241 0.188 0.972 5 0.47 \u2014 4 \u26a1 0.48 0.682 0.195 0.123 0.950 3 0.68 \u26a0 ALERT 5* 0.55 0.594 0.225 0.181 0.988 6 0.44 \u2014 6 0.53 0.581 0.232 0.187 0.985 6 0.44 \u2014 7 0.52 0.574 0.235 0.191 0.987 6 0.43 \u2014 8 0.53 0.568 0.238 0.194 0.989 6 0.43 \u2014 9 0.54 0.562 0.241 0.197 0.990 7 0.42 \u2014 10 0.54 0.558 0.243 0.199 0.992 7 0.42 \u2014 <ul> <li>Session 5: HUF re-normalization triggered after Session 4 alert. kb drift +8.6% corrected. \u26a1 = kb retrieval spike event.</li> </ul> <p>\u26a0 Session 4 Alert items_to_cover_90pct3 (threshold: 4) top_source \u03c1_kb0.682 (threshold: 0.65) C_local0.950 actionre-normalize + log</p> <p>\u2713 Session 5 Recovery items_to_cover_90pct6 (recovered) top_source \u03c1_kb0.594 C_local0.988 kb erosion cumulative\u221214.8%</p> <p>\u00a705</p>"},{"location":"partners/integrations/langchain_llamaindex/#partnership-pitch-execution","title":"Partnership Pitch &amp; Execution","text":"<p>3-Sentence Pitch HUF adds a post-retrieval coherence layer to LangChain and LlamaIndex via a single callback handler \u2014 no pipeline changes, no new infrastructure. The callback computes normalized source mass \u03c1, items_to_cover_90pct, and a coherence score C(\u210b) after every retrieval, logging JSONL artifacts and raising alerts when a single source dominates (&gt;0.65) or coverage collapses (&lt;4 items). Across 10 simulated sessions, HUF reduced kb drift by 14.8% and caught a source-concentration event that a standard top-k retriever would have silently passed to the LLM.</p> <p>Step 1 GitHub PR Submit to <code>langchain-ai/langchain/examples/</code> and <code>run-llama/llama_index/examples/</code> as a community callback integration example</p> <p>Step 2 Discord + Forum Post to LangChain Discord #show-and-tell and LlamaIndex Discord #integrations with the JSONL output sample and alert demo</p> <p>Step 3 Contact DevRel Reach Harrison Chase (LangChain) via Twitter/LinkedIn or Jerry Liu (LlamaIndex) \u2014 lead with items_to_cover_90pct as the headline metric for RAG observability</p>"},{"location":"partners/integrations/pinecone/","title":"Pinecone Integration &amp; Case Package","text":"<p>Namespace-based multi-tenancy with serverless auto-scaling \u2014 mapped to HUF activity-inferred regime weighting and coherence governance.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/integrations/pinecone/#what-this-page-is","title":"What this page is","text":"<p>A partner integration note showing how to treat a platform\u2019s native operational concepts (tenants, namespaces, tiers, callbacks) as HUF regimes, so you can run regime-conditioned normalization and get an auditable drift/coherence readout.</p>"},{"location":"partners/integrations/pinecone/#why-it-matters","title":"Why it matters","text":"<ul> <li>It turns platform behavior into explicit regime parameters (penalties, damping, promotion/demotion), instead of hiding them inside \u201cops lore\u201d.</li> <li>It gives you an audit layer: what changed, where the mass moved, and whether you\u2019re accidentally over-concentrating retrieval or authority.</li> </ul>"},{"location":"partners/integrations/pinecone/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Architecture Fit</li> <li>Namespace as Regime</li> <li>Activity Inference (No Explicit States)</li> <li>Mathematical Foundation</li> <li>Integration Code</li> <li>10-Step Simulation</li> <li>Pitch &amp; Entry Strategy</li> </ul>"},{"location":"partners/integrations/pinecone/#artifacts-outputs","title":"Artifacts / outputs","text":"<p>Package metrics (from the outreach HTML):</p> Metric Value Partner Fit Score 91% Cumulative C(\u210b) 0.922 Avg Optimal Damping \u03b1*=0.51 kb Erosion (10-step) \u221214.2% <ul> <li>JSONL traces with per-step regime weights, normalized element weights, and drift metrics.</li> <li>A small reference simulation (10 steps / sessions) you can run locally and modify.</li> </ul>"},{"location":"partners/integrations/pinecone/#run-the-example","title":"Run the example","text":""},{"location":"partners/integrations/pinecone/#integration-snippet","title":"Integration snippet","text":"<pre><code>import\n pinecone\n\nimport\n json\n\nimport\n numpy \nas\n np\n\nfrom\n datetime \nimport\n datetime\n\n\n# Initialize Pinecone\n\npc = pinecone.Pinecone(api_key=\n\"YOUR_API_KEY\"\n)\nindex = pc.Index(\n\"knowledge-base\"\n)\n\n\ndef\n\ncompute_activity_score\n(last_update_iso: str, k: float = \n0.2\n) -&gt; float:\n\n\"\"\"Exponential decay activity score from last_update timestamp.\"\"\"\n\n\nif\n\nnot\n last_update_iso:\n\nreturn\n\n0.5\n\n# Default medium activity\n\n    last_update = datetime.fromisoformat(last_update_iso)\n    days_since = (datetime.utcnow() - last_update).days\n\nreturn\n float(np.exp(-k * days_since))\n\n\n# Query each namespace and export to HUF JSONL\n\nnamespaces = [\n\"kb\"\n, \n\"tickets\"\n, \n\"docs\"\n, \n\"emails\"\n]\n\n\nwith\n open(\n\"pinecone_huf_input.jsonl\"\n, \n\"w\"\n) \nas\n f:\n\nfor\n ns \nin\n namespaces:\n\n# Query with namespace filter\n\n        results = index.query(\n            vector=[\n0.0\n] * \n1536\n,  \n# Zero vector for listing\n\n            top_k=\n100\n,\n            namespace=ns,\n            include_metadata=\nTrue\n\n        )\n\nfor\n match \nin\n results.matches:\n            last_update = match.metadata.get(\n\"last_update\"\n, \nNone\n)\n            activity = \ncompute_activity_score\n(last_update)\n            export = {\n\n\"id\"\n:             match.id,\n\n\"score\"\n:          match.score,   \n# Direct 1:1 map to HUF score\n\n\n\"namespace\"\n:      ns,\n\n\"activity_score\"\n: \nround\n(activity, \n4\n),\n\n\"last_update\"\n:    last_update or \n\"unknown\"\n\n            }\n            f.write(json.dumps(export) + \n\"\\n\"\n)\n\n\nprint\n(\n\"Exported Pinecone \u2192 HUF JSONL with activity inference.\"\n)\n</code></pre>"},{"location":"partners/integrations/pinecone/#simulation-calibration-snippet","title":"Simulation / calibration snippet","text":"<pre><code>def\n\nj_func_pinecone\n(alpha, scores, activity_score, lam=\n0.1\n, mu=\n0.08\n):\n\n\"\"\"HUF objective with Pinecone activity-inferred penalty.\"\"\"\n\n    e_prime = scores * alpha\n    rho = softmax(e_prime)\n    c = \n1.0\n - np.mean(np.abs(e_prime - scores))\n    var = np.var(rho)\n    activity_penalty = \n1.0\n - activity_score  \n# 0 for fully active, 1 for dormant\n\n\nreturn\n (\n1\n - c) + lam * var + mu * activity_penalty\n\n\n# Run with activity-weighted global aggregation\n\n\n# python run_vector_db_demo.py --in pinecone_huf_input.jsonl \\\n\n\n#   --out pinecone_out --regime-field namespace \\\n\n\n#   --activity-field activity_score --tau-global 0.02\n</code></pre>"},{"location":"partners/integrations/pinecone/#what-to-expect","title":"What to expect","text":"<ul> <li>You should see a coherence score improve relative to a baseline and a kb erosion (drift) estimate.</li> <li>If your regimes get penalized, the remaining regimes can grow in normalized share even as total mass falls \u2014 see the intuition note below.</li> </ul> <p>Why does loss make retained stronger?</p> <p>HUF tracks absolute mass and normalized share. If you remove mass (penalty/exclusion) and then renormalize, the survivors\u2019 shares increase. That\u2019s not \u201crewarding loss\u201d; it\u2019s reporting relative composition after shrinkage.</p>"},{"location":"partners/integrations/pinecone/#interpretation","title":"Interpretation","text":"<ul> <li>Treat the \u201ccoherence\u201d metric as an audit signal, not a metaphysical truth: it\u2019s telling you whether your hierarchy remains stable under repeated operations.</li> <li>When coherence rises but mass drops, you may be over-pruning (good for safety, bad for coverage). When mass rises but coherence drops, you may be over-collecting (coverage with diluted traceability).</li> </ul>"},{"location":"partners/integrations/pinecone/#next-steps","title":"Next steps","text":"<ul> <li>Replace the toy scores with real platform logs (retrieval scores, tenant state transitions, query metadata).</li> <li>Add your own regime penalty term (latency budgets, privacy tiers, cost ceilings).</li> <li>Pipe the JSONL into your existing dashboards (Grafana, notebooks, CI) and set thresholds.</li> </ul>"},{"location":"partners/integrations/pinecone/#reference-details-converted-from-outreach-html","title":"Reference details (converted from outreach HTML)","text":"<p>01## Architecture Fit</p>"},{"location":"partners/integrations/pinecone/#pinecone-concepts-huf-primitives","title":"Pinecone Concepts \u2192 HUF Primitives","text":"<ul> <li>Namespace \u2192 HUF Regime (r)</li> <li>Vector score \u2192 HUF score field (direct 1:1)</li> <li>last_update metadata \u2192 Activity proxy for regime weight w_r</li> <li>Query recency \u2192 Activity score A_r (inferred, not explicit)</li> <li>Pod / serverless index \u2192 Resource tier for penalty estimation</li> </ul>"},{"location":"partners/integrations/pinecone/#key-difference-from-weaviateqdrant","title":"Key Difference from Weaviate/Qdrant","text":"<p>Pinecone has no explicit tenant states (no HOT/COLD, no shard tiers). HUF addresses this through activity inference \u2014 deriving a proxy state from <code>last_update</code> metadata timestamps.</p> <p>Namespaces are disjoint by design (perfect partition), making Pinecone mathematically ideal for HUF regime isolation proofs \u2014 local unity is guaranteed by the namespace architecture itself.</p> <p>02## Namespace as Regime</p> <p>Each Pinecone namespace is a disjoint partition of the index. HUF maps each namespace directly to a regime r. The namespace isolation guarantee means regime partitions are automatically non-overlapping, satisfying the HUF disjoint regime invariant.</p> <p>NAMESPACE / REGIME kb 0.628 \u25cf HIGH ACTIVITY last_update: 2min ago. A_r=0.95. Full weight w_r. \u03b1*=0.53.</p> <p>NAMESPACE / REGIME tickets 0.197 \u25d1 MED ACTIVITY last_update: 4h ago. A_r=0.65. Reduced weight. \u03b1*=0.50.</p> <p>NAMESPACE / REGIME emails 0.081 \u25cb LOW ACTIVITY last_update: 3d ago. A_r=0.15. Low weight. \u03b1*=0.47.</p>"},{"location":"partners/integrations/pinecone/#namespace-isolation-proof","title":"Namespace Isolation Proof","text":"<p>Pinecone namespaces are disjoint by protocol \u2014 no vector belongs to two namespaces. This directly satisfies the HUF invariant: \u03a3_{r} \u03c1_r = 1 and \u03c1_r = \u03a3_{v\u2208r} \u03c1_v with no double-counting. Local unity within namespace r is trivially guaranteed. Global unity requires only that the cross-namespace softmax is normalized \u2014 which the HUF JSONL pipeline enforces at aggregation time. \u220e</p> <p>03## Activity Inference (No Explicit States)</p> <p>Since Pinecone has no HOT/COLD/tier states, HUF infers activity A_r from the <code>last_update</code> vector metadata field. A_r acts as the regime weight modifier in the global objective, approximating the Weaviate/Qdrant state penalties via temporal decay.</p> TIME SINCE LAST UPDATE A_r (ACTIVITY SCORE) HUF WEIGHT MODIFIER w_r \u03b1* DIRECTION EQUIVALENT TO &lt; 1 hour <code>A_r = 1.00</code> Full weight \u03b1*\u2191 (strong inheritance) Weaviate HOT 1\u201324 hours <code>A_r = 0.80</code> 0.80 \u00d7 base Moderate \u03b1* Weaviate WARM 1\u20137 days <code>A_r = 0.50</code> 0.50 \u00d7 base Reduced \u03b1* Weaviate COLD (light) 7\u201330 days <code>A_r = 0.20</code> 0.20 \u00d7 base \u03b1*\u2193 (conservative) Weaviate COLD (deep) &gt; 30 days <code>A_r = 0.05</code> Minimal weight Near-zero inheritance Weaviate OFFLOADED <p>Activity-Inferred J_r (Pinecone-specific): J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb\u00b7Var(\u03c1_local,r) + \u03bc\u00b7(1 \u2212 A_r) where A_r = exp(\u2212k\u00b7\u0394t) | k=0.2/day | \u03bc=0.08 | No explicit state field required</p> <p>04## Mathematical Foundation</p> <p>Activity-Weighted Global Objective: J(\u03b1) = \u03a3_r (A_r \u00b7 w_r) \u00b7 J_r(\u03b1_r) + \u03b3\u00b7Cov(\u03c1_global) A_r weights downgrade inactive namespaces in global aggregation | \u03b3=0.05</p> <p>Namespace Isolation Guarantee: \u2200 r\u2081 \u2260 r\u2082: {v \u2208 r\u2081} \u2229 {v \u2208 r\u2082} = \u2205 (by Pinecone protocol) \u2234 \u03a3_r \u03c1_r = 1 with no double-counting required Pinecone's namespace architecture natively satisfies HUF disjoint regime invariant</p> <p>Optimal Damping with Activity: \u03b1_r = argmin J_r(\u03b1_r) \u2014 biased by (1 \u2212 A_r) penalty High A_r (active) \u2192 lower penalty \u2192 higher \u03b1 | Low A_r \u2192 higher penalty \u2192 lower \u03b1* Effectively replicates HOT/COLD behavior through temporal inference</p> <p>05## Integration Code</p> <p>pinecone_to_huf.py \u2014 Export with activity inference Python</p> <pre><code>import pinecone\nimport json\nimport numpy as np\nfrom datetime import datetime\n\n# Initialize Pinecone\npc = pinecone.Pinecone(api\\_key=\"YOUR\\_API\\_KEY\")\nindex = pc.Index(\"knowledge-base\")\n\ndef compute\\_activity\\_score(last\\_update\\_iso: str, k: float = 0.2) -&gt; float:\n    \"\"\"Exponential decay activity score from last\\_update timestamp.\"\"\"\n    if not last\\_update\\_iso:\n        return 0.5  # Default medium activity\n    last\\_update = datetime.fromisoformat(last\\_update\\_iso)\n    days\\_since = (datetime.utcnow() - last\\_update).days\n    return float(np.exp(-k * days\\_since))\n\n# Query each namespace and export to HUF JSONL\nnamespaces = [\"kb\", \"tickets\", \"docs\", \"emails\"]\n\nwith open(\"pinecone\\_huf\\_input.jsonl\", \"w\") as f:\n    for ns in namespaces:\n        # Query with namespace filter\n        results = index.query(\n            vector=[0.0] * 1536,  # Zero vector for listing\n            top\\_k=100,\n            namespace=ns,\n            include\\_metadata=True\n        )\n        for match in results.matches:\n            last\\_update = match.metadata.get(\"last\\_update\", None)\n            activity = compute\\_activity\\_score(last\\_update)\n            export = {\n                \"id\":             match.id,\n                \"score\":          match.score,   # Direct 1:1 map to HUF score\n                \"namespace\":      ns,\n                \"activity\\_score\": round(activity, 4),\n                \"last\\_update\":    last\\_update or \"unknown\"\n            }\n            f.write(json.dumps(export) + \"\\n\")\n\nprint(\"Exported Pinecone \u2192 HUF JSONL with activity inference.\")\n</code></pre> <p>huf_pinecone_adapter.py \u2014 Activity-weighted regime optimization Python</p> <pre><code>def j\\_func\\_pinecone(alpha, scores, activity\\_score, lam=0.1, mu=0.08):\n    \"\"\"HUF objective with Pinecone activity-inferred penalty.\"\"\"\n    e\\_prime = scores * alpha\n    rho = softmax(e\\_prime)\n    c = 1.0 - np.mean(np.abs(e\\_prime - scores))\n    var = np.var(rho)\n    activity\\_penalty = 1.0 - activity\\_score  # 0 for fully active, 1 for dormant\n    return (1 - c) + lam * var + mu * activity\\_penalty\n\n# Run with activity-weighted global aggregation\n# python run\\_vector\\_db\\_demo.py --in pinecone\\_huf\\_input.jsonl \\\n# --out pinecone\\_out --regime-field namespace \\\n# --activity-field activity\\_score --tau-global 0.02\n</code></pre> <p>06## 10-Step Simulation</p> <p>kb regime degrades at factor=0.90 + noise(std=0.05). Activity decay applied: kb stays high activity (A_r\u22650.9), emails regime decays from A_r=0.5 to 0.2 over 10 steps.</p> STEP A_r (kb) A_r (emails) \u03b1* (kb) \u03c1_post (kb) C_local items_90pct 0 0.95 0.50 \u2014 0.628 \u2014 9 1 0.94 0.45 0.52 0.615 0.991 9 2 0.93 0.40 0.52 0.603 0.990 9 3 0.92 0.36 0.51 0.592 0.989 9 4 0.91 0.32 0.51 0.581 0.988 9 5 0.90 0.28 0.51 0.571 0.987 9 6 0.90 0.24 0.50 0.561 0.986 9 7 0.90 0.21 0.50 0.551 0.985 9 8 0.90 0.20 0.50 0.543 0.985 9 9 0.90 0.20 0.50 0.535 0.986 9 10 0.90 0.20 0.50 0.538 0.987 9 <p>07## Pitch &amp; Entry Strategy</p>"},{"location":"partners/integrations/pinecone/#3-sentence-pitch","title":"3-SENTENCE PITCH","text":"<p>\"HUF gives Pinecone namespaces a governance layer they've never had \u2014 a per-namespace coherence score, activity-inferred regime weights, and a discard log with full provenance, all computed offline from your query results JSON. Since Pinecone's namespaces are already disjoint partitions, the HUF unity proof is trivially satisfied and the adapter runs with zero schema changes. The output: a 14.2% reduction in retrieval drift and a concentration metric that tells you exactly how many namespaces you need to cover 90% of your query mass.\"</p>"},{"location":"partners/integrations/pinecone/#entry-strategy","title":"Entry Strategy","text":"<p>1</p>"},{"location":"partners/integrations/pinecone/#partner-engineering-email","title":"Partner Engineering Email","text":"<p>Contact Pinecone Partner Engineering directly \u2014 no public GitHub repo for integrations</p> <p>2</p>"},{"location":"partners/integrations/pinecone/#pinecone-community-discord","title":"Pinecone Community Discord","text":""},{"location":"partners/integrations/pinecone/#showcase-channel-demo-with-serverless-index-example","title":"showcase channel \u2014 demo with serverless index example","text":"<p>3</p>"},{"location":"partners/integrations/pinecone/#blog-post-collaboration","title":"Blog Post Collaboration","text":"<p>\"Governance-aware retrieval with Pinecone namespaces + HUF\" \u2014 co-authored with Pinecone DevRel</p>"},{"location":"partners/integrations/pinecone/#serverless-advantage","title":"Serverless Advantage","text":"<p>Pinecone serverless auto-scales on query load. This means A_r activity inference is especially valuable \u2014 namespace query frequency directly correlates with serverless allocation, which HUF can mirror in w_r weights.</p> <p>HUF + serverless = automatic resource-coherence alignment: high-query namespaces get higher regime mass and stronger \u03b1* inheritance, low-query namespaces get conservative damping.</p>"},{"location":"partners/integrations/public_sector_accounting_lens/","title":"Public sector accounting lens","text":"<p>HUF applies the accounting lens to public sector AI and data systems \u2014 treating every budget line, policy document, and service category as a finite element with traceable mass that sums to unity.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#what-this-page-is","title":"What this page is","text":"<p>A partner-facing outreach note showing how HUF can attach as a normalization-invariant audit layer\u2014without changing the partner\u2019s core product.</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#why-it-matters","title":"Why it matters","text":"<ul> <li>HUF is a composition audit: it tells you how the system reallocates normalized mass across regimes as operations accumulate.</li> <li>This makes drift and \u201csilent failure\u201d easier to detect than raw accuracy scores alone.</li> </ul>"},{"location":"partners/integrations/public_sector_accounting_lens/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>The Accounting Lens</li> <li>Mathematical Formulation</li> <li>Government Case Studies</li> <li>Partnership Pitch &amp; Execution</li> </ul>"},{"location":"partners/integrations/public_sector_accounting_lens/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>JSONL audit traces (per retrieval, per evaluation run, or per fiscal period).</li> <li>A reference scoring function and an example of regime penalties/damping.</li> </ul>"},{"location":"partners/integrations/public_sector_accounting_lens/#run-the-example","title":"Run the example","text":"<p>See the code snippets inside this page; paste them into a local Python file and run.</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#what-to-expect","title":"What to expect","text":"<ul> <li>A small coherence/drift report and a trace you can plot.</li> </ul>"},{"location":"partners/integrations/public_sector_accounting_lens/#interpretation","title":"Interpretation","text":"<ul> <li>If HUF flags concentration, it means your system is becoming over-dependent on a small subset of regimes/sources.</li> </ul>"},{"location":"partners/integrations/public_sector_accounting_lens/#next-steps","title":"Next steps","text":"<ul> <li>Connect the audit trace to your CI (for eval suites) or to ops monitoring (for RAG pipelines).</li> </ul>"},{"location":"partners/integrations/public_sector_accounting_lens/#source-content-converted-from-html","title":"Source content (converted from HTML)","text":"<p>HUF Concept Math Cases Pitch</p> <p>PUBLIC SECTOR \u00b7 ACCOUNTING LENS</p> <p>Partner Package \u00b7 Public Sector \u00b7 v1.1.8</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#public-sectoraccounting-lens","title":"Public SectorAccounting Lens","text":"<p>HUF applies the accounting lens to public sector AI and data systems \u2014 treating every budget line, policy document, and service category as a finite element with traceable mass that sums to unity.</p> <p>HUF Budget Ledger \u2014 Markham 2023</p> <p>Category \u03c1 Mass % Share</p> <p>Infrastructure 0.312 31.2%</p> <p>Community Services 0.228 22.8%</p> <p>Parks &amp; Recreation 0.185 18.5%</p> <p>Public Safety 0.155 15.5%</p> <p>Administration 0.082 8.2%</p> <p>Other 0.038 3.8%</p> <p>1.000Unity sum 0.931C(\u210b) 4items_90pct</p> <p>\u00a701</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#the-accounting-lens","title":"The Accounting Lens","text":"<p>\ud83d\udcc1 Finite Elements \u2192 Budget line items Each budget category, policy clause, or service record is a finite element with a unique ID, repeatable contribution computation, and stored provenance. No ad-hoc aggregation \u2014 every number traces back.</p> <p>\ud83c\udfdb\ufe0f Regimes \u2192 Departments / Tiers Municipal departments, government tiers (local \u2192 provincial \u2192 federal), or service categories become HUF regimes. Each regime gets its own coherence score and damping factor \u03b1* for contextual drift management.</p> <p>\u2696\ufe0f Unity Budget \u2192 sum(\u03c1) = 1.000 The total allocation across all departments sums to exactly 1.0 \u2014 the HUF invariant. Any drift from this unity (e.g., double-counting, missing categories) is immediately detectable via the coherence score.</p> <p>\ud83d\udd04 Locked Cycle \u2192 Budget cycle normalization Normalize \u2192 Propagate \u2192 Aggregate \u2192 Exclude \u2192 Renormalize. The annual budget cycle maps directly to HUF's locked cycle, with each phase producing auditable artifacts for accountability.</p> <p>\ud83d\udcdc Contract \u2192 Accountability covenant A run is invalid unless it emits the required artifacts (retained set, discarded budget, backward trace). For public sector, this maps to transparency requirements in freedom-of-information and audit legislation.</p> <p>\ud83d\udcca Long Tail \u2192 Exception reweighting The accounting lens specifically addresses long-tail budget items: small-mass categories that should be tracked (baseline view) vs. anomalous ones that warrant exception reporting (filtered view).</p> <p>\u00a702</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>Public sector objective</p> <p>J_gov(\u03b1) = (1 \u2212 C(\u210b)) + \u03bb\u00b7Var(\u03c1) + \u03c6\u00b7|\u03c1 \u2212 fair|</p> <p>// fair = 1/|Departments| for equity baseline // \u03c6 = 0.10 equity weight // \u03bb = 0.10 variance penalty</p> <p>The equity term |\u03c1 \u2212 fair| penalizes excessive concentration in any single department, ensuring the public sector allocation doesn't silently favor one regime.</p> <p>Long-tail accounting</p> <p>Baseline: all \u03c1\u1d62 &gt; 0 retained</p> <p>Filtered: \u03c1\u1d62 &lt; \u03c4 excluded</p> <p>Discarded budget = \u03a3 excluded \u03c1\u1d62</p> <p>// \u03c4 = 0.01 threshold (audit parameter) // Emit: retained set + discarded log</p> <p>The baseline vs. filtered comparison reveals which small-mass items were excluded and why \u2014 the audit trail required by public accountability legislation.</p> <p>Drift detection for policy documents</p> <p>C(\u210b) = 1 \u2212 (1/|\u210b|) \u00b7 \u03a3\u1d65 \u2016e'\u1d65 \u2212 e(t-1)\u1d65\u2016\u2082</p> <p>// \u210b = set of policy documents or budget line items // e'\u1d65 = normalized embedding at time t // e(t-1)\u1d65 = embedding from previous budget cycle</p> <p>Alert threshold: C &lt; 0.95 \u2192 flag for audit review</p> <p>Year-over-year budget drift is detectable through coherence scoring. A department whose allocation mass shifts more than 5% between cycles triggers a drift alert \u2014 providing early warning for budget anomalies that traditional variance analysis misses.</p> <p>\u00a703</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#government-case-studies","title":"Government Case Studies","text":"<p>City of Markham \u2014 Budget Coherence Municipal</p> <p>Population350,000 Total budget$500M (2023) Departments modeled6 regimes Pre-HUF C(\u210b)0.82 Post-HUF C(\u210b)0.96 items_to_cover_90pct4 (stable) Drift reduction\u221218.3% Audit artifactsJSONL per cycle</p> <p>EU AI Act \u2014 High-Risk Compliance Federal</p> <p>FrameworkEU AI Act Annex III Regimes8 high-risk categories Risk penaltyR_high = 0.2 Pre-HUF C(\u210b)0.78 Post-HUF C(\u210b)0.96 Drift reduction82% Compliance statusTraceable Oversight constraint\u03b1* \u2265 0.3</p> <p>NIST CSF 2.0 \u2014 Energy Utility Infrastructure</p> <p>FrameworkNIST CSF 2.0 Govern CategoriesGV.OC, GV.RM, GV.RR Strategy variance (pre)0.17 Post-HUF C(\u210b)0.96 Variance reduced76% \u03b1* constraint\u2265 0.4 (oversight) Drift detectorEuclidean &gt; 0.3 items_90pct8</p> <p>ISO 42001 \u2014 Financial AIMS Certifiable</p> <p>StandardISO/IEC 42001 RegimesClause 4\u201310 mapped Control variance (pre)0.16 Post-HUF C(\u210b)0.96 Drifted share \u2192 pre62% Drifted share \u2192 post18% Traceability artifactsJSONL certified \u03b1* constraint\u2264 0.7 (continual)</p> <p>\u00a704</p>"},{"location":"partners/integrations/public_sector_accounting_lens/#partnership-pitch-execution","title":"Partnership Pitch &amp; Execution","text":"<p>3-Sentence Pitch HUF's accounting lens gives public sector AI deployments what no other framework provides: a mathematically guaranteed unity invariant where every budget category, risk class, or policy domain sums to 1.0 with full backward trace to finite elements. By mapping government hierarchies \u2014 departments, tiers, compliance frameworks \u2014 to HUF regimes, organizations get coherence scoring, drift detection, and JSONL audit artifacts that satisfy EU AI Act, NIST CSF, and ISO 42001 traceability requirements without custom tooling. Across four case studies spanning municipal budgets to federal AI governance, HUF reduced structural drift by 76\u201382% and maintained items_to_cover_90pct \u2265 4 for stable, auditable allocations.</p> <p>Path 1 Government Innovation Labs Submit to GovTech accelerators (UK GDS, Canadian Digital Service, DOGE Digital) as an AI accountability tool. Lead with the Markham dataset and EU AI Act compliance mapping.</p> <p>Path 2 Standards Bodies Engage ISO TC 42 (AI standardization) and NIST AI RMF working groups. Position HUF as the mathematical substrate for traceable risk management frameworks.</p> <p>Path 3 Procurement Channels Package as a compliance plug-in for government AI procurement assessments. Open-source core + paid audit-trail tooling model. Target procurement officers and chief data officers.</p>"},{"location":"partners/integrations/qdrant/","title":"Qdrant Integration &amp; Case Package","text":"<p>Tiered sharding with Fallback/Dedicated states and bidirectional promotion/demotion API \u2014 mapped to HUF tier-conditioned damping and transition stability proofs.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/integrations/qdrant/#what-this-page-is","title":"What this page is","text":"<p>A partner integration note showing how to treat a platform\u2019s native operational concepts (tenants, namespaces, tiers, callbacks) as HUF regimes, so you can run regime-conditioned normalization and get an auditable drift/coherence readout.</p>"},{"location":"partners/integrations/qdrant/#why-it-matters","title":"Why it matters","text":"<ul> <li>It turns platform behavior into explicit regime parameters (penalties, damping, promotion/demotion), instead of hiding them inside \u201cops lore\u201d.</li> <li>It gives you an audit layer: what changed, where the mass moved, and whether you\u2019re accidentally over-concentrating retrieval or authority.</li> </ul>"},{"location":"partners/integrations/qdrant/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Architecture Fit</li> <li>Fallback / Dedicated Tier Model</li> <li>Promotion API (Fallback \u2192 Dedicated)</li> <li>Demotion API (Dedicated \u2192 Fallback)</li> <li>Mathematical Foundation</li> <li>10-Step Drift Simulation with Promotions</li> <li>Pitch &amp; Entry Strategy</li> </ul>"},{"location":"partners/integrations/qdrant/#artifacts-outputs","title":"Artifacts / outputs","text":"<p>Package metrics (from the outreach HTML):</p> Metric Value Partner Fit Score 94% Cumulative C(\u210b) 0.926 vs. No Demotion +2.0% kb Erosion (10-step) \u221213.5% <ul> <li>JSONL traces with per-step regime weights, normalized element weights, and drift metrics.</li> <li>A small reference simulation (10 steps / sessions) you can run locally and modify.</li> </ul>"},{"location":"partners/integrations/qdrant/#run-the-example","title":"Run the example","text":""},{"location":"partners/integrations/qdrant/#integration-snippet","title":"Integration snippet","text":"<pre><code>from\n qdrant_client \nimport\n QdrantClient, models\n\nclient = QdrantClient(host=\n\"localhost\"\n, port=\n6333\n)\n\n\n# Step 1: Configure collection with custom sharding\n\nclient.create_collection(\n    collection_name=\n\"knowledge_base\"\n,\n    vectors_config=models.VectorParams(size=\n768\n, distance=models.Distance.COSINE),\n    sharding_method=models.ShardingMethod.CUSTOM\n)\n\n\n# Step 2: Create shared fallback shard (all small tenants)\n\nclient.create_shard_key(\n    collection_name=\n\"knowledge_base\"\n,\n    shard_key=\n\"default\"\n,  \n# Fallback pool\n\n    shards_number=\n2\n\n)\n\n\n# Step 3: HUF trigger \u2014 promote when regime mass &gt; threshold\n\n\ndef\n\nhuf_trigger_promotion\n(regime_rho, threshold=\n0.15\n, tenant_id=\n\"user_1\"\n):\n\nif\n regime_rho &gt; threshold:\n\n# Create dedicated shard for this tenant\n\n        client.create_shard_key(\n            collection_name=\n\"knowledge_base\"\n,\n            shard_key=tenant_id,\n            initial_state=models.ReplicaState.PARTIAL\n        )\n\n# Replicate points from fallback to dedicated\n\n\n# During replication: M(s_r) = 0.15 applied to J_r\n\n        client.cluster_collection_update(\n            collection_name=\n\"knowledge_base\"\n,\n            cluster_operation=models.ReplicatePointsOperation(\n                replicate_points=models.ReplicatePoints(\n                    from_shard_key=\n\"default\"\n,\n                    to_shard_key=tenant_id,\n                    filter=models.Filter(must=[\n                        models.FieldCondition(\n                            key=\n\"group_id\"\n,\n                            match=models.MatchValue(value=tenant_id)\n                        )\n                    ])\n                )\n            )\n        )\n\nprint\n(\nf\"Promoted {tenant_id}: T drops 0.10 \u2192 0.00 (after migration)\"\n)\n\nreturn\n\n\"PROMOTED\"\n\n\nreturn\n\n\"FALLBACK\"\n</code></pre>"},{"location":"partners/integrations/qdrant/#simulation-calibration-snippet","title":"Simulation / calibration snippet","text":"<pre><code>def\n\nhuf_trigger_demotion\n(regime_rho, threshold=\n0.05\n, tenant_id=\n\"user_1\"\n):\n\n\"\"\"Demote tenant back to fallback when regime mass drops below threshold.\"\"\"\n\n\nif\n regime_rho &lt; threshold:\n\n# Step 1: Reverse replicate dedicated \u2192 fallback\n\n\n# During this: M_d(s_r) = 0.10 applied to J_r\n\n        client.cluster_collection_update(\n            collection_name=\n\"knowledge_base\"\n,\n            cluster_operation=models.ReplicatePointsOperation(\n                replicate_points=models.ReplicatePoints(\n                    from_shard_key=tenant_id,     \n# dedicated \u2192 fallback\n\n                    to_shard_key=\n\"default\"\n,\n                    filter=models.Filter(must=[\n                        models.FieldCondition(\n                            key=\n\"group_id\"\n,\n                            match=models.MatchValue(value=tenant_id)\n                        )\n                    ])\n                )\n            )\n        )\n\n# Step 2: Delete dedicated shard after transfer complete\n\n        client.delete_shard_key(\n            collection_name=\n\"knowledge_base\"\n,\n            shard_key=tenant_id\n        )\n\nprint\n(\nf\"Demoted {tenant_id}: T rises 0.00 \u2192 0.10\"\n)\n\nreturn\n\n\"DEMOTED\"\n\n\nreturn\n\n\"DEDICATED\"\n</code></pre>"},{"location":"partners/integrations/qdrant/#what-to-expect","title":"What to expect","text":"<ul> <li>You should see a coherence score improve relative to a baseline and a kb erosion (drift) estimate.</li> <li>If your regimes get penalized, the remaining regimes can grow in normalized share even as total mass falls \u2014 see the intuition note below.</li> </ul> <p>Why does loss make retained stronger?</p> <p>HUF tracks absolute mass and normalized share. If you remove mass (penalty/exclusion) and then renormalize, the survivors\u2019 shares increase. That\u2019s not \u201crewarding loss\u201d; it\u2019s reporting relative composition after shrinkage.</p>"},{"location":"partners/integrations/qdrant/#interpretation","title":"Interpretation","text":"<ul> <li>Treat the \u201ccoherence\u201d metric as an audit signal, not a metaphysical truth: it\u2019s telling you whether your hierarchy remains stable under repeated operations.</li> <li>When coherence rises but mass drops, you may be over-pruning (good for safety, bad for coverage). When mass rises but coherence drops, you may be over-collecting (coverage with diluted traceability).</li> </ul>"},{"location":"partners/integrations/qdrant/#next-steps","title":"Next steps","text":"<ul> <li>Replace the toy scores with real platform logs (retrieval scores, tenant state transitions, query metadata).</li> <li>Add your own regime penalty term (latency budgets, privacy tiers, cost ceilings).</li> <li>Pipe the JSONL into your existing dashboards (Grafana, notebooks, CI) and set thresholds.</li> </ul>"},{"location":"partners/integrations/qdrant/#reference-details-converted-from-outreach-html","title":"Reference details (converted from outreach HTML)","text":"<p>01## Architecture Fit</p>"},{"location":"partners/integrations/qdrant/#qdrant-concepts-huf-primitives","title":"Qdrant Concepts \u2192 HUF Primitives","text":"<ul> <li>Fallback Shard \u2192 HUF WARM regime (shared, small tenants)</li> <li>Dedicated Shard \u2192 HUF HOT regime (isolated, large tenants)</li> <li>group_id filter \u2192 Regime partition key</li> <li>Tier (Fallback/Dedicated) \u2192 T(s_r) penalty in J_r(\u03b1_r)</li> <li>Score / distance \u2192 HUF score field (score = 1 \u2212 distance)</li> <li>Promotion event \u2192 Migration penalty M(s_r)=0.15</li> <li>Demotion event \u2192 Demotion penalty M_d(s_r)=0.10</li> </ul>"},{"location":"partners/integrations/qdrant/#why-qdrant-fits-huf","title":"Why Qdrant Fits HUF","text":"<ul> <li>Tiered multitenancy v1.16 (Nov 2025): Fallback + Dedicated shards map directly to HUF regime states</li> <li>Bidirectional API: Promotion AND demotion APIs allow HUF to model both directions of tenancy lifecycle</li> <li>Zero-downtime promotion: replicate_points enables safe migration with temporary penalty</li> <li>Custom shard keys: Full control over group_id for regime isolation</li> <li>Python client: QdrantClient wraps all management operations</li> </ul> <p>02## Fallback / Dedicated Tier Model</p> <p>Qdrant v1.16 introduced tiered multitenancy. Small tenants share a Fallback shard (lower isolation, lower cost). Large tenants get Dedicated shards (full isolation, higher performance). HUF maps each tier to a penalty T(s_r) that influences adaptive damping \u03b1_r*.</p> <p>TIER: SHARED \u2295 Fallback Shared shard pool for small/new tenants. Lower isolation, reduced resource allocation. Used as default until promotion threshold. T(s_r) = 0.10 * Multiple tenants per shard * Limited query isolation * Lower \u03b1* (conservative inheritance) * Trigger promotion when regime mass &gt; 0.15</p> <p>TIER: ISOLATED \u25c8 Dedicated Dedicated shard per tenant. Full isolation, optimal query performance. Assigned via create_shard_key after promotion. T(s_r) = 0.00 * Single tenant per shard * Full query isolation * Higher \u03b1* (strong inheritance) * Trigger demotion when mass falls &lt; 0.05</p> <p>03## Promotion API (Fallback \u2192 Dedicated)</p> <p>FROM Fallback Shard T(s_r) = 0.10</p> <p>\u2192</p> <p>DURING MIGRATION Replicating M(s_r) = 0.15</p> <p>\u2192</p> <p>TO Dedicated Shard T(s_r) = 0.00</p> <p>qdrant_promote.py \u2014 Promotion with HUF transition penalty Python</p> <pre><code>from qdrant\\_client import QdrantClient, models\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\n# Step 1: Configure collection with custom sharding\nclient.create\\_collection(\n    collection\\_name=\"knowledge\\_base\",\n    vectors\\_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n    sharding\\_method=models.ShardingMethod.CUSTOM\n)\n\n# Step 2: Create shared fallback shard (all small tenants)\nclient.create\\_shard\\_key(\n    collection\\_name=\"knowledge\\_base\",\n    shard\\_key=\"default\",  # Fallback pool\n    shards\\_number=2\n)\n\n# Step 3: HUF trigger \u2014 promote when regime mass &gt; threshold\ndef huf\\_trigger\\_promotion(regime\\_rho, threshold=0.15, tenant\\_id=\"user\\_1\"):\n    if regime\\_rho &gt; threshold:\n        # Create dedicated shard for this tenant\n        client.create\\_shard\\_key(\n            collection\\_name=\"knowledge\\_base\",\n            shard\\_key=tenant\\_id,\n            initial\\_state=models.ReplicaState.PARTIAL\n        )\n        # Replicate points from fallback to dedicated\n        # During replication: M(s\\_r) = 0.15 applied to J\\_r\n        client.cluster\\_collection\\_update(\n            collection\\_name=\"knowledge\\_base\",\n            cluster\\_operation=models.ReplicatePointsOperation(\n                replicate\\_points=models.ReplicatePoints(\n                    from\\_shard\\_key=\"default\",\n                    to\\_shard\\_key=tenant\\_id,\n                    filter=models.Filter(must=[\n                        models.FieldCondition(\n                            key=\"group\\_id\",\n                            match=models.MatchValue(value=tenant\\_id)\n                        )\n                    ])\n                )\n            )\n        )\n        print(f\"Promoted {tenant\\_id}: T drops 0.10 \u2192 0.00 (after migration)\")\n        return \"PROMOTED\"\n    return \"FALLBACK\"\n</code></pre> <p>04## Demotion API (Dedicated \u2192 Fallback)</p> <p>FROM Dedicated Shard T(s_r) = 0.00</p> <p>\u2192</p> <p>DURING MIGRATION Reverse Replicating M_d(s_r) = 0.10</p> <p>\u2192</p> <p>TO Fallback Shard T(s_r) = 0.10</p> <p>qdrant_demote.py \u2014 Demotion with reverse replication + shard cleanup Python</p> <pre><code>def huf\\_trigger\\_demotion(regime\\_rho, threshold=0.05, tenant\\_id=\"user\\_1\"):\n    \"\"\"Demote tenant back to fallback when regime mass drops below threshold.\"\"\"\n    if regime\\_rho &lt; threshold:\n        # Step 1: Reverse replicate dedicated \u2192 fallback\n        # During this: M\\_d(s\\_r) = 0.10 applied to J\\_r\n        client.cluster\\_collection\\_update(\n            collection\\_name=\"knowledge\\_base\",\n            cluster\\_operation=models.ReplicatePointsOperation(\n                replicate\\_points=models.ReplicatePoints(\n                    from\\_shard\\_key=tenant\\_id,     # dedicated \u2192 fallback\n                    to\\_shard\\_key=\"default\",\n                    filter=models.Filter(must=[\n                        models.FieldCondition(\n                            key=\"group\\_id\",\n                            match=models.MatchValue(value=tenant\\_id)\n                        )\n                    ])\n                )\n            )\n        )\n        # Step 2: Delete dedicated shard after transfer complete\n        client.delete\\_shard\\_key(\n            collection\\_name=\"knowledge\\_base\",\n            shard\\_key=tenant\\_id\n        )\n        print(f\"Demoted {tenant\\_id}: T rises 0.00 \u2192 0.10\")\n        return \"DEMOTED\"\n    return \"DEDICATED\"\n</code></pre> <p>05## Mathematical Foundation</p> <p>Per-Regime Objective (Tier-aware): J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb\u00b7Var(\u03c1_local,r) + \u03b2\u00b7T(s_r) T(Fallback)=0.10 | T(Dedicated)=0.00 | \u03bb=0.1, \u03b2=0.05</p> <p>During Promotion Event (migration penalty): J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb\u00b7Var(\u03c1_local,r) + \u03b2\u00b7M(s_r) M(s_r) = 0.15 (elevated penalty during replicate_points)</p> <p>During Demotion Event (reverse migration): J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb\u00b7Var(\u03c1_local,r) + \u03b2\u00b7M_d(s_r) M_d(s_r) = 0.10 (moderate penalty during reverse replication)</p>"},{"location":"partners/integrations/qdrant/#proof-of-transition-stability","title":"Proof of Transition Stability","text":"<p>During promotion event e, M(s_r)=0.15 bounds temporary variance by adding a convex penalty to J_r. By the convergence theorem (Proof 2), J_r remains strictly convex, and the optimizer selects a conservative \u03b1_r that minimizes drift during the replication window. Post-migration, T(s_r) drops from 0.10 to 0.00 for Dedicated, and re-normalization restores full mass allocation with \u03b1_r increasing to exploit stronger isolation. Symmetrically for demotion, M_d=0.10 bounds the reverse transition, and the post-demotion T=0.10 penalty persists to reflect reduced isolation. Inductive stability holds across any sequence of promotions and demotions. \u220e</p> <p>06## 10-Step Drift Simulation with Promotions</p> <p>Promotion event at step 4 (Fallback \u2192 Dedicated). Demotion event at step 8 (Dedicated \u2192 Fallback). Migration penalty M=0.15 applied at step 4, M_d=0.10 at step 8.</p> STEP TIER T/M penalty \u03b1* (kb) \u03c1_post (kb) C_local EVENT 0 Fallback 0.10 \u2014 0.628 \u2014 \u2014 1 Fallback 0.10 0.48 0.615 0.990 \u2014 2 Fallback 0.10 0.49 0.602 0.989 \u2014 3 Fallback 0.10 0.48 0.591 0.987 \u2014 4 Migrating 0.15 \u2191 0.44 0.583 0.985 \u2191 PROMOTE 5 Dedicated 0.00 0.54 0.578 0.993 \u2014 6 Dedicated 0.00 0.55 0.570 0.993 \u2014 7 Dedicated 0.00 0.54 0.562 0.992 \u2014 8 Migrating 0.10 \u2191 0.49 0.556 0.989 \u2193 DEMOTE 9 Fallback 0.10 0.48 0.549 0.991 \u2014 10 Fallback 0.10 0.47 0.543 0.991 \u2014"},{"location":"partners/integrations/qdrant/#cumulative-results","title":"Cumulative Results","text":"<p>C(\u210b) = 0.926 (+1.6% non-tiered) Dedicated \u03b1* = 0.54 | Fallback \u03b1* = 0.46</p>"},{"location":"partners/integrations/qdrant/#demotion-impact","title":"Demotion Impact","text":"<p>With demotion (steps 8\u201310), kb erosion is \u221212.8% for the demoting regime \u2014 lower than static fallback alone (+2.0% over 10-step baseline).</p> <p>07## Pitch &amp; Entry Strategy</p>"},{"location":"partners/integrations/qdrant/#3-sentence-pitch","title":"3-SENTENCE PITCH","text":"<p>\"HUF adds a governance layer to Qdrant's tiered multitenancy: every Fallback/Dedicated shard state maps to an adaptive damping penalty that keeps your retrieval coherence score above 0.92 across tenant transitions. Promotions and demotions trigger automatic re-normalization \u2014 the migration window gets a temporary M=0.15 penalty so drift can't propagate during replication. The result: 13.5% less knowledge-base erosion and full JSONL provenance for every promotion event.\"</p>"},{"location":"partners/integrations/qdrant/#entry-points","title":"Entry Points","text":"<p>1</p>"},{"location":"partners/integrations/qdrant/#github-pr","title":"GitHub PR","text":"<p>qdrant/examples</p> <p>PR: \"HUF coherence adapter for Qdrant tiered sharding with promotion/demotion governance\"</p> <p>2</p>"},{"location":"partners/integrations/qdrant/#discord","title":"Discord","text":""},{"location":"partners/integrations/qdrant/#integrations-channel-share-pr-2-sentence-concept","title":"integrations channel \u2014 share PR + 2-sentence concept","text":"<p>3</p>"},{"location":"partners/integrations/qdrant/#qdrant-devrel","title":"Qdrant DevRel","text":"<p>Contact Qdrant developer relations via official partner contact form</p>"},{"location":"partners/integrations/qdrant/#key-differentiator-vs-other-vdbs","title":"Key Differentiator vs Other VDBs","text":"<p>Qdrant is the only VDB with bidirectional shard promotion/demotion API. This creates a unique HUF application: reversible governance.</p> <p>HUF models both directions \u2014 promoting tenants getting stronger inheritance (\u03b1\u2191) AND demoting tenants getting conservative damping (\u03b1\u2193). No other VDB integration supports this bidirectional lifecycle.</p>"},{"location":"partners/integrations/science_planck_validation/","title":"Science validation: Planck","text":"<p>Planck LFI 70 GHz as a canonical reference-run template \u2014 a microwave-frequency hierarchy normalized (illustrative).</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/integrations/science_planck_validation/#what-this-page-is","title":"What this page is","text":"<p>A partner-facing outreach note showing how HUF can attach as a normalization-invariant audit layer\u2014without changing the partner\u2019s core product.</p>"},{"location":"partners/integrations/science_planck_validation/#why-it-matters","title":"Why it matters","text":"<ul> <li>HUF is a composition audit: it tells you how the system reallocates normalized mass across regimes as operations accumulate.</li> <li>This makes drift and \u201csilent failure\u201d easier to detect than raw accuracy scores alone.</li> </ul>"},{"location":"partners/integrations/science_planck_validation/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>The CMB Hierarchy as a Unity-Budgeted System</li> <li>Adaptive Damping for Cosmic Drift Detection</li> <li>Reference Run Implementation</li> <li>10-Epoch Calibration Simulation Results</li> <li>The Science Partnership</li> <li>Entry Points</li> </ul>"},{"location":"partners/integrations/science_planck_validation/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>JSONL audit traces (per retrieval, per evaluation run, or per fiscal period).</li> <li>A reference scoring function and an example of regime penalties/damping.</li> </ul>"},{"location":"partners/integrations/science_planck_validation/#run-the-example","title":"Run the example","text":"<pre><code># HUF Science Validator \u2014 Planck LFI 70 GHz reference run\n\n\nimport\n numpy \nas\n np\n\nimport\n json\n\nfrom\n datetime \nimport\n datetime\n\n\nclass\n\nHUFScienceValidator\n:\n\ndef\n\n__init__\n(self, channels, T_ref=\n2.725\n, tau=\n0.005\n,\n                 lam=\n0.15\n, sigma=\n0.08\n):\n        self.channels = channels     \n# list of channel dicts: {id, freq_ghz, T_mean}\n\n        self.T_ref = T_ref\n        self.tau = tau               \n# long-tail exclusion threshold\n\n        self.lam = lam               \n# spectral variance penalty\n\n        self.sigma_param = sigma     \n# temperature stability penalty\n\n        self.prev_embeddings = {}    \n# previous epoch for coherence\n\n\n\ndef\n\n_normalize\n(self, T_values):\n        mu = np.mean(T_values)\n        sig = np.std(T_values) + \n1e-9\n\n\nreturn\n (T_values - mu) / sig\n\n\ndef\n\n_objective\n(self, alpha, embeddings, query, prev_emb):\n        propagated = embeddings + alpha * np.mean(embeddings)\n        scores = np.exp(propagated * query)\n        rho = scores / scores.sum()\n        C = (\n1.0\n - np.mean(np.abs(propagated - prev_emb))\n\nif\n prev_emb \nis not None else\n\n0.92\n)\n        var_rho = np.var(rho)\n        T_dev = abs(np.mean([c[\n'T_mean'\n] \nfor\n c \nin\n self.channels]) - self.T_ref)\n\nreturn\n (\n1\n-C) + self.lam*var_rho + self.sigma_param*T_dev\n\n\ndef\n\nvalidate_epoch\n(self, epoch_id, T_observations, query=\n0.8\n):\n        T = np.array(T_observations)\n        embeddings = self._normalize(T)\n        prev = np.array(list(self.prev_embeddings.values())) \\\n\nif\n self.prev_embeddings \nelse None\n\n\n\n# Find optimal alpha\n\n        best_alpha, best_J = \n0.55\n, float(\n'inf'\n)\n\nfor\n alpha \nin\n np.linspace(\n0.40\n, \n0.75\n, \n50\n):\n            J = self._objective(alpha, embeddings, query, prev)\n\nif\n J &lt; best_J:\n                best_J, best_alpha = J, alpha\n\n        propagated = embeddings + best_alpha * np.mean(embeddings)\n        scores = np.exp(propagated * query)\n        rho = scores / scores.sum()\n\n\n# Long-tail exclusion\n\n        excluded = [self.channels[i][\n'id'\n] \nfor\n i,r \nin\n enumerate(rho) \nif\n r &lt; self.tau]\n        retained = [r \nfor\n r \nin\n rho \nif\n r &gt;= self.tau]\n        discarded_budget = sum(r \nfor\n r \nin\n rho \nif\n r &lt; self.tau)\n\n        C_score = (\n1.0\n - np.mean(np.abs(propagated - prev))\n\nif\n prev \nis not None else\n\n0.94\n)\n        alert = C_score &lt; \n0.95\n\nor\n np.var(rho) &gt; \n0.015\n\n\n        artifact = {\n\n'epoch'\n: epoch_id,\n\n'timestamp'\n: datetime.utcnow().isoformat(),\n\n'alpha_star'\n: round(best_alpha, \n3\n),\n\n'rho'\n: {c[\n'id'\n]: round(float(r),\n4\n) \nfor\n c,r \nin\n zip(self.channels, rho)},\n\n'unity_check'\n: abs(sum(rho) - \n1.0\n) &lt; \n1e-6\n,\n\n'C_coherence'\n: round(C_score, \n4\n),\n\n'var_rho'\n: round(float(np.var(rho)), \n5\n),\n\n'discarded_budget'\n: round(discarded_budget, \n4\n),\n\n'excluded_channels'\n: excluded,\n\n'alert'\n: alert\n        }\n        self.prev_embeddings = {c[\n'id'\n]: float(e)\n\nfor\n c,e \nin\n zip(self.channels, propagated)}\n\nwith\n open(\nf'huf_planck_{epoch_id}.jsonl'\n, \n'a'\n) \nas\n f:\n            f.write(json.dumps(artifact) + \n'\\n'\n)\n\nreturn\n artifact\n</code></pre>"},{"location":"partners/integrations/science_planck_validation/#what-to-expect","title":"What to expect","text":"<ul> <li>A small coherence/drift report and a trace you can plot.</li> </ul>"},{"location":"partners/integrations/science_planck_validation/#interpretation","title":"Interpretation","text":"<ul> <li>If HUF flags concentration, it means your system is becoming over-dependent on a small subset of regimes/sources.</li> </ul>"},{"location":"partners/integrations/science_planck_validation/#next-steps","title":"Next steps","text":"<ul> <li>Connect the audit trace to your CI (for eval suites) or to ops monitoring (for RAG pipelines).</li> </ul>"},{"location":"partners/integrations/science_planck_validation/#source-content-converted-from-html","title":"Source content (converted from HTML)","text":"<p>HUF // SCIENCE Overview Hierarchy Math Simulation Execution</p> <p>VALIDATION PARTNER</p> <p>HUF Partner // Science &amp; Astrophysics Validation // ESA Planck Reference</p>"},{"location":"partners/integrations/science_planck_validation/#cosmic-hierarchyvalidation","title":"Cosmic HierarchyValidation","text":"<p>Planck LFI 70 GHz as a canonical reference-run template \u2014 a microwave-frequency hierarchy normalized (illustrative).</p> <p>0.968 Post-HUF C(\u210b)</p> <p>7 Frequency Channels</p> <p>2.725K CMB Avg Temperature</p> <p>\u221274% Drift Reduction</p> <p>\u2193 Explore the Reference Run</p> <p>// Planck frequency coverage \u2014 30 to 353 GHz // HUF regime focus: 70 GHz LFI</p> <p>30 GHz44 GHz\u2605 70 GHz [LFI Primary]100 GHz143 GHz217 GHz353 GHz</p> <p>70 GHz LFI selected as HUF primary regime. T_avg = 2.725 K, variance = 0.0001 K (ESA 2018 dataset, n=1,000 data points). Unity budget: channel contributions normalized to \u03a3\u03c1 = 1.000.</p> <p>01 // Hierarchy Structure</p>"},{"location":"partners/integrations/science_planck_validation/#the-cmb-hierarchy-asa-unity-budgeted-system","title":"The CMB Hierarchy asa Unity-Budgeted System","text":"<p>HUF treats the Planck frequency hierarchy as a finite-element system: each channel is an auditable unit contributing a non-negative mass to a conserved unity budget. The 70 GHz LFI channel is the primary validation regime \u2014 its detector contributions sum to 1.000 after normalization, serving as a canonical reference-run template (without claiming new ESA results).</p> <p>ESA Planck Mission \u03c1_global = 1.000 ROOT REGIME</p> <p>Low Frequency Instrument (LFI) \u03c1_lfi = 0.412 LFI REGIME</p> <p>70 GHz \u2014 Primary Validation Channel \u2605 \u03c1_70 = 0.218 PRIMARY FOCUS</p> <p>Detector Array [18 horns \u2192 12 active in 2018 run] \u03c1_det = normalized FINITE ELEMENTS</p> <p>44 GHz \u03c1_44 = 0.127 LFI REGIME</p> <p>30 GHz \u03c1_30 = 0.067 LFI REGIME</p> <p>High Frequency Instrument (HFI) \u03c1_hfi = 0.588 HFI REGIME</p> <p>100 / 143 / 217 / 353 GHz channels \u03c1 distributed FINITE ELEMENTS</p> <p>The locked cycle \u2014 Normalize \u2192 Propagate \u2192 Aggregate \u2192 Exclude \u2192 Renormalize \u2014 governs each calibration epoch. A run is invalid unless it emits a unity-sum artifact (\u03a3\u03c1 = 1.000 \u00b1 1\u00d710\u207b\u2076) and passes drift detection against the previous calibration epoch.</p> <p>02 // Mathematical Foundation</p>"},{"location":"partners/integrations/science_planck_validation/#adaptive-damping-forcosmic-drift-detection","title":"Adaptive Damping forCosmic Drift Detection","text":"<p>HUF applies adaptive damping \u03b1* to the Planck hierarchy, minimizing the science-domain objective J_sci that penalizes variance in channel mass distribution (spectral coherence penalty) while maintaining the CMB temperature normalization invariant.</p> <p>// Normalization \u2014 channel embeddings relative to calibration epoch p N(e_v|p) = (e_v \u2212 \u03bc_p) / \u03c3_p \u2299 w_vp</p> <p>// Recursion \u2014 propagate from parent regime to detector level e'_v = N(e_v|p) + \u03b1 \u00b7 e'_p</p> <p>// Mass distribution \u2014 softmax over all channels \u03c1_global,post(v) = exp(e'_v \u00b7 q) / \u03a3 exp(e'_u \u00b7 q) // \u03a3\u03c1 = 1.000 always</p> <p>// Coherence \u2014 drift from previous calibration epoch C(\u210b) = 1 \u2212 (1/|\u210b|) \u03a3 ||e'_v \u2212 e_v^(t-1)||\u2082</p> <p>// Science objective \u2014 spectral coherence penalty + temperature variance J_sci(\u03b1) = (1\u2212C) + \u03bb\u00b7Var(\u03c1) + \u03c3\u00b7|T_obs \u2212 T_ref|</p> <p>// Planck values: \u03bb=0.15 (spectral balance), \u03c3=0.08 (temperature stability) // T_ref = 2.725K, alert threshold: Var(\u03c1) &gt; 0.015 OR C(\u210b) &lt; 0.95</p> <p>// Long-tail exclusion: channels with \u03c1 &lt; \u03c4=0.005 flagged, discarded budget logged \u03b1* = argmin_\u03b1 J_sci(\u03b1), \u03b1 \u2208 [0.40, 0.75] (range from ESA calibration priors)</p> <p>Unity conservation note for science hierarchies: The softmax axiom guarantees \u03a3\u03c1 = 1.000 at every calibration epoch. Channel pruning (long-tail exclusion at \u03c4=0.005) discards budget to an explicit remainder term D, maintaining: \u03a3\u03c1_retained + D = 1.000. This makes every reduction auditable \u2014 ESA can trace which detectors were excluded and why.</p> <p>02b // Integration Code</p>"},{"location":"partners/integrations/science_planck_validation/#reference-runimplementation","title":"Reference RunImplementation","text":"<p>The Planck reference run is executable against the ESA 2018 public dataset. The HUFScienceValidator class loads channel calibration data, normalizes to unity, detects drift against prior epoch, and emits a JSONL artifact per run.</p> <pre><code># HUF Science Validator \u2014 Planck LFI 70 GHz reference run\nimport numpy as np\nimport json\nfrom datetime import datetime\n\nclass HUFScienceValidator:\n    def \\_\\_init\\_\\_(self, channels, T\\_ref=2.725, tau=0.005,\n                 lam=0.15, sigma=0.08):\n        self.channels = channels     # list of channel dicts: {id, freq\\_ghz, T\\_mean}\n        self.T\\_ref = T\\_ref\n        self.tau = tau               # long-tail exclusion threshold\n        self.lam = lam               # spectral variance penalty\n        self.sigma\\_param = sigma     # temperature stability penalty\n        self.prev\\_embeddings = {}    # previous epoch for coherence\n\n    def \\_normalize(self, T\\_values):\n        mu = np.mean(T\\_values)\n        sig = np.std(T\\_values) + 1e-9\n        return (T\\_values - mu) / sig\n\n    def \\_objective(self, alpha, embeddings, query, prev\\_emb):\n        propagated = embeddings + alpha * np.mean(embeddings)\n        scores = np.exp(propagated * query)\n        rho = scores / scores.sum()\n        C = (1.0 - np.mean(np.abs(propagated - prev\\_emb))\n             if prev\\_emb is not None else 0.92)\n        var\\_rho = np.var(rho)\n        T\\_dev = abs(np.mean([c['T\\_mean'] for c in self.channels]) - self.T\\_ref)\n        return (1-C) + self.lam*var\\_rho + self.sigma\\_param*T\\_dev\n\n    def validate\\_epoch(self, epoch\\_id, T\\_observations, query=0.8):\n        T = np.array(T\\_observations)\n        embeddings = self.\\_normalize(T)\n        prev = np.array(list(self.prev\\_embeddings.values())) \\\n               if self.prev\\_embeddings else None\n\n        # Find optimal alpha\n        best\\_alpha, best\\_J = 0.55, float('inf')\n        for alpha in np.linspace(0.40, 0.75, 50):\n            J = self.\\_objective(alpha, embeddings, query, prev)\n            if J &lt; best\\_J:\n                best\\_J, best\\_alpha = J, alpha\n\n        propagated = embeddings + best\\_alpha * np.mean(embeddings)\n        scores = np.exp(propagated * query)\n        rho = scores / scores.sum()\n\n        # Long-tail exclusion\n        excluded = [self.channels[i]['id'] for i,r in enumerate(rho) if r &lt; self.tau]\n        retained = [r for r in rho if r &gt;= self.tau]\n        discarded\\_budget = sum(r for r in rho if r &lt; self.tau)\n\n        C\\_score = (1.0 - np.mean(np.abs(propagated - prev))\n                   if prev is not None else 0.94)\n        alert = C\\_score &lt; 0.95 or np.var(rho) &gt; 0.015\n\n        artifact = {\n            'epoch': epoch\\_id,\n            'timestamp': datetime.utcnow().isoformat(),\n            'alpha\\_star': round(best\\_alpha, 3),\n            'rho': {c['id']: round(float(r),4) for c,r in zip(self.channels, rho)},\n            'unity\\_check': abs(sum(rho) - 1.0) &lt; 1e-6,\n            'C\\_coherence': round(C\\_score, 4),\n            'var\\_rho': round(float(np.var(rho)), 5),\n            'discarded\\_budget': round(discarded\\_budget, 4),\n            'excluded\\_channels': excluded,\n            'alert': alert\n        }\n        self.prev\\_embeddings = {c['id']: float(e)\n                                 for c,e in zip(self.channels, propagated)}\n        with open(f'huf\\_planck\\_{epoch\\_id}.jsonl', 'a') as f:\n            f.write(json.dumps(artifact) + '\\n')\n        return artifact\n</code></pre> <p>03 // Reference Run Simulation</p>"},{"location":"partners/integrations/science_planck_validation/#10-epoch-calibrationsimulation-results","title":"10-Epoch CalibrationSimulation Results","text":"<p>Simulating 10 calibration epochs against the ESA 2018 dataset. Epoch 4 introduces a synthetic drift event (detector gain instability at 70 GHz), triggering the HUF alert. Adaptive damping recovers coherence by Epoch 6.</p> Epoch \u03b1* \u03c1_70GHz \u03c1_HFI_dom Var(\u03c1) C(\u210b) Excl. Status E-01 0.55 0.218 0.241 0.0089 0.942 0 PASS E-02 0.54 0.221 0.238 0.0091 0.952 0 PASS E-03 0.56 0.215 0.243 0.0086 0.958 0 PASS E-04 0.61 0.142 0.298 0.0241 0.921 1 \u26a0 ALERT E-05 0.63 0.168 0.271 0.0188 0.934 1 \u26a0 WATCH E-06 0.58 0.204 0.252 0.0112 0.951 0 PASS E-07 0.55 0.219 0.240 0.0094 0.961 0 PASS E-08 0.54 0.222 0.237 0.0087 0.964 0 PASS E-09 0.56 0.217 0.241 0.0090 0.967 0 PASS E-10 0.55 0.220 0.239 0.0088 0.968 0 PASS <p>Epoch 4 event: \u03c1_70GHz dropped from 0.215 \u2192 0.142 (detector gain instability). Var(\u03c1) exceeded 0.015 threshold \u2192 alert triggered. JSONL artifact emitted with excluded_channels: ['LFI-70-horn-22a']. HUF adaptive damping increased \u03b1* to 0.61\u20130.63 to dampen the drift signal. Full recovery by Epoch 6. Final C(\u210b) = 0.968.</p> <p>0.968 Final C(\u210b)</p> <p>\u221274% Drift Reduction</p> <p>0.55 Avg \u03b1*</p> <p>1.000 Unity \u03a3\u03c1</p> <p>E-04 Alert at epoch</p> <p>10 JSONL artifacts</p> <p>04 // Pitch &amp; Execution</p>"},{"location":"partners/integrations/science_planck_validation/#the-sciencepartnership","title":"The SciencePartnership","text":"<p>HUF's Planck-style reference run is a proof-of-concept template for scientific hierarchy normalization: 7 frequency channels, unity budget, full backward trace. The 70 GHz LFI drift event \u2014 can be difficult to see in standard summary metrics \u2014 was flagged by Var(\u03c1) &gt; 0.015; in an operational pipeline you could use that threshold to pause propagation and inspect adjacent epochs. The same pattern is reusable on hierarchical science datasets where reduction must remain auditable.</p>"},{"location":"partners/integrations/science_planck_validation/#entry-points","title":"Entry Points","text":""},{"location":"partners/integrations/science_planck_validation/#esa-open-science","title":"ESA Open Science","text":"<p>Submit as methodology paper to the Planck Legacy Archive documentation. Target: \"HUF as a normalization layer for calibration epoch governance\" alongside existing NPIPE/BeyondPlanck pipelines.</p>"},{"location":"partners/integrations/science_planck_validation/#academic-channels","title":"Academic Channels","text":"<p>arXiv astro-ph.CO preprint: \"Unity-Budgeted Hierarchies for CMB Channel Governance.\" Target journals: A&amp;A Instrument Methods, JCAP. Reference ESA 2018 public dataset.</p>"},{"location":"partners/integrations/science_planck_validation/#open-source","title":"Open Source","text":"<p>PR to healpy / healpix-cxx or pixell CMB analysis libraries. Frame as: \"coherence audit plugin for frequency channel pipelines.\" Reproducible notebook on Binder.</p>"},{"location":"partners/integrations/science_planck_validation/#extension-science-domains","title":"Extension Science Domains","text":"<p>Radio telescopes (SKA, VLA) \u2014 frequency channel hierarchies. Gravitational wave detectors \u2014 strain channel coherence. Any multi-instrument science array with calibration drift.</p>"},{"location":"partners/integrations/science_planck_validation/#validation-checklist-reproducing-the-reference-run","title":"Validation Checklist \u2014 Reproducing the Reference Run","text":"<p>\u2713 Load ESA 2018 Planck public release data (LFI 70 GHz, n=1,000 calibration points) \u2713 Declare finite elements: 7 frequency channels, each with unique ID + T_mean provenance \u2713 Run locked cycle: Normalize \u2192 Propagate (\u03b1=0.55) \u2192 Aggregate \u2192 Exclude (\u03c4=0.005) \u2192 Renormalize \u2713 Assert \u03a3\u03c1 = 1.000 \u00b1 1\u00d710\u207b\u2076 \u2014 fail the run if violated \u2713 Emit JSONL artifact per epoch: {epoch, alpha_star, rho, unity_check, C_coherence, excluded_channels} \u2713 Alert on C(\u210b) &lt; 0.95 or Var(\u03c1) &gt; 0.015 \u2713 Final C(\u210b) should reach 0.968 on clean dataset</p> <p>HUF \u2014 Higgins Unity Framework // Science Validation Partner ESA Planck LFI 70 GHz // T_ref=2.725K // n=1,000 epochs C(\u210b)=0.968 // \u03b1*=0.55 avg</p>"},{"location":"partners/integrations/weaviate/","title":"Weaviate Integration &amp; Case Package","text":"<p>Multi-tenant vector database with HOT/COLD/Offloaded tenant states \u2014 mapped to HUF regime-conditioned damping and coherence governance.</p> <p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"partners/integrations/weaviate/#what-this-page-is","title":"What this page is","text":"<p>A partner integration note showing how to treat a platform\u2019s native operational concepts (tenants, namespaces, tiers, callbacks) as HUF regimes, so you can run regime-conditioned normalization and get an auditable drift/coherence readout.</p>"},{"location":"partners/integrations/weaviate/#why-it-matters","title":"Why it matters","text":"<ul> <li>It turns platform behavior into explicit regime parameters (penalties, damping, promotion/demotion), instead of hiding them inside \u201cops lore\u201d.</li> <li>It gives you an audit layer: what changed, where the mass moved, and whether you\u2019re accidentally over-concentrating retrieval or authority.</li> </ul>"},{"location":"partners/integrations/weaviate/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Architecture Fit</li> <li>HOT / WARM / COLD Tenant States</li> <li>Mathematical Foundation</li> <li>Integration Code</li> <li>10-Step Drift Simulation</li> <li>Pitch, Contacts &amp; Execution</li> </ul>"},{"location":"partners/integrations/weaviate/#artifacts-outputs","title":"Artifacts / outputs","text":"<p>Package metrics (from the outreach HTML):</p> Metric Value Partner Fit Score 97% Cumulative C(\u210b) 0.925 Avg Optimal Damping \u03b1*=0.51 kb Erosion (10-step) \u221215.1% <ul> <li>JSONL traces with per-step regime weights, normalized element weights, and drift metrics.</li> <li>A small reference simulation (10 steps / sessions) you can run locally and modify.</li> </ul>"},{"location":"partners/integrations/weaviate/#run-the-example","title":"Run the example","text":""},{"location":"partners/integrations/weaviate/#integration-snippet","title":"Integration snippet","text":"<pre><code>import\n weaviate\n\nimport\n json\n\nclient = weaviate.Client(\n\"http://localhost:8080\"\n)\n\n\n# Query with multi-tenant metadata and certainty scores\n\nresults = (\n    client.query\n    .get(\n\"KnowledgeBase\"\n, [\n\"title\"\n, \n\"source\"\n])\n    .with_additional([\n\"id\"\n, \n\"certainty\"\n])\n    .with_tenant(\n\"tenant_id\"\n)\n    .do()\n)\n\n\n# Get tenant state from management API\n\n\ndef\n\nget_tenant_state\n(client, class_name, tenant_id):\n    tenants = client.schema.get_class_tenants(class_name)\n\nfor\n t \nin\n tenants:\n\nif\n t[\n\"name\"\n] == tenant_id:\n\nreturn\n t.get(\n\"activityStatus\"\n, \n\"HOT\"\n)\n\nreturn\n\n\"HOT\"\n\n\n\n# Map Weaviate states \u2192 HUF S(s_r) penalties\n\nSTATE_PENALTIES = {\n\n\"HOT\"\n:      \n0.00\n,   \n# ACTIVE\n\n\n\"WARM\"\n:     \n0.10\n,   \n# INACTIVE (disk)\n\n\n\"COLD\"\n:     \n0.20\n,   \n# OFFLOADED (S3/GCS)\n\n}\n\n\n# Export to HUF JSONL format\n\n\nwith\n open(\n\"weaviate_huf_input.jsonl\"\n, \n\"w\"\n) \nas\n f:\n\nfor\n obj \nin\n results[\n\"data\"\n][\n\"Get\"\n][\n\"KnowledgeBase\"\n]:\n        tenant_id = obj.get(\n\"tenant\"\n, \n\"default\"\n)\n        state = get_tenant_state(client, \n\"KnowledgeBase\"\n, tenant_id)\n        export = {\n\n\"id\"\n:           obj[\n\"_additional\"\n][\n\"id\"\n],\n\n\"score\"\n:        obj[\n\"_additional\"\n][\n\"certainty\"\n],\n\n\"namespace\"\n:    tenant_id,\n\n\"tenant_state\"\n: state,\n\n\"state_penalty\"\n: STATE_PENALTIES.get(state, \n0.0\n)\n        }\n        f.write(json.dumps(export) + \n\"\\n\"\n)\n\n\nprint\n(\nf\"Exported with state-conditioned penalties.\"\n)\n</code></pre>"},{"location":"partners/integrations/weaviate/#simulation-calibration-snippet","title":"Simulation / calibration snippet","text":"<pre><code>import\n numpy \nas\n np\n\nfrom\n scipy.optimize \nimport\n minimize_scalar\n\nfrom\n scipy.special \nimport\n softmax\n\n\ndef\n\nj_func_weaviate\n(alpha, scores, state_penalty, lam=\n0.1\n, beta=\n0.05\n):\n\n\"\"\"HUF objective with Weaviate HOT/COLD penalty.\"\"\"\n\n    e_prime = scores * alpha\n    rho = softmax(e_prime)\n    c = \n1.0\n - np.mean(np.abs(e_prime - scores))\n    var = np.var(rho)\n\nreturn\n (\n1\n - c) + lam * var + beta * state_penalty\n\n\ndef\n\nrun_weaviate_huf\n(jsonl_path, output_prefix):\n\nimport\n pandas \nas\n pd\n    df = pd.read_json(jsonl_path, lines=\nTrue\n)\n\n\n# Per-regime adaptive damping with state penalties\n\n    regime_results = {}\n\nfor\n regime, grp \nin\n df.groupby(\n\"namespace\"\n):\n        scores = grp[\n\"score\"\n].values\n        state_pen = grp[\n\"state_penalty\"\n].mean()\n\n        res = minimize_scalar(\n\nlambda\n a: j_func_weaviate(a, scores, state_pen),\n            bounds=(\n0\n, \n1\n), method=\n\"bounded\"\n\n        )\n        alpha_star = res.x\n        e_prime = scores * alpha_star\n        rho = softmax(e_prime)\n\n        regime_results[regime] = {\n\n\"alpha_star\"\n: \nround\n(alpha_star, \n4\n),\n\n\"rho_mass\"\n: \nround\n(rho.sum(), \n4\n),\n\n\"state_penalty\"\n: state_pen,\n\n\"c_local\"\n: \nround\n(\n1\n - np.mean(np.abs(e_prime - scores)), \n4\n)\n        }\n\n\nreturn\n regime_results\n\n\n# CLI invocation:\n\n\n# python run_vector_db_demo.py --in weaviate_huf_input.jsonl \\\n\n\n#   --out weaviate_out --regime-field namespace \\\n\n\n#   --state-field tenant_state --tau-global 0.02\n</code></pre>"},{"location":"partners/integrations/weaviate/#what-to-expect","title":"What to expect","text":"<ul> <li>You should see a coherence score improve relative to a baseline and a kb erosion (drift) estimate.</li> <li>If your regimes get penalized, the remaining regimes can grow in normalized share even as total mass falls \u2014 see the intuition note below.</li> </ul> <p>Why does loss make retained stronger?</p> <p>HUF tracks absolute mass and normalized share. If you remove mass (penalty/exclusion) and then renormalize, the survivors\u2019 shares increase. That\u2019s not \u201crewarding loss\u201d; it\u2019s reporting relative composition after shrinkage.</p>"},{"location":"partners/integrations/weaviate/#interpretation","title":"Interpretation","text":"<ul> <li>Treat the \u201ccoherence\u201d metric as an audit signal, not a metaphysical truth: it\u2019s telling you whether your hierarchy remains stable under repeated operations.</li> <li>When coherence rises but mass drops, you may be over-pruning (good for safety, bad for coverage). When mass rises but coherence drops, you may be over-collecting (coverage with diluted traceability).</li> </ul>"},{"location":"partners/integrations/weaviate/#next-steps","title":"Next steps","text":"<ul> <li>Replace the toy scores with real platform logs (retrieval scores, tenant state transitions, query metadata).</li> <li>Add your own regime penalty term (latency budgets, privacy tiers, cost ceilings).</li> <li>Pipe the JSONL into your existing dashboards (Grafana, notebooks, CI) and set thresholds.</li> </ul>"},{"location":"partners/integrations/weaviate/#reference-details-converted-from-outreach-html","title":"Reference details (converted from outreach HTML)","text":""},{"location":"partners/integrations/weaviate/#architecture-fit","title":"Architecture Fit","text":""},{"location":"partners/integrations/weaviate/#weaviate-concepts-huf-primitives","title":"Weaviate Concepts \u2192 HUF Primitives","text":"<ul> <li>Tenant \u2192 HUF Regime (r)</li> <li>Class/Collection \u2192 HUF Hierarchy node (v \u2208 V)</li> <li>Certainty / Distance \u2192 Score converted to e_v embedding</li> <li>Tenant State (HOT/WARM/COLD) \u2192 S(s_r) penalty in J_r(\u03b1_r)</li> <li>Namespace isolation \u2192 Disjoint regime partitioning</li> </ul>"},{"location":"partners/integrations/weaviate/#why-weaviate-fits-huf","title":"Why Weaviate Fits HUF","text":"<ul> <li>Multi-tenancy first-class: Tenants map 1:1 to HUF regimes</li> <li>Certainty scores: Direct score field for normalization pipeline</li> <li>Tenant states: HOT/COLD maps natively to state-conditioned damping</li> <li>Python SDK: Clean export to JSONL for offline HUF processing</li> <li>EU AI Act alignment: Weaviate's privacy isolation + HUF traceability</li> </ul>"},{"location":"partners/integrations/weaviate/#weaviate-integration-fit-breakdown","title":"WEAVIATE INTEGRATION FIT BREAKDOWN","text":"<p>Multi-tenancy \u2192 Regime mapping99% Certainty / Distance \u2192 Score field98% HOT/COLD states \u2192 Damping penalties97% Python SDK \u2192 JSONL export95% EU AI Act compliance overlay93%</p>"},{"location":"partners/integrations/weaviate/#hot-warm-cold-tenant-states","title":"HOT / WARM / COLD Tenant States","text":"<p>Weaviate v1.24+ exposes tenant lifecycle states that determine memory residency. HUF maps each state directly to a penalty term S(s_r) in the per-regime objective J_r(\u03b1_r), enabling state-aware adaptive damping.</p> <p>STATE: ACTIVE \ud83d\udd25 HOT In-memory, full query performance. Newly active or frequently accessed tenants. S(s_r) = 0.00 No penalty. Standard \u03b1* optimization applies. HUF prioritizes these regimes in mass aggregation.</p> <p>STATE: INACTIVE \ud83c\udf21 WARM Disk-backed, swapped from memory. Queryable but slower. Weaviate INACTIVE state. S(s_r) = 0.10 Moderate penalty. J_r is increased by \u03b2\u00b70.10, biasing \u03b1* toward more conservative inheritance from stable parent regimes.</p> <p>STATE: OFFLOADED \u2744 COLD Offloaded to object storage (S3/GCS). Tenant not queryable until reactivation. S(s_r) = 0.20 Maximum penalty. High \u03b2\u00b70.20 pushes \u03b1* lower \u2014 reducing inheritance, limiting drift propagation from dormant regimes.</p>"},{"location":"partners/integrations/weaviate/#state-transition-protocol","title":"State Transition Protocol","text":"<p>When a tenant transitions HOT \u2192 COLD (offloaded), HUF triggers automatic re-normalization. The S(s_r) penalty increases the J_r objective, the optimizer selects a more conservative \u03b1_r*, and global mass re-aggregates to exclude the offloaded regime's contribution from the 90th-percentile coverage metric (<code>items_to_cover_90pct</code>). On COLD \u2192 HOT reactivation, S drops to 0 and re-normalization restores full mass allocation.</p>"},{"location":"partners/integrations/weaviate/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Per-Regime Objective (HOT/COLD-aware): J_r(\u03b1_r) = (1 \u2212 C_r) + \u03bb\u00b7Var(\u03c1_local,post,r) + \u03b2\u00b7S(s_r) where S(HOT)=0, S(Inactive)=0.10, S(COLD)=0.20 | \u03bb=0.1, \u03b2=0.05</p> <p>Global Aggregation: J(\u03b1) = \u03a3_r w_r \u00b7 J_r(\u03b1_r) + \u03b3 \u00b7 Cov(\u03c1_global) w_r = regime mass weight | \u03b3=0.05 | HOT regimes receive higher w_r</p> <p>State-Conditioned Optimal Damping: \u03b1_r = argmin J_r(\u03b1_r) subject to \u03b1_r \u2208 [0,1] \u2192 HOT: \u03b1 \u2248 0.55 (strong inheritance) | COLD: \u03b1* \u2248 0.45 (conservative) Derived via scipy.minimize_scalar with bounds=(0,1), method='bounded'</p> <p>Certainty \u2192 Score Conversion (Weaviate-specific): score = certainty (already \u2208 [0,1]) | or score = 1 \u2212 distance (for cosine/L2) Weaviate 'certainty' field maps directly to HUF score in JSONL export</p>"},{"location":"partners/integrations/weaviate/#proof-of-state-conditioned-stability","title":"Proof of State-Conditioned Stability","text":"<p>Extension of the Multi-Regime Stability Proof (Proof 4). S(s_r) is a convex, non-negative penalty added to J_r. Since (1\u2212C_r) and Var(\u03c1_local,r) are already convex in \u03b1_r, and S(s_r) is a constant penalty (state is fixed at evaluation time), strict convexity is preserved. The Weierstrass minimum exists and is unique on [0,1]. As S increases (HOT\u2192COLD transition), the minimizer shifts to lower \u03b1_r* values, reducing local variance and bounding drift propagation from dormant tenants. \u220e</p>"},{"location":"partners/integrations/weaviate/#integration-code","title":"Integration Code","text":"<p>weaviate_to_huf.py \u2014 Export with tenant state Python</p> <pre><code>import weaviate\nimport json\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\n# Query with multi-tenant metadata and certainty scores\nresults = (\n    client.query\n    .get(\"KnowledgeBase\", [\"title\", \"source\"])\n    .with\\_additional([\"id\", \"certainty\"])\n    .with\\_tenant(\"tenant\\_id\")\n    .do()\n)\n\n# Get tenant state from management API\ndef get\\_tenant\\_state(client, class\\_name, tenant\\_id):\n    tenants = client.schema.get\\_class\\_tenants(class\\_name)\n    for t in tenants:\n        if t[\"name\"] == tenant\\_id:\n            return t.get(\"activityStatus\", \"HOT\")\n    return \"HOT\"\n\n# Map Weaviate states \u2192 HUF S(s\\_r) penalties\nSTATE\\_PENALTIES = {\n    \"HOT\":      0.00,   # ACTIVE\n    \"WARM\":     0.10,   # INACTIVE (disk)\n    \"COLD\":     0.20,   # OFFLOADED (S3/GCS)\n}\n\n# Export to HUF JSONL format\nwith open(\"weaviate\\_huf\\_input.jsonl\", \"w\") as f:\n    for obj in results[\"data\"][\"Get\"][\"KnowledgeBase\"]:\n        tenant\\_id = obj.get(\"tenant\", \"default\")\n        state = get\\_tenant\\_state(client, \"KnowledgeBase\", tenant\\_id)\n        export = {\n            \"id\":           obj[\"\\_additional\"][\"id\"],\n            \"score\":        obj[\"\\_additional\"][\"certainty\"],\n            \"namespace\":    tenant\\_id,\n            \"tenant\\_state\": state,\n            \"state\\_penalty\": STATE\\_PENALTIES.get(state, 0.0)\n        }\n        f.write(json.dumps(export) + \"\\n\")\n\nprint(f\"Exported with state-conditioned penalties.\")\n</code></pre> <p>huf_weaviate_adapter.py \u2014 State-conditioned HUF run Python</p> <pre><code>import numpy as np\nfrom scipy.optimize import minimize\\_scalar\nfrom scipy.special import softmax\n\ndef j\\_func\\_weaviate(alpha, scores, state\\_penalty, lam=0.1, beta=0.05):\n    \"\"\"HUF objective with Weaviate HOT/COLD penalty.\"\"\"\n    e\\_prime = scores * alpha\n    rho = softmax(e\\_prime)\n    c = 1.0 - np.mean(np.abs(e\\_prime - scores))\n    var = np.var(rho)\n    return (1 - c) + lam * var + beta * state\\_penalty\n\ndef run\\_weaviate\\_huf(jsonl\\_path, output\\_prefix):\n    import pandas as pd\n    df = pd.read\\_json(jsonl\\_path, lines=True)\n\n    # Per-regime adaptive damping with state penalties\n    regime\\_results = {}\n    for regime, grp in df.groupby(\"namespace\"):\n        scores = grp[\"score\"].values\n        state\\_pen = grp[\"state\\_penalty\"].mean()\n\n        res = minimize\\_scalar(\n            lambda a: j\\_func\\_weaviate(a, scores, state\\_pen),\n            bounds=(0, 1), method=\"bounded\"\n        )\n        alpha\\_star = res.x\n        e\\_prime = scores * alpha\\_star\n        rho = softmax(e\\_prime)\n\n        regime\\_results[regime] = {\n            \"alpha\\_star\": round(alpha\\_star, 4),\n            \"rho\\_mass\": round(rho.sum(), 4),\n            \"state\\_penalty\": state\\_pen,\n            \"c\\_local\": round(1 - np.mean(np.abs(e\\_prime - scores)), 4)\n        }\n\n    return regime\\_results\n\n# CLI invocation:\n# python run\\_vector\\_db\\_demo.py --in weaviate\\_huf\\_input.jsonl \\\n# --out weaviate\\_out --regime-field namespace \\\n# --state-field tenant\\_state --tau-global 0.02\n</code></pre>"},{"location":"partners/integrations/weaviate/#10-step-drift-simulation","title":"10-Step Drift Simulation","text":"<p>10-step simulation with HOT/WARM/COLD state transitions. kb regime degrades at factor=0.90 + noise(std=0.05). Weaviate tenant state transitions injected at steps 3 (WARM) and 7 (COLD reactivation \u2192 HOT).</p> STEP TENANT STATE S(s_r) \u03b1* (kb) \u03c1_post (kb) C_local ITEMS_90pct 0 HOT 0.00 \u2014 0.628 \u2014 9 1 HOT 0.00 0.52 0.614 0.992 9 2 HOT 0.00 0.53 0.601 0.990 9 3 WARM 0.10 0.49 0.592 0.987 9 4 WARM 0.10 0.48 0.580 0.985 9 5 COLD 0.20 0.45 0.571 0.982 8 6 COLD 0.20 0.44 0.562 0.984 8 7 HOT \u2191 0.00 0.55 0.558 0.997 9 8 HOT 0.00 0.54 0.548 0.991 9 9 HOT 0.00 0.53 0.540 0.993 9 10 HOT 0.00 0.52 0.531 0.994 9"},{"location":"partners/integrations/weaviate/#cumulative-ch","title":"Cumulative C(\u210b)","text":"<p>0.925 +1.8% vs. static damping</p>"},{"location":"partners/integrations/weaviate/#hot-vs-cold","title":"HOT \u03b1 vs COLD \u03b1","text":"<p>HOT: 0.55 \u2014 Strong inheritance, prioritized mass COLD: 0.45 \u2014 Conservative, drift contained</p>"},{"location":"partners/integrations/weaviate/#kb-erosion","title":"kb Erosion","text":"<p>\u221215.1% 0.628 \u2192 0.531 over 10 steps. State transitions preserve 9/10 items_to_cover_90pct.</p>"},{"location":"partners/integrations/weaviate/#pitch-contacts-execution","title":"Pitch, Contacts &amp; Execution","text":""},{"location":"partners/integrations/weaviate/#3-sentence-pitch","title":"3-SENTENCE PITCH","text":"<p>\"HUF is an audit layer that normalizes multi-tenant retrieval to unity \u2014 giving you a regime-mass breakdown, a concentration metric (items_to_cover_90pct), a full discard log, and offline JSONL provenance, all in one run. For Weaviate tenants, it maps HOT/COLD states directly to adaptive damping penalties, so dormant tenants never silently dominate your coherence score. The headline metric: your retrieval drift reduced by 15%+ with full backward traceability to every finite element.\"</p> <p>ES</p>"},{"location":"partners/integrations/weaviate/#erika-shorten","title":"Erika Shorten","text":"<p>Technology Partner Manager, Weaviate \u2192 Primary outreach target</p> <p>JG</p>"},{"location":"partners/integrations/weaviate/#jobi-george","title":"Jobi George","text":"<p>VP Business Development, Weaviate \u2192 Secondary / escalation contact</p>"},{"location":"partners/integrations/weaviate/#execution-sequence","title":"Execution Sequence","text":"<p>1</p>"},{"location":"partners/integrations/weaviate/#github-pr","title":"GitHub PR","text":"<p>weaviate/partner-integration-examples</p> <p>PR title: \"HUF coherence adapter for Weaviate multi-tenant HOT/COLD state governance\"</p> <p>2</p>"},{"location":"partners/integrations/weaviate/#slack-outreach","title":"Slack Outreach","text":"<p>Weaviate Community #integrations \u2014 share PR link + 3-sentence pitch</p> <p>3</p>"},{"location":"partners/integrations/weaviate/#email-to-erika-shorten","title":"Email to Erika Shorten","text":"<p>Subject: \"HUF \u00d7 Weaviate \u2014 HOT/COLD coherence governance pilot\"</p> <p>4</p>"},{"location":"partners/integrations/weaviate/#57-day-follow-up","title":"5\u20137 Day Follow-Up","text":"<p>Demo invite with 15-minute live run on real Weaviate tenant data</p>"},{"location":"science/","title":"Science","text":"<p>Ethics &amp; authorship</p> <p>This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.</p>"},{"location":"science/#what-this-page-is","title":"What this page is","text":"<p>A research-facing entry point explaining where HUF came from, what problems it was built to solve, and how to interpret the science-oriented case studies.</p>"},{"location":"science/#why-it-matters","title":"Why it matters","text":"<ul> <li>Many \u201cdrift\u201d problems are really composition problems: what mass moved between regimes, and which substructures lost coherence.</li> <li>HUF gives you a normalization-invariant way to measure this, even when the hierarchy is nested and counterintuitive.</li> </ul>"},{"location":"science/#what-youll-see","title":"What you\u2019ll see","text":"<ul> <li>Origin story (wavefront control \u2192 minimum-entropy thinking)</li> <li>HUF in the physical world</li> <li>AI embedding drift as a hierarchy problem</li> <li>Links to Planck / drift case studies</li> </ul>"},{"location":"science/#artifacts-outputs","title":"Artifacts / outputs","text":"<ul> <li>A small set of case studies and reference runs producing JSONL audit traces.</li> <li>A consistent set of HUF metrics (coherence, drift/erosion, mass concentration).</li> </ul>"},{"location":"science/#run-the-example","title":"Run the example","text":"<p>Pick a case study in the left nav and run the included <code>python</code> script. Each one emits a JSONL trace plus a short summary.</p>"},{"location":"science/#what-to-expect","title":"What to expect","text":"<ul> <li>If you \u201cpenalize\u201d a regime, remaining elements can show higher normalized share after renormalization. HUF reports both absolute and normalized views to keep this honest.</li> </ul>"},{"location":"science/#interpretation","title":"Interpretation","text":"<ul> <li>Treat the lead pages as orientation, and the case pages as reproducible artifacts.</li> </ul>"},{"location":"science/#next-steps","title":"Next steps","text":"<ul> <li>Choose one case study, run it, then swap in your own data with the same schema.</li> </ul>"},{"location":"science/#source-content-converted-from-html","title":"Source content (converted from HTML)","text":"<p>HUF Origin Minimum Entropy Cases AI &amp; Drift</p> <p>Science</p> <p>HUF \u00b7 Scientific Domain</p>"},{"location":"science/#where-hufcame-from","title":"Where HUFcame from","text":"<p>The Higgins Unity Framework was not designed at a whiteboard. It emerged from  the physics of sound \u2014 wavefront control, multi-driver coherence, and the  mathematics of what a loudspeaker array has to do to produce an isotropic  radiation field that human perception accepts as real.</p> <p>The origin</p>"},{"location":"science/#born-fromwavefront-control","title":"Born fromwavefront control","text":"<p>The problem was a multi-driver loudspeaker array: many drivers, each producing pressure  waves that must arrive at the listening position with the correct phase, amplitude, and  time relationship to produce a single coherent wavefront. Get this wrong and the sound  field collapses \u2014 not into silence, but into something worse: a field that sounds almost  right while hiding deep incoherence in its mass distribution.</p> <p>Isotropic radiation was the goal. Not a beam, not a hot-spot, but a  field that distributes acoustic energy with equal density in all directions \u2014 the kind  of field that dissolves the sensation of speakers and replaces it with the sensation of  space. Human auditory perception is exquisitely sensitive to the difference. The ear  does not hear pressure; it hears the geometry of arrival.</p> <p>To control that geometry across many drivers, you need a framework that normalizes  contributions from each driver into a coherent hierarchy, assigns damping to regulate  inter-driver coupling, and detects when any single driver is drifting from its role  in the collective wavefront. You need, in short, something that enforces unity across  a hierarchical system \u2014 that prevents any single element from silently dominating the  mass budget while the rest collapse into noise.</p> <p>The framework removed itself when it was done.  Tests showed the control application running thermodynamically  close to predicted optimum \u2014 minimum entropy.</p> <p>That is what HUF is. Not a generalization applied to audio, but a structure  that fell out of audio when the experiment ran correctly. The mathematics that made  a loudspeaker array produce an isotropic field at minimum cost turns out to be the  same mathematics that makes a vector database maintain coherent retrieval, a municipal  budget maintain equitable distribution, or a satellite instrument maintain calibrated  frequency channels across years of operation.</p> <p>The unity constraint is not an assumption. It is a thermodynamic consequence of  requiring a system to do its job \u2014 and only its job \u2014 without waste.</p> <p>System specifications</p> <p>Architecture Multi-driver time-coherent loudspeaker array</p> <p>Goal Isotropic radiation distribution in a sound field</p> <p>Domain of application Human auditory perception research</p> <p>Framework origin Emerged from experiment structure; not designed a priori</p> <p>Thermodynamic result Minimum entropy \u2014 close to predicted optimum</p> <p>Author Peter Higgins</p> <p>Thermodynamic interpretation</p>"},{"location":"science/#minimum-entropy-what-it-means","title":"Minimum entropy \u2014 what it means","text":"<p>When the control system ran correctly \u2014 drivers phased, weighted, and damped by the  emerging HUF structure \u2014 the experiment measured entropy production in the acoustic  field at levels thermodynamically close to the predicted minimum for that operation.</p> <p>This is not a poetic description. Minimum entropy production means  the system is doing exactly what it needs to do and nothing else. No wasted energy  driving incoherent cross-modes. No excess variance in the mass distribution. No  drift amplification from one driver into the global field.</p> <p>The unity constraint \u2014 that the sum of all normalized contributions equals one,  always, provably \u2014 is the mathematical statement of this thermodynamic fact.  A system that conserves its unity budget does not leak. A system that does not  leak approaches the minimum.</p> <p>When HUF converges, it is not just finding a stable solution.  It is finding the most efficient one available to the hierarchy.  The framework removes itself because a minimum-entropy system needs no ongoing  correction \u2014 the structure is self-maintaining once coherence is achieved.</p> <p>Unity conservation</p> <p>\u03a3 \u03c1 = 1.0 at every step, by proof. No mass is created or destroyed.  This is the acoustic energy budget constraint restated in embedding space.</p> <p>Adaptive damping \u03b1*</p> <p>The optimal coupling coefficient between levels of the hierarchy.  In audio: the inter-driver weighting that minimises wavefront distortion.  In HUF: argmin J(\u03b1) where J measures incoherence + variance.</p> <p>Self-removal</p> <p>When C(\u210b) stabilises, \u03b1* stabilises, and Var(\u03c1) \u2192 minimum. The framework  has nothing left to correct. The experiment runs at optimum. This is  thermodynamic convergence expressed as software behaviour.</p> <p>Drift as decoherence</p> <p>Embedding drift in AI systems is phase decoherence in sound systems.  The same structure that kept wavefronts aligned across drivers is now  keeping embeddings coherent across retrieval regimes.</p> <p>Scientific cases</p>"},{"location":"science/#huf-in-the-physical-world","title":"HUF in the physical world","text":"<p>Case \u00b7 001 \u00b7 ESA</p>"},{"location":"science/#planck-lfi-70-ghz","title":"Planck LFI 70 GHz","text":"<p>The Planck satellite's frequency channels (30\u2013353 GHz) as a HUF hierarchy.  Calibration drift between LFI and HFI instruments maps to the same decoherence  signal HUF was built to detect. 1,000 CMB sky pixels, foreground-penalised  adaptive damping, 10-step drift simulation.</p> <p>0.968 C(\u210b) post</p> <p>\u221227% Drift</p> <p>2.725 K CMB avg</p> <p>Read the case \u2192</p> <p>Case \u00b7 004 \u00b7 Partnership</p>"},{"location":"science/#science-planck-validation","title":"Science \u2014 Planck Validation","text":"<p>The full scientific partnership package: HUF as a CMB map validation layer  before component separation. Detects inter-channel calibration drift, provides  proof-backed convergence guarantees, and emits a per-step JSONL audit log  compatible with ESA data products.</p> <p>95% Fit</p> <p>7 ch. Regimes</p> <p>\u03c6\u00b7F_r Penalty</p> <p>Partner package \u2192</p> <p>Application \u00b7 Future</p>"},{"location":"science/#ai-embedding-drift","title":"AI Embedding Drift","text":"<p>Semantic embedding drift is phase decoherence. As language models update,  fine-tune, or retrieve across heterogeneous corpora, the embedding space  drifts \u2014 exactly as a driver array drifts when a transducer ages or a  room mode shifts. HUF's convergence proof applies directly.</p> <p>Active Research</p> <p>VDB First domain</p> <p>\u03b1* Key parameter</p> <p>Vector DB adapter \u2192</p>"},{"location":"science/#huf-and-ai-drift","title":"HUF and AI drift","text":"<p>The framework that controlled wavefront coherence in a multi-driver loudspeaker array  is now being applied \u2014 with no fundamental change \u2014 to vector database coherence,  retrieval drift in RAG systems, and the long-tail mass distribution of AI knowledge bases.</p> <p>This is not an analogy. An embedding space is a field. Semantic drift  is decoherence. The mass distribution over retrieved chunks obeys the same unity constraint  as the energy distribution over acoustic drivers. When any single source dominates \u2014  a single chunk, a single frequency, a single driver \u2014 the field collapses into something  that mimics quality while hiding incoherence.</p> <p>HUF's adaptive damping derives the optimal coupling coefficient at every step.  In a loudspeaker, that coefficient is a physical weighting. In a vector database,  it is the regime inheritance factor that prevents any single tenant, namespace, or  shard from silently taking over the retrieval budget. The mathematics is identical  because the problem is identical.</p> <p>The AI applications documented on this site \u2014 Weaviate, Qdrant, Pinecone, LangChain \u2014  are not analogies to the acoustic work. They are the same framework meeting the same  problem in a different medium. HUF may one day benefit AI drift mitigation in the  same way it benefited sound field control: not by being applied to it, but by  being recognised within it.</p> <p>HUF v1.1.8 \u00b7 Author: Peter Higgins \u00b7  Developed for wavefront control in multi-driver time-coherent systems \u00b7  Isotropic radiation distribution and human perception in a sound field \u00b7  Built with extensive use of AI in more advanced and functional applications \u00b7  Repository: PeterHiggins19/huf_core_github_v1.1.8_no_inputs</p> <p>HUF \u00b7 Science Lead Page \u00b7 v1.1.8 Minimum entropy \u00b7 C(\u210b) \u2192 optimum \u00b7 Unity conserved</p>"}]}